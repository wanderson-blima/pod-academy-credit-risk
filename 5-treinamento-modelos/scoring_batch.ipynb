{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Batch Pipeline - Credit Risk FPD\n",
    "\n",
    "**Story**: HD-3.2 / Fase 4.3 — Deploy\n",
    "**Objetivo**: Carregar modelo do MLflow Registry e aplicar scoring sobre novas SAFRAs\n",
    "**Output**: `Gold.feature_store.clientes_scores` (particionado por SAFRA)\n",
    "\n",
    "**Uso**: Parametrizar `SCORING_SAFRAS` e executar todas as celulas sequencialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/lakehouse/default/Files/projeto-final\")\n",
    "\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, StructType, StructField\n",
    "\n",
    "from config.pipeline_config import (\n",
    "    PATH_FEATURE_STORE, GOLD_BASE, EXPERIMENT_NAME,\n",
    "    SPARK_BROADCAST_THRESHOLD, SPARK_SHUFFLE_PARTITIONS, SPARK_AQE_ENABLED,\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\", datefmt=\"%H:%M:%S\")\n",
    "logger = logging.getLogger(\"scoring_batch\")\n",
    "\n",
    "logger.info(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PARAMETROS DE SCORING — ajustar antes de executar\n",
    "# =============================================================================\n",
    "\n",
    "# Modelo a usar (nome registrado no MLflow Model Registry)\n",
    "MODEL_NAME = \"credit-risk-fpd-lgbm_baseline\"\n",
    "MODEL_STAGE = \"Production\"  # ou \"Staging\" para testes\n",
    "\n",
    "# SAFRAs para scoring (lista de int YYYYMM)\n",
    "SCORING_SAFRAS = [202502, 202503]\n",
    "\n",
    "# Output path\n",
    "SCHEMA_SCORES = \"feature_store\"\n",
    "TABLE_SCORES = \"clientes_scores\"\n",
    "PATH_SCORES = f\"{GOLD_BASE}/Tables/{SCHEMA_SCORES}/{TABLE_SCORES}\"\n",
    "\n",
    "# Faixas de risco (quintis)\n",
    "N_FAIXAS = 5\n",
    "\n",
    "logger.info(\"Modelo: %s (%s)\", MODEL_NAME, MODEL_STAGE)\n",
    "logger.info(\"SAFRAs para scoring: %s\", SCORING_SAFRAS)\n",
    "logger.info(\"Output: %s\", PATH_SCORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SPARK CONFIG\n",
    "# =============================================================================\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", str(SPARK_BROADCAST_THRESHOLD))\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", str(SPARK_AQE_ENABLED).lower())\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", str(SPARK_SHUFFLE_PARTITIONS))\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "logger.info(\"Spark config OK (AQE=%s, shuffle=%d)\", SPARK_AQE_ENABLED, SPARK_SHUFFLE_PARTITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CARREGAR MODELO DO MLFLOW REGISTRY\n",
    "# =============================================================================\n",
    "client = MlflowClient()\n",
    "\n",
    "# Buscar versao do modelo no stage especificado\n",
    "model_versions = client.get_latest_versions(MODEL_NAME, stages=[MODEL_STAGE])\n",
    "if not model_versions:\n",
    "    raise RuntimeError(\n",
    "        f\"Nenhuma versao encontrada para '{MODEL_NAME}' no stage '{MODEL_STAGE}'. \"\n",
    "        f\"Registre o modelo primeiro via export_model.py\"\n",
    "    )\n",
    "\n",
    "mv = model_versions[0]\n",
    "logger.info(\"Modelo encontrado: %s v%s (run_id=%s)\", mv.name, mv.version, mv.run_id)\n",
    "\n",
    "# Carregar modelo como pyfunc (funciona com sklearn e lightgbm)\n",
    "model_uri = f\"models:/{MODEL_NAME}/{MODEL_STAGE}\"\n",
    "model = mlflow.pyfunc.load_model(model_uri)\n",
    "logger.info(\"Modelo carregado: %s\", model_uri)\n",
    "\n",
    "# Recuperar lista de features do run original\n",
    "run = client.get_run(mv.run_id)\n",
    "artifacts_path = client.download_artifacts(mv.run_id, \"\")\n",
    "\n",
    "# Tentar carregar metadata JSON (gerado pelo export_model.py)\n",
    "import glob\n",
    "metadata_files = glob.glob(f\"{artifacts_path}/*metadata*.json\")\n",
    "if metadata_files:\n",
    "    with open(metadata_files[0]) as f:\n",
    "        model_metadata = json.load(f)\n",
    "    FEATURE_NAMES = model_metadata[\"feature_names\"]\n",
    "    logger.info(\"Features carregadas do metadata: %d features\", len(FEATURE_NAMES))\n",
    "else:\n",
    "    logger.warning(\"Metadata JSON nao encontrado — usando features do run param\")\n",
    "    n_features = int(run.data.params.get(\"n_features\", 0))\n",
    "    raise RuntimeError(\n",
    "        \"Feature names nao disponiveis. Execute export_model.py primeiro para gerar metadata.\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nModelo: {MODEL_NAME} v{mv.version}\")\n",
    "print(f\"Features: {len(FEATURE_NAMES)}\")\n",
    "print(f\"Primeiras 10: {FEATURE_NAMES[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CARREGAR FEATURE STORE\n",
    "# =============================================================================\n",
    "safra_list = \", \".join(str(s) for s in SCORING_SAFRAS)\n",
    "df_feature_store = spark.read.format(\"delta\").load(PATH_FEATURE_STORE) \\\n",
    "    .filter(f\"SAFRA IN ({safra_list})\")\n",
    "\n",
    "total_records = df_feature_store.count()\n",
    "logger.info(\"Feature store carregada: %d registros para SAFRAs %s\", total_records, SCORING_SAFRAS)\n",
    "\n",
    "if total_records == 0:\n",
    "    raise RuntimeError(f\"Nenhum registro encontrado para SAFRAs {SCORING_SAFRAS} no feature store\")\n",
    "\n",
    "# Verificar que todas as features existem\n",
    "available_cols = set(df_feature_store.columns)\n",
    "missing_features = [f for f in FEATURE_NAMES if f not in available_cols]\n",
    "if missing_features:\n",
    "    raise RuntimeError(\n",
    "        f\"{len(missing_features)} features ausentes no feature store: {missing_features[:10]}\"\n",
    "    )\n",
    "logger.info(\"Todas as %d features encontradas no feature store\", len(FEATURE_NAMES))\n",
    "\n",
    "# Volumetria por SAFRA\n",
    "df_feature_store.groupBy(\"SAFRA\").count().orderBy(\"SAFRA\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SCORING — por SAFRA (para controle de memoria)\n",
    "# =============================================================================\n",
    "all_scores = []\n",
    "\n",
    "for safra in SCORING_SAFRAS:\n",
    "    logger.info(\"Scoring SAFRA %d...\", safra)\n",
    "\n",
    "    # Filtrar SAFRA\n",
    "    df_safra = df_feature_store.filter(f\"SAFRA = {safra}\")\n",
    "\n",
    "    # Selecionar chaves + features\n",
    "    df_keys = df_safra.select(\"NUM_CPF\", \"SAFRA\").toPandas()\n",
    "    df_X = df_safra.select(FEATURE_NAMES).toPandas()\n",
    "\n",
    "    # Tratar missing (mesmo tratamento do treino — fillna 0 para numericos)\n",
    "    for col in df_X.columns:\n",
    "        if df_X[col].dtype in [\"float64\", \"float32\", \"int64\", \"int32\"]:\n",
    "            df_X[col] = df_X[col].fillna(0)\n",
    "        else:\n",
    "            df_X[col] = df_X[col].fillna(\"MISSING\")\n",
    "\n",
    "    # Predizer probabilidade da classe positiva (FPD=1)\n",
    "    scores = model.predict(df_X)\n",
    "\n",
    "    # Se o modelo retorna classes (0/1) em vez de probabilidades,\n",
    "    # usar predict_proba via modelo unwrapped\n",
    "    if hasattr(model, '_model_impl'):\n",
    "        inner = model._model_impl\n",
    "        if hasattr(inner, 'predict_proba'):\n",
    "            scores = inner.predict_proba(df_X)[:, 1]\n",
    "        else:\n",
    "            scores = inner.predict(df_X)\n",
    "\n",
    "    # Montar DataFrame de saida\n",
    "    df_result = df_keys.copy()\n",
    "    df_result[\"SCORE_PROB\"] = scores.astype(float)\n",
    "\n",
    "    # Score invertido (menor = melhor, padrao mercado credito)\n",
    "    df_result[\"SCORE\"] = (1000 * (1 - df_result[\"SCORE_PROB\"])).round(0).astype(int)\n",
    "\n",
    "    # Faixa de risco por quintil (1=menor risco, 5=maior risco)\n",
    "    df_result[\"FAIXA_RISCO\"] = pd.qcut(\n",
    "        df_result[\"SCORE_PROB\"],\n",
    "        q=N_FAIXAS,\n",
    "        labels=[i for i in range(1, N_FAIXAS + 1)],\n",
    "        duplicates=\"drop\"\n",
    "    ).astype(int)\n",
    "\n",
    "    df_result[\"MODEL_NAME\"] = MODEL_NAME\n",
    "    df_result[\"MODEL_VERSION\"] = str(mv.version)\n",
    "    df_result[\"DT_SCORING\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    all_scores.append(df_result)\n",
    "    logger.info(\"  SAFRA %d: %d registros scored (score medio=%.0f)\",\n",
    "                safra, len(df_result), df_result[\"SCORE\"].mean())\n",
    "\n",
    "# Consolidar\n",
    "df_all_scores = pd.concat(all_scores, ignore_index=True)\n",
    "logger.info(\"Total scored: %d registros\", len(df_all_scores))\n",
    "\n",
    "print(f\"\\nDistribuicao de scores:\")\n",
    "print(df_all_scores[\"SCORE\"].describe())\n",
    "print(f\"\\nDistribuicao por faixa de risco:\")\n",
    "print(df_all_scores.groupby([\"SAFRA\", \"FAIXA_RISCO\"]).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SALVAR SCORES NO GOLD LAKEHOUSE (Delta)\n",
    "# =============================================================================\n",
    "schema = StructType([\n",
    "    StructField(\"NUM_CPF\", StringType(), True),\n",
    "    StructField(\"SAFRA\", IntegerType(), True),\n",
    "    StructField(\"SCORE_PROB\", DoubleType(), True),\n",
    "    StructField(\"SCORE\", IntegerType(), True),\n",
    "    StructField(\"FAIXA_RISCO\", IntegerType(), True),\n",
    "    StructField(\"MODEL_NAME\", StringType(), True),\n",
    "    StructField(\"MODEL_VERSION\", StringType(), True),\n",
    "    StructField(\"DT_SCORING\", StringType(), True),\n",
    "])\n",
    "\n",
    "df_spark_scores = spark.createDataFrame(df_all_scores, schema=schema)\n",
    "\n",
    "for safra in SCORING_SAFRAS:\n",
    "    df_safra = df_spark_scores.filter(f\"SAFRA = {safra}\")\n",
    "    df_safra.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"replaceWhere\", f\"SAFRA = {safra}\") \\\n",
    "        .partitionBy(\"SAFRA\") \\\n",
    "        .save(PATH_SCORES)\n",
    "    logger.info(\"SAFRA %d escrita em %s\", safra, PATH_SCORES)\n",
    "\n",
    "# Validacao pos-escrita\n",
    "df_check = spark.read.format(\"delta\").load(PATH_SCORES)\n",
    "print(f\"\\nValidacao pos-escrita:\")\n",
    "df_check.groupBy(\"SAFRA\").count().orderBy(\"SAFRA\").show()\n",
    "print(f\"Total: {df_check.count()} registros, {len(df_check.columns)} colunas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOG DE SCORING NO MLFLOW\n",
    "# =============================================================================\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "with mlflow.start_run(run_name=f\"scoring_batch_{datetime.now().strftime('%Y%m%d_%H%M')}\"):\n",
    "    mlflow.log_param(\"model_name\", MODEL_NAME)\n",
    "    mlflow.log_param(\"model_version\", mv.version)\n",
    "    mlflow.log_param(\"model_stage\", MODEL_STAGE)\n",
    "    mlflow.log_param(\"scoring_safras\", str(SCORING_SAFRAS))\n",
    "    mlflow.log_param(\"n_features\", len(FEATURE_NAMES))\n",
    "    mlflow.log_param(\"output_path\", PATH_SCORES)\n",
    "\n",
    "    mlflow.log_metric(\"total_records_scored\", len(df_all_scores))\n",
    "    mlflow.log_metric(\"score_mean\", float(df_all_scores[\"SCORE\"].mean()))\n",
    "    mlflow.log_metric(\"score_std\", float(df_all_scores[\"SCORE\"].std()))\n",
    "    mlflow.log_metric(\"score_prob_mean\", float(df_all_scores[\"SCORE_PROB\"].mean()))\n",
    "\n",
    "    for safra in SCORING_SAFRAS:\n",
    "        mask = df_all_scores[\"SAFRA\"] == safra\n",
    "        mlflow.log_metric(f\"records_safra_{safra}\", int(mask.sum()))\n",
    "        mlflow.log_metric(f\"score_mean_safra_{safra}\", float(df_all_scores.loc[mask, \"SCORE\"].mean()))\n",
    "\n",
    "    # Salvar distribuicao como artefato CSV\n",
    "    dist_path = \"/tmp/scoring_distribution.csv\"\n",
    "    df_all_scores.groupby([\"SAFRA\", \"FAIXA_RISCO\"]).agg(\n",
    "        count=(\"NUM_CPF\", \"count\"),\n",
    "        score_mean=(\"SCORE\", \"mean\"),\n",
    "        score_prob_mean=(\"SCORE_PROB\", \"mean\"),\n",
    "    ).reset_index().to_csv(dist_path, index=False)\n",
    "    mlflow.log_artifact(dist_path)\n",
    "\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    logger.info(\"MLflow scoring run: %s\", run_id)\n",
    "\n",
    "print(f\"\\nScoring batch concluido com sucesso!\")\n",
    "print(f\"MLflow Run ID: {run_id}\")\n",
    "print(f\"Output: {PATH_SCORES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo\n",
    "\n",
    "| Item | Valor |\n",
    "|------|-------|\n",
    "| Modelo | `credit-risk-fpd-lgbm_baseline` |\n",
    "| Stage | Production |\n",
    "| Output | `Gold.feature_store.clientes_scores` |\n",
    "| Particionado por | SAFRA |\n",
    "| Colunas output | NUM_CPF, SAFRA, SCORE_PROB, SCORE, FAIXA_RISCO, MODEL_NAME, MODEL_VERSION, DT_SCORING |\n",
    "\n",
    "**Proximos passos**: Executar `validacao_deploy.py` para confirmar que metricas do scoring == metricas da avaliacao."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}