{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "%%configure\n",
    "\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.driver.maxResultSize\": \"8g\",\n",
    "        \"spark.driver.memory\": \"54g\",\n",
    "        \"spark.driver.cores\": 8,\n",
    "        \"spark.executor.instances\": 0,\n",
    "        \"spark.sql.execution.arrow.pyspark.enabled\": \"true\",\n",
    "        \"spark.sql.execution.arrow.pyspark.selfDestruct.enabled\": \"true\"\n",
    "    }\n",
    "}"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Instalacao de pacote para uso de Target/CountEncoder\n",
    "!pip install category-encoders==2.6.3"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Batch Pipeline - Credit Risk FPD\n",
    "\n",
    "**Story**: HD-3.2 / Fase 4.3 — Deploy\n",
    "**Objetivo**: Carregar modelo do MLflow Registry e aplicar scoring sobre novas SAFRAs\n",
    "**Output**: `Gold.feature_store.clientes_scores` (particionado por SAFRA)\n",
    "\n",
    "**Uso**: Parametrizar `SCORING_SAFRAS` e executar todas as celulas sequencialmente."
   ],
   "id": "f85c38d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/lakehouse/default/Files/projeto-final\")\n",
    "\n",
    "import glob  # M4: moved from cell-4 to top-level for consistency\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types as ptypes\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, StructType, StructField\n",
    "\n",
    "from config.pipeline_config import (\n",
    "    PATH_FEATURE_STORE, GOLD_BASE, EXPERIMENT_NAME,\n",
    "    TARGET_COLUMNS, REGISTERED_MODEL_NAME,\n",
    "    SPARK_BROADCAST_THRESHOLD, SPARK_SHUFFLE_PARTITIONS, SPARK_AQE_ENABLED,\n",
    ")\n",
    "\n",
    "# FIX: sklearn >= 1.6 renamed force_all_finite -> ensure_all_finite\n",
    "# LightGBM sklearn wrapper still uses old name, causing TypeError on predict_proba\n",
    "import lightgbm.sklearn as _lgbm_sklearn\n",
    "_orig_check = _lgbm_sklearn._LGBMCheckArray\n",
    "def _patched_lgbm_check(*args, **kwargs):\n",
    "    kwargs.pop('force_all_finite', None)\n",
    "    kwargs.pop('ensure_all_finite', None)\n",
    "    return _orig_check(*args, **kwargs)\n",
    "_lgbm_sklearn._LGBMCheckArray = _patched_lgbm_check\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\", datefmt=\"%H:%M:%S\")\n",
    "logger = logging.getLogger(\"scoring_batch\")\n",
    "\n",
    "SCORE_SCALE = 1000  # Credit score range: 0-1000\n",
    "\n",
    "logger.info(\"Imports OK (LightGBM check_array patched)\")"
   ],
   "id": "efb19edc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PARAMETROS DE SCORING — ajustar antes de executar\n",
    "# =============================================================================\n",
    "\n",
    "# M1-FIX: Modelo centralizado no config\n",
    "MODEL_NAME = REGISTERED_MODEL_NAME\n",
    "MODEL_STAGE = \"Production\"  # ou \"Staging\" para testes\n",
    "\n",
    "# SAFRAs para scoring (lista de int YYYYMM)\n",
    "SCORING_SAFRAS = [202502, 202503]\n",
    "\n",
    "# Validar formato SAFRA\n",
    "for safra in SCORING_SAFRAS:\n",
    "    if not isinstance(safra, int):\n",
    "        raise ValueError(f\"SAFRA deve ser int, got {type(safra)}: {safra}\")\n",
    "    y, m = divmod(safra, 100)\n",
    "    if not (1 <= m <= 12):\n",
    "        raise ValueError(f\"SAFRA invalida {safra}: mes {m} fora de 1-12\")\n",
    "\n",
    "# Output path\n",
    "SCHEMA_SCORES = \"feature_store\"\n",
    "TABLE_SCORES = \"clientes_scores\"\n",
    "PATH_SCORES = f\"{GOLD_BASE}/Tables/{SCHEMA_SCORES}/{TABLE_SCORES}\"\n",
    "\n",
    "# Faixas de risco (quintis)\n",
    "N_FAIXAS = 5\n",
    "\n",
    "logger.info(\"Modelo: %s (%s)\", MODEL_NAME, MODEL_STAGE)\n",
    "logger.info(\"SAFRAs para scoring: %s\", SCORING_SAFRAS)\n",
    "logger.info(\"Output: %s\", PATH_SCORES)"
   ],
   "id": "f1932df3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SPARK CONFIG\n",
    "# =============================================================================\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", str(SPARK_BROADCAST_THRESHOLD))\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", str(SPARK_AQE_ENABLED).lower())\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", str(SPARK_SHUFFLE_PARTITIONS))\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "logger.info(\"Spark config OK (AQE=%s, shuffle=%d)\", SPARK_AQE_ENABLED, SPARK_SHUFFLE_PARTITIONS)"
   ],
   "id": "0bbf0792"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CARREGAR MODELO DO MLFLOW REGISTRY\n",
    "# =============================================================================\n",
    "client = MlflowClient()\n",
    "\n",
    "# Buscar versao do modelo no stage especificado\n",
    "model_versions = client.get_latest_versions(MODEL_NAME, stages=[MODEL_STAGE])\n",
    "if not model_versions:\n",
    "    raise RuntimeError(\n",
    "        f\"Nenhuma versao encontrada para '{MODEL_NAME}' no stage '{MODEL_STAGE}'. \"\n",
    "        f\"Registre o modelo primeiro via export_model.py\"\n",
    "    )\n",
    "\n",
    "mv = model_versions[0]\n",
    "logger.info(\"Modelo encontrado: %s v%s (run_id=%s)\", mv.name, mv.version, mv.run_id)\n",
    "\n",
    "# L2-FIX: Carregar como sklearn (acesso direto a predict_proba)\n",
    "model_uri = f\"models:/{MODEL_NAME}/{MODEL_STAGE}\"\n",
    "model = mlflow.sklearn.load_model(model_uri)\n",
    "logger.info(\"Modelo carregado (sklearn): %s\", model_uri)\n",
    "\n",
    "# FIX: Patch SimpleImputer._fill_dtype para compatibilidade sklearn >=1.4\n",
    "# O modelo pode ter sido serializado com sklearn <1.4 onde _fill_dtype nao existia.\n",
    "# sklearn >=1.4 espera esse atributo em transform().\n",
    "from sklearn.impute import SimpleImputer as _SI\n",
    "\n",
    "def _patch_fill_dtype(obj):\n",
    "    \"\"\"Recursively patch _fill_dtype on all SimpleImputer instances in a pipeline.\"\"\"\n",
    "    if isinstance(obj, _SI) and not hasattr(obj, '_fill_dtype'):\n",
    "        if hasattr(obj, 'statistics_'):\n",
    "            obj._fill_dtype = obj.statistics_.dtype\n",
    "    if hasattr(obj, 'steps'):\n",
    "        for _, step in obj.steps:\n",
    "            _patch_fill_dtype(step)\n",
    "    if hasattr(obj, 'transformers_'):\n",
    "        for _, transformer, _ in obj.transformers_:\n",
    "            _patch_fill_dtype(transformer)\n",
    "\n",
    "_patch_fill_dtype(model)\n",
    "logger.info(\"SimpleImputer._fill_dtype patched (sklearn compat)\")\n",
    "\n",
    "# Recuperar lista de features do run original\n",
    "run = client.get_run(mv.run_id)\n",
    "artifacts_path = client.download_artifacts(mv.run_id, \"\")\n",
    "\n",
    "# Tentar carregar metadata JSON (gerado pelo export_model.py)\n",
    "metadata_files = glob.glob(f\"{artifacts_path}/*metadata*.json\")\n",
    "if metadata_files:\n",
    "    with open(metadata_files[0]) as f:\n",
    "        model_metadata = json.load(f)\n",
    "    FEATURE_NAMES = model_metadata[\"feature_names\"]\n",
    "    logger.info(\"Features carregadas do metadata: %d features\", len(FEATURE_NAMES))\n",
    "else:\n",
    "    logger.warning(\"Metadata JSON nao encontrado — usando features do run param\")\n",
    "    n_features = int(run.data.params.get(\"n_features\", 0))\n",
    "    raise RuntimeError(\n",
    "        \"Feature names nao disponiveis. Execute export_model.py primeiro para gerar metadata.\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nModelo: {MODEL_NAME} v{mv.version}\")\n",
    "print(f\"Features: {len(FEATURE_NAMES)}\")\n",
    "print(f\"Primeiras 10: {FEATURE_NAMES[:10]}\")"
   ],
   "id": "1b60fb1a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CARREGAR FEATURE STORE (C1: excluir target, H2: usar F.col, H5: schema check)\n",
    "# =============================================================================\n",
    "df_feature_store = spark.read.format(\"delta\").load(PATH_FEATURE_STORE)\n",
    "\n",
    "# FIX: Filtrar apenas clientes aprovados (mesma populacao do treino v5)\n",
    "if \"FLAG_INSTALACAO\" in df_feature_store.columns:\n",
    "    n_total_fs = df_feature_store.count()\n",
    "    df_feature_store = df_feature_store.filter(F.col(\"FLAG_INSTALACAO\") == 1)\n",
    "    n_aprovados_fs = df_feature_store.count()\n",
    "    logger.info(\"FLAG_INSTALACAO filter: %d -> %d (%d reprovados removidos)\",\n",
    "                n_total_fs, n_aprovados_fs, n_total_fs - n_aprovados_fs)\n",
    "\n",
    "# Validar tipo da coluna SAFRA\n",
    "safra_type = dict(df_feature_store.dtypes).get(\"SAFRA\")\n",
    "if safra_type not in (\"int\", \"bigint\"):\n",
    "    logger.warning(\"SAFRA type is '%s' (expected int) — cast may be needed\", safra_type)\n",
    "\n",
    "# Filtrar SAFRAs usando API parametrizada (H2)\n",
    "df_feature_store = df_feature_store.filter(F.col(\"SAFRA\").isin(SCORING_SAFRAS))\n",
    "\n",
    "total_records = df_feature_store.count()\n",
    "logger.info(\"Feature store carregada: %d registros para SAFRAs %s\", total_records, SCORING_SAFRAS)\n",
    "\n",
    "if total_records == 0:\n",
    "    raise RuntimeError(f\"Nenhum registro encontrado para SAFRAs {SCORING_SAFRAS} no feature store\")\n",
    "\n",
    "# C1: Excluir apenas FPD (target real) do feature store.\n",
    "# TARGET_SCORE_01/02 sao scores de bureau (input legitimo), nao derivados de FPD.\n",
    "PREDICTION_TARGET = \"FPD\"\n",
    "if PREDICTION_TARGET in df_feature_store.columns:\n",
    "    df_feature_store = df_feature_store.drop(PREDICTION_TARGET)\n",
    "    logger.info(\"Target '%s' removido do DataFrame\", PREDICTION_TARGET)\n",
    "if PREDICTION_TARGET in FEATURE_NAMES:\n",
    "    raise RuntimeError(f\"FEATURE_NAMES contem target de predicao (leakage!): {PREDICTION_TARGET}\")\n",
    "\n",
    "# Verificar que todas as features existem\n",
    "available_cols = set(df_feature_store.columns)\n",
    "missing_features = [f for f in FEATURE_NAMES if f not in available_cols]\n",
    "if missing_features:\n",
    "    logger.error(\"Missing features (%d): %s\", len(missing_features), missing_features)\n",
    "    raise RuntimeError(f\"{len(missing_features)} features ausentes no feature store\")\n",
    "logger.info(\"Todas as %d features encontradas no feature store\", len(FEATURE_NAMES))\n",
    "\n",
    "# Volumetria por SAFRA\n",
    "df_feature_store.groupBy(\"SAFRA\").count().orderBy(\"SAFRA\").show()"
   ],
   "id": "185b761d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SCORING — por SAFRA (H3: output validation, M1: ptypes, M3: qcut fallback)\n",
    "# =============================================================================\n",
    "all_scores = []\n",
    "\n",
    "for safra in SCORING_SAFRAS:\n",
    "    logger.info(\"Scoring SAFRA %d...\", safra)\n",
    "\n",
    "    # Filtrar SAFRA usando API parametrizada\n",
    "    df_safra = df_feature_store.filter(F.col(\"SAFRA\") == safra)\n",
    "\n",
    "    # Selecionar chaves + features\n",
    "    # H-NEW-2: .copy() para evitar SettingWithCopyWarning ao modificar com fillna\n",
    "    df_keys = df_safra.select(\"NUM_CPF\", \"SAFRA\").toPandas().copy()\n",
    "    df_X = df_safra.select(FEATURE_NAMES).toPandas().copy()\n",
    "\n",
    "    # M1: Tratar missing usando pandas type API\n",
    "    for col in df_X.columns:\n",
    "        if ptypes.is_numeric_dtype(df_X[col]):\n",
    "            df_X[col] = df_X[col].fillna(0)\n",
    "        else:\n",
    "            df_X[col] = df_X[col].fillna(\"MISSING\")\n",
    "\n",
    "    # L2-FIX: Com sklearn.load_model, predict_proba acessivel diretamente\n",
    "    try:\n",
    "        raw_scores = model.predict_proba(df_X)\n",
    "        if raw_scores.ndim == 2 and raw_scores.shape[1] == 2:\n",
    "            scores = raw_scores[:, 1]\n",
    "        else:\n",
    "            raise RuntimeError(f\"predict_proba shape inesperado: {raw_scores.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Predicao falhou para SAFRA %d: %s\", safra, e)\n",
    "        logger.error(\"X shape: %s\", df_X.shape)\n",
    "        raise\n",
    "\n",
    "    scores = np.asarray(scores, dtype=float)\n",
    "    if len(scores) != len(df_X):\n",
    "        raise RuntimeError(f\"Score count mismatch: {len(scores)} != {len(df_X)}\")\n",
    "    if not np.all(np.isfinite(scores)):\n",
    "        n_invalid = (~np.isfinite(scores)).sum()\n",
    "        logger.warning(\"  %d scores nao-finitos — clipping to [0, 1]\", n_invalid)\n",
    "        scores = np.clip(np.nan_to_num(scores, nan=0.5), 0, 1)\n",
    "    if np.any((scores < 0) | (scores > 1)):\n",
    "        logger.warning(\"  Scores fora de [0, 1] — clipping\")\n",
    "        scores = np.clip(scores, 0, 1)\n",
    "\n",
    "    logger.info(\"  Scores range: [%.4f, %.4f]\", scores.min(), scores.max())\n",
    "\n",
    "    # Montar DataFrame de saida\n",
    "    df_result = df_keys.copy()\n",
    "    df_result[\"SCORE_PROB\"] = scores\n",
    "\n",
    "    # Score invertido (menor = melhor, padrao mercado credito)\n",
    "    df_result[\"SCORE\"] = (SCORE_SCALE * (1 - df_result[\"SCORE_PROB\"])).round(0).astype(int)\n",
    "\n",
    "    # M3: Faixa de risco por quintil com fallback\n",
    "    try:\n",
    "        df_result[\"FAIXA_RISCO\"] = pd.qcut(\n",
    "            df_result[\"SCORE_PROB\"],\n",
    "            q=N_FAIXAS,\n",
    "            labels=list(range(1, N_FAIXAS + 1)),\n",
    "            duplicates=\"drop\"\n",
    "        ).astype(int)\n",
    "        n_bins = df_result[\"FAIXA_RISCO\"].nunique()\n",
    "        if n_bins < N_FAIXAS:\n",
    "            logger.warning(\"  SAFRA %d: qcut produced %d bins (expected %d)\", safra, n_bins, N_FAIXAS)\n",
    "    except ValueError:\n",
    "        logger.warning(\"  SAFRA %d: qcut failed — using rank-based binning\", safra)\n",
    "        df_result[\"FAIXA_RISCO\"] = pd.cut(\n",
    "            df_result[\"SCORE_PROB\"].rank(pct=True),\n",
    "            bins=N_FAIXAS,\n",
    "            labels=list(range(1, N_FAIXAS + 1))\n",
    "        ).astype(int)\n",
    "\n",
    "    df_result[\"MODEL_NAME\"] = MODEL_NAME\n",
    "    df_result[\"MODEL_VERSION\"] = str(mv.version)\n",
    "    df_result[\"DT_SCORING\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    all_scores.append(df_result)\n",
    "    logger.info(\"  SAFRA %d: %d registros scored (score medio=%.0f)\",\n",
    "                safra, len(df_result), df_result[\"SCORE\"].mean())\n",
    "\n",
    "# Consolidar\n",
    "df_all_scores = pd.concat(all_scores, ignore_index=True)\n",
    "logger.info(\"Total scored: %d registros\", len(df_all_scores))\n",
    "\n",
    "logger.info(\"Distribuicao de scores:\")\n",
    "logger.info(\"\\n%s\", df_all_scores[\"SCORE\"].describe())\n",
    "logger.info(\"Distribuicao por faixa de risco:\")\n",
    "logger.info(\"\\n%s\", df_all_scores.groupby([\"SAFRA\", \"FAIXA_RISCO\"]).size().unstack(fill_value=0))"
   ],
   "id": "1be151fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SALVAR SCORES (H4: single dynamic write, M5: enhanced validation)\n",
    "# =============================================================================\n",
    "schema = StructType([\n",
    "    StructField(\"NUM_CPF\", StringType(), True),\n",
    "    StructField(\"SAFRA\", IntegerType(), True),\n",
    "    StructField(\"SCORE_PROB\", DoubleType(), True),\n",
    "    StructField(\"SCORE\", IntegerType(), True),\n",
    "    StructField(\"FAIXA_RISCO\", IntegerType(), True),\n",
    "    StructField(\"MODEL_NAME\", StringType(), True),\n",
    "    StructField(\"MODEL_VERSION\", StringType(), True),\n",
    "    StructField(\"DT_SCORING\", StringType(), True),\n",
    "])\n",
    "\n",
    "df_spark_scores = spark.createDataFrame(df_all_scores, schema=schema)\n",
    "\n",
    "# H4: Single write com dynamic partition overwrite (idempotente, sem race condition)\n",
    "df_spark_scores.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"partitionOverwriteMode\", \"dynamic\") \\\n",
    "    .partitionBy(\"SAFRA\") \\\n",
    "    .save(PATH_SCORES)\n",
    "logger.info(\"Scores escritos em %s\", PATH_SCORES)\n",
    "\n",
    "# M5: Validacao pos-escrita aprimorada\n",
    "df_check = spark.read.format(\"delta\").load(PATH_SCORES)\n",
    "\n",
    "# Check 1: Count por SAFRA\n",
    "logger.info(\"Validacao pos-escrita:\")\n",
    "df_check.groupBy(\"SAFRA\").count().orderBy(\"SAFRA\").show()\n",
    "\n",
    "# Check 2: Todas as SAFRAs presentes\n",
    "written_safras = set(r[0] for r in df_check.select(\"SAFRA\").distinct().collect())\n",
    "expected_safras = set(SCORING_SAFRAS)\n",
    "missing_safras = expected_safras - written_safras\n",
    "if missing_safras:\n",
    "    raise RuntimeError(f\"SAFRAs nao escritas: {missing_safras}\")\n",
    "\n",
    "# Check 3: Duplicatas\n",
    "dup_count = df_check.filter(F.col(\"SAFRA\").isin(SCORING_SAFRAS)) \\\n",
    "    .groupBy(\"NUM_CPF\", \"SAFRA\").count().filter(\"count > 1\").count()\n",
    "if dup_count > 0:\n",
    "    raise RuntimeError(f\"Encontradas {dup_count} duplicatas (NUM_CPF, SAFRA)\")\n",
    "\n",
    "# Check 4: Score ranges validos — H3: raise RuntimeError instead of warning\n",
    "# Invalid scores in Gold indicate upstream corruption that must be investigated\n",
    "invalid_scores = df_check.filter(F.col(\"SAFRA\").isin(SCORING_SAFRAS)).filter(\n",
    "    (F.col(\"SCORE\") < 0) | (F.col(\"SCORE\") > SCORE_SCALE) |\n",
    "    (F.col(\"SCORE_PROB\") < 0) | (F.col(\"SCORE_PROB\") > 1)\n",
    ").count()\n",
    "if invalid_scores > 0:\n",
    "    raise RuntimeError(\n",
    "        f\"Encontrados {invalid_scores} registros com scores fora do range valido \"\n",
    "        f\"(SCORE: 0-{SCORE_SCALE}, SCORE_PROB: 0-1). Dados corrompidos escritos no Gold — \"\n",
    "        f\"investigue o pipeline de scoring antes de prosseguir.\"\n",
    "    )\n",
    "\n",
    "total = df_check.filter(F.col(\"SAFRA\").isin(SCORING_SAFRAS)).count()\n",
    "logger.info(\"Validacao OK: %d registros, %d colunas, 0 duplicatas, 0 invalidos\", total, len(df_check.columns))"
   ],
   "id": "3acd3d23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOG DE SCORING NO MLFLOW\n",
    "# =============================================================================\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "with mlflow.start_run(run_name=f\"scoring_batch_{datetime.now().strftime('%Y%m%d_%H%M')}\"):\n",
    "    mlflow.log_param(\"model_name\", MODEL_NAME)\n",
    "    mlflow.log_param(\"model_version\", mv.version)\n",
    "    mlflow.log_param(\"model_stage\", MODEL_STAGE)\n",
    "    mlflow.log_param(\"scoring_safras\", str(SCORING_SAFRAS))\n",
    "    mlflow.log_param(\"n_features\", len(FEATURE_NAMES))\n",
    "    mlflow.log_param(\"output_path\", PATH_SCORES)\n",
    "\n",
    "    mlflow.log_metric(\"total_records_scored\", len(df_all_scores))\n",
    "    mlflow.log_metric(\"score_mean\", float(df_all_scores[\"SCORE\"].mean()))\n",
    "    mlflow.log_metric(\"score_std\", float(df_all_scores[\"SCORE\"].std()))\n",
    "    mlflow.log_metric(\"score_prob_mean\", float(df_all_scores[\"SCORE_PROB\"].mean()))\n",
    "\n",
    "    for safra in SCORING_SAFRAS:\n",
    "        mask = df_all_scores[\"SAFRA\"] == safra\n",
    "        mlflow.log_metric(f\"records_safra_{safra}\", int(mask.sum()))\n",
    "        mlflow.log_metric(f\"score_mean_safra_{safra}\", float(df_all_scores.loc[mask, \"SCORE\"].mean()))\n",
    "\n",
    "    # Salvar distribuicao como artefato CSV\n",
    "    dist_path = \"/tmp/scoring_distribution.csv\"\n",
    "    df_all_scores.groupby([\"SAFRA\", \"FAIXA_RISCO\"]).agg(\n",
    "        count=(\"NUM_CPF\", \"count\"),\n",
    "        score_mean=(\"SCORE\", \"mean\"),\n",
    "        score_prob_mean=(\"SCORE_PROB\", \"mean\"),\n",
    "    ).reset_index().to_csv(dist_path, index=False)\n",
    "    mlflow.log_artifact(dist_path)\n",
    "\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    logger.info(\"MLflow scoring run: %s\", run_id)\n",
    "\n",
    "print(f\"\\nScoring batch concluido com sucesso!\")\n",
    "print(f\"MLflow Run ID: {run_id}\")\n",
    "print(f\"Output: {PATH_SCORES}\")"
   ],
   "id": "02720dd4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo\n",
    "\n",
    "| Item | Valor |\n",
    "|------|-------|\n",
    "| Modelo | `credit-risk-fpd-lgbm_baseline` |\n",
    "| Stage | Production |\n",
    "| Output | `Gold.feature_store.clientes_scores` |\n",
    "| Particionado por | SAFRA |\n",
    "| Colunas output | NUM_CPF, SAFRA, SCORE_PROB, SCORE, FAIXA_RISCO, MODEL_NAME, MODEL_VERSION, DT_SCORING |\n",
    "\n",
    "**Proximos passos**: Executar `validacao_deploy.py` para confirmar que metricas do scoring == metricas da avaliacao."
   ],
   "id": "ceb56ae1"
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  },
  "microsoft": {
   "language": "python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}