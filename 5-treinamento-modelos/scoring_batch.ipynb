{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Batch Pipeline - Credit Risk FPD\n",
    "\n",
    "**Story**: HD-3.2 / Fase 4.3 — Deploy\n",
    "**Objetivo**: Carregar modelo do MLflow Registry e aplicar scoring sobre novas SAFRAs\n",
    "**Output**: `Gold.feature_store.clientes_scores` (particionado por SAFRA)\n",
    "\n",
    "**Uso**: Parametrizar `SCORING_SAFRAS` e executar todas as celulas sequencialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, \"/lakehouse/default/Files/projeto-final\")\n\nimport logging\nimport json\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\nimport pandas.api.types as ptypes\nimport mlflow\nfrom mlflow.tracking import MlflowClient\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import DoubleType, IntegerType, StringType, StructType, StructField\n\nfrom config.pipeline_config import (\n    PATH_FEATURE_STORE, GOLD_BASE, EXPERIMENT_NAME,\n    TARGET_COLUMNS,\n    SPARK_BROADCAST_THRESHOLD, SPARK_SHUFFLE_PARTITIONS, SPARK_AQE_ENABLED,\n)\n\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\", datefmt=\"%H:%M:%S\")\nlogger = logging.getLogger(\"scoring_batch\")\n\nSCORE_SCALE = 1000  # Credit score range: 0-1000\n\nlogger.info(\"Imports OK\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# PARAMETROS DE SCORING — ajustar antes de executar\n# =============================================================================\n\n# Modelo a usar (nome registrado no MLflow Model Registry)\nMODEL_NAME = \"credit-risk-fpd-lgbm_baseline\"\nMODEL_STAGE = \"Production\"  # ou \"Staging\" para testes\n\n# SAFRAs para scoring (lista de int YYYYMM)\nSCORING_SAFRAS = [202502, 202503]\n\n# Validar formato SAFRA\nfor safra in SCORING_SAFRAS:\n    if not isinstance(safra, int):\n        raise ValueError(f\"SAFRA deve ser int, got {type(safra)}: {safra}\")\n    y, m = divmod(safra, 100)\n    if not (1 <= m <= 12):\n        raise ValueError(f\"SAFRA invalida {safra}: mes {m} fora de 1-12\")\n\n# Output path\nSCHEMA_SCORES = \"feature_store\"\nTABLE_SCORES = \"clientes_scores\"\nPATH_SCORES = f\"{GOLD_BASE}/Tables/{SCHEMA_SCORES}/{TABLE_SCORES}\"\n\n# Faixas de risco (quintis)\nN_FAIXAS = 5\n\nlogger.info(\"Modelo: %s (%s)\", MODEL_NAME, MODEL_STAGE)\nlogger.info(\"SAFRAs para scoring: %s\", SCORING_SAFRAS)\nlogger.info(\"Output: %s\", PATH_SCORES)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SPARK CONFIG\n",
    "# =============================================================================\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", str(SPARK_BROADCAST_THRESHOLD))\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", str(SPARK_AQE_ENABLED).lower())\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", str(SPARK_SHUFFLE_PARTITIONS))\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "logger.info(\"Spark config OK (AQE=%s, shuffle=%d)\", SPARK_AQE_ENABLED, SPARK_SHUFFLE_PARTITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CARREGAR MODELO DO MLFLOW REGISTRY\n",
    "# =============================================================================\n",
    "client = MlflowClient()\n",
    "\n",
    "# Buscar versao do modelo no stage especificado\n",
    "model_versions = client.get_latest_versions(MODEL_NAME, stages=[MODEL_STAGE])\n",
    "if not model_versions:\n",
    "    raise RuntimeError(\n",
    "        f\"Nenhuma versao encontrada para '{MODEL_NAME}' no stage '{MODEL_STAGE}'. \"\n",
    "        f\"Registre o modelo primeiro via export_model.py\"\n",
    "    )\n",
    "\n",
    "mv = model_versions[0]\n",
    "logger.info(\"Modelo encontrado: %s v%s (run_id=%s)\", mv.name, mv.version, mv.run_id)\n",
    "\n",
    "# Carregar modelo como pyfunc (funciona com sklearn e lightgbm)\n",
    "model_uri = f\"models:/{MODEL_NAME}/{MODEL_STAGE}\"\n",
    "model = mlflow.pyfunc.load_model(model_uri)\n",
    "logger.info(\"Modelo carregado: %s\", model_uri)\n",
    "\n",
    "# Recuperar lista de features do run original\n",
    "run = client.get_run(mv.run_id)\n",
    "artifacts_path = client.download_artifacts(mv.run_id, \"\")\n",
    "\n",
    "# Tentar carregar metadata JSON (gerado pelo export_model.py)\n",
    "import glob\n",
    "metadata_files = glob.glob(f\"{artifacts_path}/*metadata*.json\")\n",
    "if metadata_files:\n",
    "    with open(metadata_files[0]) as f:\n",
    "        model_metadata = json.load(f)\n",
    "    FEATURE_NAMES = model_metadata[\"feature_names\"]\n",
    "    logger.info(\"Features carregadas do metadata: %d features\", len(FEATURE_NAMES))\n",
    "else:\n",
    "    logger.warning(\"Metadata JSON nao encontrado — usando features do run param\")\n",
    "    n_features = int(run.data.params.get(\"n_features\", 0))\n",
    "    raise RuntimeError(\n",
    "        \"Feature names nao disponiveis. Execute export_model.py primeiro para gerar metadata.\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nModelo: {MODEL_NAME} v{mv.version}\")\n",
    "print(f\"Features: {len(FEATURE_NAMES)}\")\n",
    "print(f\"Primeiras 10: {FEATURE_NAMES[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CARREGAR FEATURE STORE (C1: excluir targets, H2: usar F.col, H5: schema check)\n# =============================================================================\ndf_feature_store = spark.read.format(\"delta\").load(PATH_FEATURE_STORE)\n\n# Validar tipo da coluna SAFRA\nsafra_type = dict(df_feature_store.dtypes).get(\"SAFRA\")\nif safra_type not in (\"int\", \"bigint\"):\n    logger.warning(\"SAFRA type is '%s' (expected int) — cast may be needed\", safra_type)\n\n# Filtrar SAFRAs usando API parametrizada (H2)\ndf_feature_store = df_feature_store.filter(F.col(\"SAFRA\").isin(SCORING_SAFRAS))\n\ntotal_records = df_feature_store.count()\nlogger.info(\"Feature store carregada: %d registros para SAFRAs %s\", total_records, SCORING_SAFRAS)\n\nif total_records == 0:\n    raise RuntimeError(f\"Nenhum registro encontrado para SAFRAs {SCORING_SAFRAS} no feature store\")\n\n# C1: Verificar e excluir target columns que possam existir no feature store\nleaked = [c for c in df_feature_store.columns if c in TARGET_COLUMNS]\nif leaked:\n    logger.warning(\"Target columns encontradas no feature store — excluindo: %s\", leaked)\nleaked_in_features = [f for f in FEATURE_NAMES if f in TARGET_COLUMNS]\nif leaked_in_features:\n    raise RuntimeError(f\"FEATURE_NAMES contem target columns (leakage!): {leaked_in_features}\")\n\n# Verificar que todas as features existem\navailable_cols = set(df_feature_store.columns)\nmissing_features = [f for f in FEATURE_NAMES if f not in available_cols]\nif missing_features:\n    logger.error(\"Missing features (%d): %s\", len(missing_features), missing_features)\n    raise RuntimeError(f\"{len(missing_features)} features ausentes no feature store\")\nlogger.info(\"Todas as %d features encontradas no feature store\", len(FEATURE_NAMES))\n\n# Volumetria por SAFRA\ndf_feature_store.groupBy(\"SAFRA\").count().orderBy(\"SAFRA\").show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# SCORING — por SAFRA (H3: output validation, M1: ptypes, M3: qcut fallback)\n# =============================================================================\nall_scores = []\n\nfor safra in SCORING_SAFRAS:\n    logger.info(\"Scoring SAFRA %d...\", safra)\n\n    # Filtrar SAFRA usando API parametrizada\n    df_safra = df_feature_store.filter(F.col(\"SAFRA\") == safra)\n\n    # Selecionar chaves + features\n    # H-NEW-2: .copy() para evitar SettingWithCopyWarning ao modificar com fillna\n    df_keys = df_safra.select(\"NUM_CPF\", \"SAFRA\").toPandas().copy()\n    df_X = df_safra.select(FEATURE_NAMES).toPandas().copy()\n\n    # M1: Tratar missing usando pandas type API\n    for col in df_X.columns:\n        if ptypes.is_numeric_dtype(df_X[col]):\n            df_X[col] = df_X[col].fillna(0)\n        else:\n            df_X[col] = df_X[col].fillna(\"MISSING\")\n\n    # H3: Predizer com validacao de output\n    try:\n        if hasattr(model, '_model_impl'):\n            inner = model._model_impl\n            if hasattr(inner, 'predict_proba'):\n                raw_scores = inner.predict_proba(df_X)\n                if raw_scores.ndim == 2 and raw_scores.shape[1] == 2:\n                    scores = raw_scores[:, 1]\n                else:\n                    raise RuntimeError(f\"predict_proba shape inesperado: {raw_scores.shape}\")\n            else:\n                scores = inner.predict(df_X)\n        else:\n            scores = model.predict(df_X)\n    except Exception as e:\n        logger.error(\"Predicao falhou para SAFRA %d: %s\", safra, e)\n        logger.error(\"X shape: %s\", df_X.shape)\n        raise\n\n    scores = np.asarray(scores, dtype=float)\n    if len(scores) != len(df_X):\n        raise RuntimeError(f\"Score count mismatch: {len(scores)} != {len(df_X)}\")\n    if not np.all(np.isfinite(scores)):\n        n_invalid = (~np.isfinite(scores)).sum()\n        logger.warning(\"  %d scores nao-finitos — clipping to [0, 1]\", n_invalid)\n        scores = np.clip(np.nan_to_num(scores, nan=0.5), 0, 1)\n    if np.any((scores < 0) | (scores > 1)):\n        logger.warning(\"  Scores fora de [0, 1] — clipping\")\n        scores = np.clip(scores, 0, 1)\n\n    logger.info(\"  Scores range: [%.4f, %.4f]\", scores.min(), scores.max())\n\n    # Montar DataFrame de saida\n    df_result = df_keys.copy()\n    df_result[\"SCORE_PROB\"] = scores\n\n    # Score invertido (menor = melhor, padrao mercado credito)\n    df_result[\"SCORE\"] = (SCORE_SCALE * (1 - df_result[\"SCORE_PROB\"])).round(0).astype(int)\n\n    # M3: Faixa de risco por quintil com fallback\n    try:\n        df_result[\"FAIXA_RISCO\"] = pd.qcut(\n            df_result[\"SCORE_PROB\"],\n            q=N_FAIXAS,\n            labels=list(range(1, N_FAIXAS + 1)),\n            duplicates=\"drop\"\n        ).astype(int)\n        n_bins = df_result[\"FAIXA_RISCO\"].nunique()\n        if n_bins < N_FAIXAS:\n            logger.warning(\"  SAFRA %d: qcut produced %d bins (expected %d)\", safra, n_bins, N_FAIXAS)\n    except ValueError:\n        logger.warning(\"  SAFRA %d: qcut failed — using rank-based binning\", safra)\n        df_result[\"FAIXA_RISCO\"] = pd.cut(\n            df_result[\"SCORE_PROB\"].rank(pct=True),\n            bins=N_FAIXAS,\n            labels=list(range(1, N_FAIXAS + 1))\n        ).astype(int)\n\n    df_result[\"MODEL_NAME\"] = MODEL_NAME\n    df_result[\"MODEL_VERSION\"] = str(mv.version)\n    df_result[\"DT_SCORING\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    all_scores.append(df_result)\n    logger.info(\"  SAFRA %d: %d registros scored (score medio=%.0f)\",\n                safra, len(df_result), df_result[\"SCORE\"].mean())\n\n# Consolidar\ndf_all_scores = pd.concat(all_scores, ignore_index=True)\nlogger.info(\"Total scored: %d registros\", len(df_all_scores))\n\nlogger.info(\"Distribuicao de scores:\")\nlogger.info(\"\\n%s\", df_all_scores[\"SCORE\"].describe())\nlogger.info(\"Distribuicao por faixa de risco:\")\nlogger.info(\"\\n%s\", df_all_scores.groupby([\"SAFRA\", \"FAIXA_RISCO\"]).size().unstack(fill_value=0))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# SALVAR SCORES (H4: single dynamic write, M5: enhanced validation)\n# =============================================================================\nschema = StructType([\n    StructField(\"NUM_CPF\", StringType(), True),\n    StructField(\"SAFRA\", IntegerType(), True),\n    StructField(\"SCORE_PROB\", DoubleType(), True),\n    StructField(\"SCORE\", IntegerType(), True),\n    StructField(\"FAIXA_RISCO\", IntegerType(), True),\n    StructField(\"MODEL_NAME\", StringType(), True),\n    StructField(\"MODEL_VERSION\", StringType(), True),\n    StructField(\"DT_SCORING\", StringType(), True),\n])\n\ndf_spark_scores = spark.createDataFrame(df_all_scores, schema=schema)\n\n# H4: Single write com dynamic partition overwrite (idempotente, sem race condition)\ndf_spark_scores.write.format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .option(\"partitionOverwriteMode\", \"dynamic\") \\\n    .partitionBy(\"SAFRA\") \\\n    .save(PATH_SCORES)\nlogger.info(\"Scores escritos em %s\", PATH_SCORES)\n\n# M5: Validacao pos-escrita aprimorada\ndf_check = spark.read.format(\"delta\").load(PATH_SCORES)\n\n# Check 1: Count por SAFRA\nlogger.info(\"Validacao pos-escrita:\")\ndf_check.groupBy(\"SAFRA\").count().orderBy(\"SAFRA\").show()\n\n# Check 2: Todas as SAFRAs presentes\nwritten_safras = set(r[0] for r in df_check.select(\"SAFRA\").distinct().collect())\nexpected_safras = set(SCORING_SAFRAS)\nmissing_safras = expected_safras - written_safras\nif missing_safras:\n    raise RuntimeError(f\"SAFRAs nao escritas: {missing_safras}\")\n\n# Check 3: Duplicatas\ndup_count = df_check.filter(F.col(\"SAFRA\").isin(SCORING_SAFRAS)) \\\n    .groupBy(\"NUM_CPF\", \"SAFRA\").count().filter(\"count > 1\").count()\nif dup_count > 0:\n    raise RuntimeError(f\"Encontradas {dup_count} duplicatas (NUM_CPF, SAFRA)\")\n\n# Check 4: Score ranges validos\ninvalid_scores = df_check.filter(F.col(\"SAFRA\").isin(SCORING_SAFRAS)).filter(\n    (F.col(\"SCORE\") < 0) | (F.col(\"SCORE\") > SCORE_SCALE) |\n    (F.col(\"SCORE_PROB\") < 0) | (F.col(\"SCORE_PROB\") > 1)\n).count()\nif invalid_scores > 0:\n    logger.warning(\"Encontrados %d registros com scores invalidos\", invalid_scores)\n\ntotal = df_check.filter(F.col(\"SAFRA\").isin(SCORING_SAFRAS)).count()\nlogger.info(\"Validacao OK: %d registros, %d colunas, 0 duplicatas\", total, len(df_check.columns))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOG DE SCORING NO MLFLOW\n",
    "# =============================================================================\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "with mlflow.start_run(run_name=f\"scoring_batch_{datetime.now().strftime('%Y%m%d_%H%M')}\"):\n",
    "    mlflow.log_param(\"model_name\", MODEL_NAME)\n",
    "    mlflow.log_param(\"model_version\", mv.version)\n",
    "    mlflow.log_param(\"model_stage\", MODEL_STAGE)\n",
    "    mlflow.log_param(\"scoring_safras\", str(SCORING_SAFRAS))\n",
    "    mlflow.log_param(\"n_features\", len(FEATURE_NAMES))\n",
    "    mlflow.log_param(\"output_path\", PATH_SCORES)\n",
    "\n",
    "    mlflow.log_metric(\"total_records_scored\", len(df_all_scores))\n",
    "    mlflow.log_metric(\"score_mean\", float(df_all_scores[\"SCORE\"].mean()))\n",
    "    mlflow.log_metric(\"score_std\", float(df_all_scores[\"SCORE\"].std()))\n",
    "    mlflow.log_metric(\"score_prob_mean\", float(df_all_scores[\"SCORE_PROB\"].mean()))\n",
    "\n",
    "    for safra in SCORING_SAFRAS:\n",
    "        mask = df_all_scores[\"SAFRA\"] == safra\n",
    "        mlflow.log_metric(f\"records_safra_{safra}\", int(mask.sum()))\n",
    "        mlflow.log_metric(f\"score_mean_safra_{safra}\", float(df_all_scores.loc[mask, \"SCORE\"].mean()))\n",
    "\n",
    "    # Salvar distribuicao como artefato CSV\n",
    "    dist_path = \"/tmp/scoring_distribution.csv\"\n",
    "    df_all_scores.groupby([\"SAFRA\", \"FAIXA_RISCO\"]).agg(\n",
    "        count=(\"NUM_CPF\", \"count\"),\n",
    "        score_mean=(\"SCORE\", \"mean\"),\n",
    "        score_prob_mean=(\"SCORE_PROB\", \"mean\"),\n",
    "    ).reset_index().to_csv(dist_path, index=False)\n",
    "    mlflow.log_artifact(dist_path)\n",
    "\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    logger.info(\"MLflow scoring run: %s\", run_id)\n",
    "\n",
    "print(f\"\\nScoring batch concluido com sucesso!\")\n",
    "print(f\"MLflow Run ID: {run_id}\")\n",
    "print(f\"Output: {PATH_SCORES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo\n",
    "\n",
    "| Item | Valor |\n",
    "|------|-------|\n",
    "| Modelo | `credit-risk-fpd-lgbm_baseline` |\n",
    "| Stage | Production |\n",
    "| Output | `Gold.feature_store.clientes_scores` |\n",
    "| Particionado por | SAFRA |\n",
    "| Colunas output | NUM_CPF, SAFRA, SCORE_PROB, SCORE, FAIXA_RISCO, MODEL_NAME, MODEL_VERSION, DT_SCORING |\n",
    "\n",
    "**Proximos passos**: Executar `validacao_deploy.py` para confirmar que metricas do scoring == metricas da avaliacao."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}