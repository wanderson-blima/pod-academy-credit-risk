{
 "cells": [
  {
   "cell_type": "code",
   "id": "v6_000",
   "metadata": {},
   "source": [
    "%%configure\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.driver.memory\": \"28g\",\n",
    "        \"spark.executor.memory\": \"28g\",\n",
    "        \"spark.driver.maxResultSize\": \"8g\"\n",
    "    }\n",
    "}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v6_001",
   "metadata": {},
   "source": [
    "# Modelo Baseline de Risco - Telecom (v6 - LR L1 + LGBM + 26 Visualizacoes)\n",
    "\n",
    "Este notebook implementa a **comparacao dual-model (Logistic Regression L1 vs LightGBM)** para risco de inadimplencia (FPD) de clientes Claro, com visualizacoes avancadas para o time de negocio.\n",
    "\n",
    "### Evolucao\n",
    "- **v1**: PySpark para limpeza + Pandas para modelagem (original)\n",
    "- **v2**: Python puro com `deltalake` (delta-rs) — falha no Fabric\n",
    "- **v3**: Spark read + Pandas modeling, RL + LGBM + KS Incremental + SHAP\n",
    "- **v4**: LGBM only, SHAP feature selection otimizado\n",
    "- **v5**: Swap analysis corrigido (modelo vs FPD real) + 16 visualizacoes + decis + PSI\n",
    "- **v6 (este)**: Dual-model (LR L1 + LGBM) + SHAP + Swap corrigido + 26 visualizacoes business-friendly\n",
    "\n",
    "### Principais etapas\n",
    "1. Leitura do Gold Feature Store via `spark.read.format(\"delta\")` (5-layer memory opt)\n",
    "2. Limpeza de dados (missing, correlacao, leakage)\n",
    "3. Split temporal: Treino (202410-202412), Val (202501), OOS (75%), OOT (202502-202503)\n",
    "4. Amostragem estratificada 25% por (SAFRA, FPD)\n",
    "5. **Dual Pipeline**: LR L1 (SimpleImputer+StandardScaler) + LGBM (SimpleImputer+CountEncoder)\n",
    "6. **GridSearch** para ambos modelos\n",
    "7. Avaliacao baseline (AUC, KS por safra) — 13 splits x 2 modelos\n",
    "8. **KS Incremental** por fonte de dados (7 steps x 2 modelos)\n",
    "9. **SHAP TreeExplainer** — feature selection 90% cumulativo\n",
    "10. **Modelos finais** com features SHAP — determinar melhor modelo\n",
    "11. **Swap analysis correto** (modelo vs FPD real, mesma populacao)\n",
    "12. **Analise por decis + Capture Rate + PSI**\n",
    "13. **26 visualizacoes** (3 paineis + SHAP + KS Incremental)\n",
    "14. Export via MLflow Registry (ambos modelos)"
   ]
  },
  {
   "cell_type": "code",
   "id": "v6_010",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 1. IMPORTS E CONFIGURACAO\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gc\n",
    "import os\n",
    "import shap\n",
    "import pickle\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import matplotlib\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType, IntegerType, DoubleType, LongType\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, precision_recall_curve\n",
    "from lightgbm import LGBMClassifier\n",
    "from category_encoders import CountEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FIX: Fabric sklearn rejects BOTH force_all_finite AND ensure_all_finite\n",
    "# LightGBM sklearn wrapper passes force_all_finite -> drop both params\n",
    "import lightgbm.sklearn as _lgbm_sklearn\n",
    "_orig_check = _lgbm_sklearn._LGBMCheckArray\n",
    "def _patched_lgbm_check(*args, **kwargs):\n",
    "    kwargs.pop('force_all_finite', None)\n",
    "    kwargs.pop('ensure_all_finite', None)\n",
    "    return _orig_check(*args, **kwargs)\n",
    "_lgbm_sklearn._LGBMCheckArray = _patched_lgbm_check\n",
    "\n",
    "# Config centralizado do pipeline\n",
    "import sys; sys.path.insert(0, '/lakehouse/default/Files/projeto-final')\n",
    "from config.pipeline_config import (\n",
    "    PATH_FEATURE_STORE, EXPERIMENT_NAME, SAFRAS,\n",
    "    LEAKAGE_BLACKLIST, TARGET_COLUMNS\n",
    ")\n",
    "\n",
    "# ---- CONSTANTES V6 ----\n",
    "OUTPUT_DIR_V6 = \"/lakehouse/default/Files/projeto-final/docs/analytics/v6\"\n",
    "os.makedirs(OUTPUT_DIR_V6, exist_ok=True)\n",
    "DPI = 150\n",
    "SWAP_CUTOFFS = [0.05, 0.10, 0.20, 0.30]\n",
    "\n",
    "COLORS = {\n",
    "    \"blue\": \"#2196F3\", \"orange\": \"#FF9800\", \"green\": \"#4CAF50\",\n",
    "    \"red\": \"#F44336\", \"purple\": \"#9C27B0\", \"gray\": \"#607D8B\",\n",
    "    \"teal\": \"#009688\", \"pink\": \"#E91E63\",\n",
    "}\n",
    "BOOK_COLORS = {\n",
    "    'Base (Telco+Score)': '#607D8B', 'Recarga (REC_)': '#2196F3',\n",
    "    'Pagamento (PAG_)': '#FF9800', 'Faturamento (FAT_)': '#9C27B0',\n",
    "}\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette([COLORS[\"blue\"], COLORS[\"orange\"], COLORS[\"green\"], COLORS[\"red\"]])\n",
    "\n",
    "print('Imports OK — v6 (LR L1 + LGBM + 26 Viz + check_array patched)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v6_020",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 2. MLFLOW SETUP\n",
    "# =============================================================================\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.autolog(disable=True)  # Controle manual para evitar conflitos\n",
    "\n",
    "print(f'MLflow experiment: {EXPERIMENT_NAME}')\n",
    "print(f'Tracking URI: {mlflow.get_tracking_uri()}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v6_md_data",
   "metadata": {},
   "source": [
    "## 3. Leitura Otimizada do Gold Feature Store\n",
    "\n",
    "Estrategia de 5 camadas para contornar o limite de `spark.driver.maxResultSize` (8 GiB):\n",
    "1. **Arrow + selfDestruct** — serializacao 2-3x mais eficiente\n",
    "2. **Drop colunas audit/leakage no Spark** — menos dados para serializar\n",
    "3. **FLAG_INSTALACAO == 1 filtrado no Spark** — reduz ~30-40% das linhas\n",
    "4. **Cast Double->Float, Long->Int no Spark** — metade do tamanho numerico\n",
    "5. **Conversao chunked por SAFRA** + gc.collect() entre chunks"
   ]
  },
  {
   "cell_type": "code",
   "id": "v6_031",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 3. LEITURA OTIMIZADA DO GOLD FEATURE STORE (Spark -> Pandas)\n",
    "# =============================================================================\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.selfDestruct.enabled\", \"true\")\n",
    "\n",
    "print(f'Lendo feature store de: {PATH_FEATURE_STORE}\\n')\n",
    "\n",
    "df_spark = spark.read.format(\"delta\").load(PATH_FEATURE_STORE)\n",
    "n_original = df_spark.count()\n",
    "print(f'Original: {n_original:,} rows x {len(df_spark.columns)} cols')\n",
    "\n",
    "# Drop colunas audit + leakage no Spark\n",
    "cols_audit = ['_execution_id', '_data_inclusao', '_data_alteracao_silver', 'DT_PROCESSAMENTO']\n",
    "cols_drop = [c for c in cols_audit + LEAKAGE_BLACKLIST if c in df_spark.columns]\n",
    "if cols_drop:\n",
    "    df_spark = df_spark.drop(*cols_drop)\n",
    "    print(f'Drop {len(cols_drop)} colunas (audit+leakage): {cols_drop}')\n",
    "\n",
    "# Filtrar FLAG_INSTALACAO == 1 no Spark\n",
    "n_reprovados = 0\n",
    "if 'FLAG_INSTALACAO' in df_spark.columns:\n",
    "    n_reprovados = df_spark.filter(F.col('FLAG_INSTALACAO') == 0).count()\n",
    "    df_spark = df_spark.filter(F.col('FLAG_INSTALACAO') == 1).drop('FLAG_INSTALACAO')\n",
    "    n_pos = n_original - n_reprovados\n",
    "    print(f'FLAG_INSTALACAO: {n_original:,} -> {n_pos:,} ({n_reprovados:,} reprovados removidos)')\n",
    "else:\n",
    "    n_pos = n_original\n",
    "\n",
    "# Cast tipos via .select() (plano flat)\n",
    "cast_exprs = []\n",
    "n_double, n_long = 0, 0\n",
    "for field in df_spark.schema.fields:\n",
    "    if isinstance(field.dataType, DoubleType):\n",
    "        cast_exprs.append(F.col(field.name).cast(FloatType()).alias(field.name))\n",
    "        n_double += 1\n",
    "    elif isinstance(field.dataType, LongType):\n",
    "        cast_exprs.append(F.col(field.name).cast(IntegerType()).alias(field.name))\n",
    "        n_long += 1\n",
    "    else:\n",
    "        cast_exprs.append(F.col(field.name))\n",
    "df_spark = df_spark.select(*cast_exprs)\n",
    "print(f'Cast tipos: {n_double} Double->Float, {n_long} Long->Int')\n",
    "\n",
    "# Conversao chunked por SAFRA\n",
    "safras_disponiveis = sorted([row.SAFRA for row in df_spark.select('SAFRA').distinct().collect()])\n",
    "print(f'\\nSAFRAs: {safras_disponiveis} | Colunas: {len(df_spark.columns)}')\n",
    "print('Convertendo por SAFRA...')\n",
    "\n",
    "chunks = []\n",
    "for safra in safras_disponiveis:\n",
    "    chunk = df_spark.filter(F.col('SAFRA') == safra).toPandas()\n",
    "    mem_mb = chunk.memory_usage(deep=True).sum() / 1e6\n",
    "    print(f'  SAFRA {safra}: {len(chunk):,} rows | {mem_mb:.0f} MB')\n",
    "    chunks.append(chunk)\n",
    "    gc.collect()\n",
    "\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks\n",
    "gc.collect()\n",
    "\n",
    "print(f'\\nDataset carregado:')\n",
    "print(f'  Shape: {df.shape}')\n",
    "print(f'  Memory: {df.memory_usage(deep=True).sum() / 1e9:.2f} GB')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v6_md_clean",
   "metadata": {},
   "source": [
    "## 4. Limpeza de Dados (Pandas puro)"
   ]
  },
  {
   "cell_type": "code",
   "id": "v6_041",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 4.1 FUNCOES DE LIMPEZA\n",
    "# =============================================================================\n",
    "\n",
    "def clean_empty_keys(df):\n",
    "    return df.dropna(subset=['NUM_CPF', 'SAFRA'])\n",
    "\n",
    "def convert_cep3_uf_regiao(df):\n",
    "    cep_map = {\n",
    "        '01':('SP','SUDESTE'),'02':('SP','SUDESTE'),'03':('SP','SUDESTE'),\n",
    "        '04':('SP','SUDESTE'),'05':('SP','SUDESTE'),'06':('SP','SUDESTE'),\n",
    "        '07':('SP','SUDESTE'),'08':('SP','SUDESTE'),'09':('SP','SUDESTE'),\n",
    "        '20':('RJ','SUDESTE'),'21':('RJ','SUDESTE'),'22':('RJ','SUDESTE'),\n",
    "        '23':('RJ','SUDESTE'),'24':('RJ','SUDESTE'),'29':('ES','SUDESTE'),\n",
    "        '30':('MG','SUDESTE'),'31':('MG','SUDESTE'),'32':('MG','SUDESTE'),\n",
    "        '33':('MG','SUDESTE'),'34':('MG','SUDESTE'),'35':('MG','SUDESTE'),\n",
    "        '36':('MG','SUDESTE'),'37':('MG','SUDESTE'),'38':('MG','SUDESTE'),\n",
    "        '39':('MG','SUDESTE'),\n",
    "        '40':('BA','NORDESTE'),'41':('BA','NORDESTE'),'42':('BA','NORDESTE'),\n",
    "        '43':('BA','NORDESTE'),'44':('BA','NORDESTE'),'45':('BA','NORDESTE'),\n",
    "        '46':('BA','NORDESTE'),'47':('BA','NORDESTE'),'48':('BA','NORDESTE'),\n",
    "        '49':('SE','NORDESTE'),\n",
    "        '50':('PE','NORDESTE'),'51':('PE','NORDESTE'),'52':('PE','NORDESTE'),\n",
    "        '53':('PE','NORDESTE'),'54':('PE','NORDESTE'),'55':('PE','NORDESTE'),\n",
    "        '56':('AL','NORDESTE'),'57':('AL','NORDESTE'),\n",
    "        '58':('PB','NORDESTE'),'59':('RN','NORDESTE'),\n",
    "        '60':('CE','NORDESTE'),'61':('CE','NORDESTE'),'62':('CE','NORDESTE'),\n",
    "        '63':('PI','NORDESTE'),'64':('PI','NORDESTE'),'65':('MA','NORDESTE'),\n",
    "        '66':('PA','NORTE'),'67':('PA','NORTE'),'68':('AC','NORTE'),\n",
    "        '69':('AM','NORTE'),'77':('TO','NORTE'),\n",
    "        '70':('DF','CENTRO-OESTE'),'71':('DF','CENTRO-OESTE'),\n",
    "        '72':('GO','CENTRO-OESTE'),'73':('GO','CENTRO-OESTE'),\n",
    "        '74':('GO','CENTRO-OESTE'),'75':('GO','CENTRO-OESTE'),\n",
    "        '76':('GO','CENTRO-OESTE'),\n",
    "        '78':('MT','CENTRO-OESTE'),'79':('MS','CENTRO-OESTE'),\n",
    "        '80':('PR','SUL'),'81':('PR','SUL'),'82':('PR','SUL'),\n",
    "        '83':('PR','SUL'),'84':('PR','SUL'),'85':('PR','SUL'),\n",
    "        '86':('PR','SUL'),'87':('PR','SUL'),\n",
    "        '88':('SC','SUL'),'89':('SC','SUL'),\n",
    "        '90':('RS','SUL'),'91':('RS','SUL'),'92':('RS','SUL'),\n",
    "        '93':('RS','SUL'),'94':('RS','SUL'),'95':('RS','SUL'),\n",
    "        '96':('RS','SUL'),'97':('RS','SUL'),'98':('RS','SUL'),'99':('RS','SUL'),\n",
    "    }\n",
    "    if 'CEP_3_digitos' not in df.columns:\n",
    "        return df\n",
    "    cep2 = df['CEP_3_digitos'].astype(str).str[:2]\n",
    "    mapped = cep2.map(cep_map)\n",
    "    df['UF'] = mapped.apply(lambda x: x[0] if isinstance(x, tuple) else 'OUTROS')\n",
    "    df['REGIAO'] = mapped.apply(lambda x: x[1] if isinstance(x, tuple) else 'OUTROS')\n",
    "    return df.drop(columns=['CEP_3_digitos'])\n",
    "\n",
    "def adjust_and_drop_date_cols(df):\n",
    "    if 'var_12' in df.columns:\n",
    "        df['var_12'] = pd.to_datetime(df['var_12'], format='%d/%m/%Y', errors='coerce')\n",
    "    df['DATA_REF_SAFRA'] = pd.to_datetime(df['SAFRA'].astype(str), format='%Y%m')\n",
    "    if 'var_12' in df.columns:\n",
    "        df['DIAS_VAR_12'] = (df['DATA_REF_SAFRA'] - df['var_12']).dt.days\n",
    "    if 'PAG_DT_PRIMEIRA_FATURA' in df.columns:\n",
    "        df['PAG_DT_PRIMEIRA_FATURA'] = pd.to_datetime(df['PAG_DT_PRIMEIRA_FATURA'], errors='coerce')\n",
    "        df['PAG_DIAS_DESDE_PRIMEIRA_FATURA'] = (df['DATA_REF_SAFRA'] - df['PAG_DT_PRIMEIRA_FATURA']).dt.days\n",
    "    date_cols = df.select_dtypes(include=['datetime64', 'datetimetz']).columns.tolist()\n",
    "    date_cols.append('DATA_REF_SAFRA')\n",
    "    return df.drop(columns=[c for c in date_cols if c in df.columns])\n",
    "\n",
    "def remove_high_missing(df, threshold=0.75):\n",
    "    null_pct = df.isnull().mean()\n",
    "    cols_to_drop = null_pct[null_pct >= threshold].index.tolist()\n",
    "    print(f'  High missing (>= {threshold:.0%}): {len(cols_to_drop)} colunas removidas')\n",
    "    return df.drop(columns=cols_to_drop)\n",
    "\n",
    "def remove_low_cardinality(df):\n",
    "    low_card = [c for c in df.columns if df[c].nunique() <= 1]\n",
    "    print(f'  Low cardinality (== 1): {len(low_card)} colunas removidas')\n",
    "    return df.drop(columns=low_card)\n",
    "\n",
    "def remove_high_correlation(df, threshold=0.8, safras_train=None):\n",
    "    if safras_train is not None:\n",
    "        df_corr_base = df[df['SAFRA'].isin(safras_train)]\n",
    "    else:\n",
    "        df_corr_base = df\n",
    "    df_sample = df_corr_base.groupby(['SAFRA', 'FPD'], group_keys=False).apply(\n",
    "        lambda x: x.sample(frac=0.25, random_state=42))\n",
    "    num_cols = df_sample.select_dtypes(include=['int32','int64','float32','float64']).columns\n",
    "    num_cols = [c for c in num_cols if c != 'FPD']\n",
    "    corr_matrix = df_sample[num_cols].corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = []\n",
    "    while True:\n",
    "        max_corr = upper.max().max()\n",
    "        if max_corr < threshold:\n",
    "            break\n",
    "        col_to_drop = upper.max().sort_values(ascending=False).index[0]\n",
    "        to_drop.append(col_to_drop)\n",
    "        upper = upper.drop(index=col_to_drop, columns=col_to_drop)\n",
    "    print(f'  High correlation (> {threshold}): {len(to_drop)} colunas removidas')\n",
    "    return df.drop(columns=to_drop)\n",
    "\n",
    "def remove_misused_columns(df):\n",
    "    misused = ['PROD', 'flag_mig2', 'FAT_VLR_FPD', 'FAT_FLAG_MIG2_AQUISICAO']\n",
    "    existing = [c for c in misused if c in df.columns]\n",
    "    if existing:\n",
    "        print(f'  Misused columns removed: {existing}')\n",
    "    return df.drop(columns=existing, errors='ignore')\n",
    "\n",
    "print('Funcoes de limpeza carregadas')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v6_042",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 4.2 APLICAR LIMPEZAS\n",
    "# =============================================================================\n",
    "safras_train_val = SAFRAS[:4]\n",
    "\n",
    "print('Aplicando limpezas...')\n",
    "print(f'Shape original: {df.shape}')\n",
    "\n",
    "df = clean_empty_keys(df)\n",
    "df = convert_cep3_uf_regiao(df)\n",
    "df = adjust_and_drop_date_cols(df)\n",
    "df = remove_high_missing(df)\n",
    "df = remove_low_cardinality(df)\n",
    "df = remove_high_correlation(df, threshold=0.8, safras_train=safras_train_val)\n",
    "df = remove_misused_columns(df)\n",
    "\n",
    "print(f'Shape apos limpezas: {df.shape}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v6_md_split",
   "metadata": {},
   "source": [
    "## 5. Split Temporal e Amostragem Estratificada"
   ]
  },
  {
   "cell_type": "code",
   "id": "v6_051",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 5. SPLIT TEMPORAL + AMOSTRAGEM ESTRATIFICADA\n",
    "# =============================================================================\n",
    "safras_ord = sorted(df['SAFRA'].unique())\n",
    "safras_train_oos = safras_ord[:4]  # 202410-202501\n",
    "safras_oot = safras_ord[4:]        # 202502-202503\n",
    "\n",
    "df_4_safras = df[df['SAFRA'].isin(safras_train_oos)]\n",
    "df_oot_full = df[df['SAFRA'].isin(safras_oot)]\n",
    "\n",
    "# Amostragem estratificada 25%\n",
    "df_sample = df_4_safras.groupby(['SAFRA', 'FPD'], group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=0.25, random_state=42))\n",
    "df_oos = df_4_safras.drop(df_sample.index)\n",
    "\n",
    "df_sample = df_sample.reset_index(drop=True).drop_duplicates()\n",
    "df_oos = df_oos.reset_index(drop=True).drop_duplicates()\n",
    "df_oot = df_oot_full.reset_index(drop=True).drop_duplicates()\n",
    "\n",
    "del df, df_4_safras, df_oot_full\n",
    "gc.collect()\n",
    "\n",
    "print(f'Sample (train+val): {df_sample.shape}')\n",
    "print(f'OOS:                {df_oos.shape}')\n",
    "print(f'OOT:                {df_oot.shape}')\n",
    "\n",
    "for name, data in [('Sample', df_sample), ('OOS', df_oos), ('OOT', df_oot)]:\n",
    "    print(f'\\n--- {name} ---')\n",
    "    print(data[['SAFRA', 'FPD']].value_counts().sort_index().to_string())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v6_053",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 6. SEPARACAO TREINO / VALIDACAO / X / Y\n",
    "# =============================================================================\n",
    "safras_train = [202410, 202411, 202412]\n",
    "safras_val = [202501]\n",
    "\n",
    "df_train = df_sample[df_sample['SAFRA'].isin(safras_train)]\n",
    "df_val = df_sample[df_sample['SAFRA'].isin(safras_val)]\n",
    "\n",
    "X_train = df_train.drop(columns=['FPD'])\n",
    "y_train = df_train['FPD']\n",
    "X_val = df_val.drop(columns=['FPD'])\n",
    "y_val = df_val['FPD']\n",
    "\n",
    "X_train_final = pd.concat([X_train, X_val], axis=0).reset_index(drop=True)\n",
    "y_train_final = pd.concat([y_train, y_val], axis=0).reset_index(drop=True)\n",
    "\n",
    "X_oos_agg = df_oos.drop(columns=['FPD'])\n",
    "y_oos_agg = df_oos['FPD']\n",
    "X_oot_agg = df_oot.drop(columns=['FPD'])\n",
    "y_oot_agg = df_oot['FPD']\n",
    "\n",
    "# Variaveis numericas e categoricas\n",
    "num_features = [n for n in X_train.select_dtypes(include=['int32','int64','float32','float64']).columns if n != 'SAFRA']\n",
    "cat_features = [c for c in X_train.select_dtypes(include=['object','category']).columns if c != 'NUM_CPF']\n",
    "\n",
    "print(f'X_train_final: {X_train_final.shape}')\n",
    "print(f'X_oos: {X_oos_agg.shape}')\n",
    "print(f'X_oot: {X_oot_agg.shape}')\n",
    "print(f'Numericas: {len(num_features)} | Categoricas: {len(cat_features)}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v6_md_pipe",
   "metadata": {},
   "source": [
    "## 7. Dual Pipeline — Logistic Regression L1 + LightGBM\n",
    "\n",
    "**LR L1**: SimpleImputer(median) + StandardScaler + LogisticRegression(penalty='l1', class_weight='balanced')\n",
    "**LGBM**: SimpleImputer(median) + CountEncoder + LGBMClassifier(objective='binary', boosting_type='gbdt')"
   ]
  },
  {
   "cell_type": "code",
   "id": "v6_061",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 7. DUAL PIPELINES\n",
    "# =============================================================================\n",
    "\n",
    "# --- Logistic Regression L1 ---\n",
    "numeric_pipe_lr = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "categorical_pipe_lr = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', CountEncoder(normalize=True)),\n",
    "])\n",
    "preprocess_lr = ColumnTransformer([\n",
    "    ('num', numeric_pipe_lr, num_features),\n",
    "    ('cat', categorical_pipe_lr, cat_features),\n",
    "])\n",
    "pipeline_LR = Pipeline([\n",
    "    ('prep', preprocess_lr),\n",
    "    ('model', LogisticRegression(\n",
    "        solver='liblinear', penalty='l1',\n",
    "        max_iter=2000, tol=1e-3,\n",
    "        class_weight='balanced', random_state=42,\n",
    "    )),\n",
    "])\n",
    "\n",
    "# --- LightGBM ---\n",
    "numeric_pipe_lgbm = Pipeline([('imputer', SimpleImputer(strategy='median'))])\n",
    "categorical_pipe_lgbm = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', CountEncoder(normalize=True, handle_unknown=0, handle_missing=0)),\n",
    "])\n",
    "preprocess_lgbm = ColumnTransformer([\n",
    "    ('num', numeric_pipe_lgbm, num_features),\n",
    "    ('cat', categorical_pipe_lgbm, cat_features),\n",
    "], remainder='drop')\n",
    "pipeline_LGBM = Pipeline([\n",
    "    ('prep', preprocess_lgbm),\n",
    "    ('model', LGBMClassifier(\n",
    "        objective='binary', boosting_type='gbdt',\n",
    "        learning_rate=0.05, colsample_bytree=0.8,\n",
    "        random_state=42, n_jobs=-1, verbosity=-1,\n",
    "    )),\n",
    "])\n",
    "\n",
    "print('Pipelines criados: LR L1 + LGBM')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v6_071",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 8.1 GRID SEARCH — LOGISTIC REGRESSION L1\n",
    "# =============================================================================\n",
    "param_grid_LR = {'model__C': [0.01, 0.05, 0.1, 0.5]}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "grid_LR = GridSearchCV(\n",
    "    pipeline_LR, param_grid=param_grid_LR,\n",
    "    scoring='roc_auc', cv=cv, n_jobs=-1, verbose=3,\n",
    ")\n",
    "grid_LR.fit(X_val, y_val)\n",
    "\n",
    "print(f'Melhores HP LR: {grid_LR.best_params_}')\n",
    "print(f'Melhor AUC LR:  {grid_LR.best_score_:.5f}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v6_072",
   "metadata": {},
   "source": [
    "# Treino final LR com best params (train + val)\n",
    "pipeline_LR.set_params(**grid_LR.best_params_)\n",
    "pipeline_LR.fit(X_train_final, y_train_final)\n",
    "print('LR L1 treinado com train+val')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v6_073",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 8.2 GRID SEARCH — LIGHTGBM\n",
    "# =============================================================================\n",
    "param_grid_LGBM = {\n",
    "    'model__n_estimators': [250, 500],\n",
    "    'model__max_depth': [4, 7],\n",
    "}\n",
    "\n",
    "grid_LGBM = GridSearchCV(\n",
    "    pipeline_LGBM, param_grid=param_grid_LGBM,\n",
    "    scoring='roc_auc', cv=cv, n_jobs=-1, verbose=3, error_score='raise',\n",
    ")\n",
    "grid_LGBM.fit(X_val, y_val)\n",
    "\n",
    "print(f'Melhores HP LGBM: {grid_LGBM.best_params_}')\n",
    "print(f'Melhor AUC LGBM:  {grid_LGBM.best_score_:.5f}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v6_074",
   "metadata": {},
   "source": [
    "# Treino final LGBM com best params (train + val)\n",
    "pipeline_LGBM.set_params(**grid_LGBM.best_params_)\n",
    "pipeline_LGBM.fit(X_train_final, y_train_final)\n",
    "print('LGBM treinado com train+val')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v6_md_eval",
   "metadata": {},
   "source": [
    "## 9. Avaliacao Baseline (AUC, KS) — 13 splits x 2 modelos"
   ]
  },
  {
   "cell_type": "code",
   "id": "v6_081",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 9.1 FUNCOES DE AVALIACAO\n",
    "# =============================================================================\n",
    "\n",
    "def ks_stat(y_true, y_score):\n",
    "    df_ks = pd.DataFrame({'y': y_true.values if hasattr(y_true, 'values') else y_true, 'p': y_score})\n",
    "    df_ks = df_ks.sort_values('p')\n",
    "    df_ks['cum_good'] = (1 - df_ks['y']).cumsum() / (1 - df_ks['y']).sum()\n",
    "    df_ks['cum_bad'] = df_ks['y'].cumsum() / df_ks['y'].sum()\n",
    "    return np.max(np.abs(df_ks['cum_bad'] - df_ks['cum_good']))\n",
    "\n",
    "def evaluation_auc_ks(X, y, pipe, name='', verbose=True):\n",
    "    proba = pipe.predict_proba(X)[:, 1]\n",
    "    auc = round(roc_auc_score(y, proba), 5)\n",
    "    ks = round(ks_stat(y, proba), 5)\n",
    "    if verbose:\n",
    "        print(f'  {name}: AUC={auc}, KS={ks}')\n",
    "    return auc, ks\n",
    "\n",
    "def filter_xy_by_safra(X, y, list_safras):\n",
    "    mask = X['SAFRA'].isin(list_safras)\n",
    "    return X[mask], y.loc[X[mask].index]\n",
    "\n",
    "def _sanitize_mlflow_key(key):\n",
    "    safe = re.sub(r'[^a-zA-Z0-9_\\-]', '_', key)\n",
    "    return re.sub(r'_+', '_', safe).strip('_')\n",
    "\n",
    "def _var_num(col):\n",
    "    m = re.search(r'var_(\\d+)', col)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "# Mapa de 13 splits para avaliacao\n",
    "dict_safras = {\n",
    "    'TREINO - 202410': [202410], 'TREINO - 202411': [202411],\n",
    "    'TREINO - 202412': [202412], 'TREINO / VAL - 202501': [202501],\n",
    "    'TREINO (CONS)': [202410, 202411, 202412, 202501],\n",
    "    'OOS - 202410': [202410], 'OOS - 202411': [202411],\n",
    "    'OOS - 202412': [202412], 'OOS - 202501': [202501],\n",
    "    'OOS (CONS)': [202410, 202411, 202412, 202501],\n",
    "    'OOT - 202502': [202502], 'OOT - 202503': [202503],\n",
    "    'OOT GERAL (CONS)': [202502, 202503],\n",
    "}\n",
    "\n",
    "def generate_map_step_data(X_train, y_train, X_oos, y_oos, X_oot, y_oot):\n",
    "    base = {}\n",
    "    for key in dict_safras:\n",
    "        if 'TREINO' in key:\n",
    "            base[key] = {'X': X_train, 'Y': y_train}\n",
    "        elif 'OOS' in key:\n",
    "            base[key] = {'X': X_oos, 'Y': y_oos}\n",
    "        else:\n",
    "            base[key] = {'X': X_oot, 'Y': y_oot}\n",
    "    return base\n",
    "\n",
    "def update_pipeline(X, name_model):\n",
    "    nf = [n for n in X.select_dtypes(include=['int32','int64','float32','float64']).columns if n != 'SAFRA']\n",
    "    cf = [c for c in X.select_dtypes(include=['object','category']).columns if c != 'NUM_CPF']\n",
    "    if name_model == 'LR':\n",
    "        num_pipe = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "        cat_pipe = Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', CountEncoder(normalize=True))])\n",
    "        prep = ColumnTransformer([('num', num_pipe, nf), ('cat', cat_pipe, cf)])\n",
    "        model = LogisticRegression(\n",
    "            solver='liblinear', penalty='l1', max_iter=2000,\n",
    "            C=grid_LR.best_params_['model__C'],\n",
    "            tol=1e-3, class_weight='balanced', random_state=42,\n",
    "        )\n",
    "    else:\n",
    "        num_pipe = Pipeline([('imputer', SimpleImputer(strategy='median'))])\n",
    "        cat_pipe = Pipeline([('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                             ('encoder', CountEncoder(normalize=True, handle_unknown=0, handle_missing=0))])\n",
    "        prep = ColumnTransformer([('num', num_pipe, nf), ('cat', cat_pipe, cf)], remainder='drop')\n",
    "        model = LGBMClassifier(\n",
    "            objective='binary', boosting_type='gbdt', learning_rate=0.05,\n",
    "            max_depth=grid_LGBM.best_params_['model__max_depth'],\n",
    "            n_estimators=grid_LGBM.best_params_['model__n_estimators'],\n",
    "            colsample_bytree=0.8, random_state=42, n_jobs=-1, verbosity=-1,\n",
    "        )\n",
    "    return Pipeline([('prep', prep), ('model', model)])\n",
    "\n",
    "def current_step(step_num):\n",
    "    return {\n",
    "        0: 'Score1', 1: 'Score1+Score2', 2: '+Cadastro',\n",
    "        3: '+Telco', 4: '+Recarga', 5: '+Pagamento', 6: '+Faturamento',\n",
    "    }[step_num]\n",
    "\n",
    "print('Funcoes de avaliacao carregadas')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v6_082",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 9.2 AVALIACAO LR + MLFLOW + COEFICIENTES + ODDS RATIOS\n",
    "# =============================================================================\n",
    "list_results_baseline = []\n",
    "\n",
    "with mlflow.start_run(run_name='LogisticRegression_Baseline_v6') as run_lr:\n",
    "    params_lr = pipeline_LR.named_steps['model'].get_params()\n",
    "    mlflow.log_param('model_type', 'LogisticRegression_L1')\n",
    "    mlflow.log_param('C', params_lr.get('C'))\n",
    "    mlflow.log_param('n_features', len(X_train_final.columns))\n",
    "    mlflow.log_param('n_samples_train', len(X_train_final))\n",
    "\n",
    "    print('Avaliacao LR L1 por base:')\n",
    "    map_data = generate_map_step_data(X_train_final, y_train_final, X_oos_agg, y_oos_agg, X_oot_agg, y_oot_agg)\n",
    "    for key in dict_safras:\n",
    "        X_f, y_f = filter_xy_by_safra(map_data[key]['X'], map_data[key]['Y'], dict_safras[key])\n",
    "        auc, ks = evaluation_auc_ks(X_f, y_f, pipeline_LR, key)\n",
    "        safe_key = _sanitize_mlflow_key(key)\n",
    "        mlflow.log_metric(f'LR_AUC_{safe_key}', auc)\n",
    "        mlflow.log_metric(f'LR_KS_{safe_key}', ks)\n",
    "        list_results_baseline.append({'MODEL': 'LR', 'BASE': key, 'AUC': auc, 'KS': ks})\n",
    "\n",
    "    # Extrair coeficientes e odds ratios\n",
    "    coefs = pipeline_LR.named_steps['model'].coef_[0]\n",
    "    feat_names_lr = pipeline_LR.named_steps['prep'].get_feature_names_out()\n",
    "    df_coefs = pd.DataFrame({\n",
    "        'feature': feat_names_lr, 'coef': coefs,\n",
    "        'abs_coef': np.abs(coefs), 'odds_ratio': np.exp(coefs),\n",
    "    }).sort_values('abs_coef', ascending=False)\n",
    "    df_coefs.to_csv(f'{OUTPUT_DIR_V6}/lr_coefficients_odds_ratios.csv', index=False)\n",
    "    mlflow.log_artifact(f'{OUTPUT_DIR_V6}/lr_coefficients_odds_ratios.csv', 'feature_analysis')\n",
    "    mlflow.sklearn.log_model(pipeline_LR, 'model_lr_baseline')\n",
    "\n",
    "    print(f'\\nMLflow Run ID (LR): {run_lr.info.run_id}')\n",
    "    print(f'\\nTop 15 LR Coeficientes:')\n",
    "    print(df_coefs[['feature','coef','odds_ratio']].head(15).to_string(index=False))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v6_083",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 9.3 AVALIACAO LGBM + MLFLOW + FEATURE IMPORTANCES\n",
    "# =============================================================================\n",
    "with mlflow.start_run(run_name='LightGBM_Baseline_v6') as run_lgbm:\n",
    "    params_lgbm = pipeline_LGBM.named_steps['model'].get_params()\n",
    "    mlflow.log_param('model_type', 'LightGBM')\n",
    "    mlflow.log_param('n_estimators', params_lgbm.get('n_estimators'))\n",
    "    mlflow.log_param('max_depth', params_lgbm.get('max_depth'))\n",
    "    mlflow.log_param('learning_rate', params_lgbm.get('learning_rate'))\n",
    "    mlflow.log_param('n_features', len(X_train_final.columns))\n",
    "    mlflow.log_param('n_samples_train', len(X_train_final))\n",
    "\n",
    "    print('Avaliacao LGBM por base:')\n",
    "    map_data = generate_map_step_data(X_train_final, y_train_final, X_oos_agg, y_oos_agg, X_oot_agg, y_oot_agg)\n",
    "    for key in dict_safras:\n",
    "        X_f, y_f = filter_xy_by_safra(map_data[key]['X'], map_data[key]['Y'], dict_safras[key])\n",
    "        auc, ks = evaluation_auc_ks(X_f, y_f, pipeline_LGBM, key)\n",
    "        safe_key = _sanitize_mlflow_key(key)\n",
    "        mlflow.log_metric(f'LGBM_AUC_{safe_key}', auc)\n",
    "        mlflow.log_metric(f'LGBM_KS_{safe_key}', ks)\n",
    "        list_results_baseline.append({'MODEL': 'LGBM', 'BASE': key, 'AUC': auc, 'KS': ks})\n",
    "\n",
    "    # Feature importances\n",
    "    feat_names_lgbm = pipeline_LGBM.named_steps['prep'].get_feature_names_out()\n",
    "    lgbm_model = pipeline_LGBM.named_steps['model']\n",
    "    df_importance = pd.DataFrame({\n",
    "        'feature': feat_names_lgbm,\n",
    "        'importance': lgbm_model.feature_importances_,\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    df_importance.to_csv(f'{OUTPUT_DIR_V6}/lgbm_feature_importance_baseline.csv', index=False)\n",
    "    mlflow.log_artifact(f'{OUTPUT_DIR_V6}/lgbm_feature_importance_baseline.csv', 'feature_analysis')\n",
    "    mlflow.sklearn.log_model(pipeline_LGBM, 'model_lgbm_baseline')\n",
    "\n",
    "    print(f'\\nMLflow Run ID (LGBM): {run_lgbm.info.run_id}')\n",
    "\n",
    "df_results_baseline = pd.DataFrame(list_results_baseline)\n",
    "\n",
    "# Comparacao rapida OOT\n",
    "oot_lr = df_results_baseline[(df_results_baseline['MODEL']=='LR') & (df_results_baseline['BASE']=='OOT GERAL (CONS)')]\n",
    "oot_lgbm = df_results_baseline[(df_results_baseline['MODEL']=='LGBM') & (df_results_baseline['BASE']=='OOT GERAL (CONS)')]\n",
    "print(f\"\\nComparacao Baseline OOT:\")\n",
    "print(f\"  LR:   AUC={oot_lr['AUC'].values[0]:.5f}, KS={oot_lr['KS'].values[0]:.5f}\")\n",
    "print(f\"  LGBM: AUC={oot_lgbm['AUC'].values[0]:.5f}, KS={oot_lgbm['KS'].values[0]:.5f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v6_md_ksinc",
   "metadata": {},
   "source": [
    "## 10. KS Incremental por Fonte de Dados\n",
    "\n",
    "Analise de contribuicao marginal de cada fonte de dados (7 steps):\n",
    "Score1 → Score2 → Cadastro → Telco → Recarga → Pagamento → Faturamento\n",
    "\n",
    "Executado para ambos modelos (LR + LGBM) em 13 splits."
   ]
  },
  {
   "cell_type": "code",
   "id": "v6_091",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 10.1 DEFINIR 7 GRUPOS DE FEATURES\n",
    "# =============================================================================\n",
    "feat_score_1 = ['TARGET_SCORE_01']\n",
    "feat_score_2 = ['TARGET_SCORE_02']\n",
    "feats_cadastro = [x for x in X_train_final.columns if 'var_' in x and _var_num(x) <= 25]\n",
    "feats_cadastro += [c for c in ['STATUSRF', 'UF', 'REGIAO', 'DIAS_VAR_12'] if c in X_train_final.columns]\n",
    "feats_telco = [x for x in X_train_final.columns if 'var_' in x and _var_num(x) >= 26]\n",
    "feats_recargas = [x for x in X_train_final.columns if x.startswith('REC_')]\n",
    "feats_pagamentos = [x for x in X_train_final.columns if x.startswith('PAG_')]\n",
    "feats_faturamento = [x for x in X_train_final.columns if x.startswith('FAT_')]\n",
    "\n",
    "list_sources = [feat_score_1, feat_score_2, feats_cadastro, feats_telco,\n",
    "                feats_recargas, feats_pagamentos, feats_faturamento]\n",
    "\n",
    "print('Grupos de features para KS Incremental:')\n",
    "for i, (name, feats) in enumerate(zip(\n",
    "    ['Score1','Score2','Cadastro','Telco','Recarga','Pagamento','Faturamento'], list_sources)):\n",
    "    print(f'  Step {i}: {name} ({len(feats)} features)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v6_092",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 10.2 KS INCREMENTAL — 7 STEPS x 2 MODELOS x 13 SPLITS\n",
    "# COMENTADO PARA ACELERAR EXECUCAO — descomentar quando necessario\n",
    "# =============================================================================\n",
    "# list_features_inc = ['NUM_CPF', 'SAFRA']\n",
    "# list_dict_results_ks = []\n",
    "# list_models = ['LR', 'LGBM']\n",
    "# \n",
    "# for idx, source in enumerate(list_sources):\n",
    "#     list_features_inc.extend(source)\n",
    "#     X_tr_filt = X_train_final[list_features_inc]\n",
    "#     X_oos_filt = X_oos_agg[list_features_inc]\n",
    "#     X_oot_filt = X_oot_agg[list_features_inc]\n",
    "# \n",
    "#     for model_name in list_models:\n",
    "#         pipe = update_pipeline(X_tr_filt, name_model=model_name)\n",
    "#         pipe.fit(X_tr_filt, y_train_final)\n",
    "# \n",
    "#         for key in dict_safras:\n",
    "#             map_data = generate_map_step_data(\n",
    "#                 X_tr_filt, y_train_final, X_oos_filt, y_oos_agg, X_oot_filt, y_oot_agg)\n",
    "#             X_f, y_f = filter_xy_by_safra(map_data[key]['X'], map_data[key]['Y'], dict_safras[key])\n",
    "#             auc, ks = evaluation_auc_ks(X_f, y_f, pipe, key, verbose=False)\n",
    "#             list_dict_results_ks.append({\n",
    "#                 'MODELO': model_name, 'CONJ FEATURES': current_step(idx),\n",
    "#                 'BASE': key, 'AUC': auc, 'KS': ks,\n",
    "#             })\n",
    "# \n",
    "#     print(f'  Step {idx} ({current_step(idx)}): concluido')\n",
    "# \n",
    "# df_results_ks_inc = pd.DataFrame(list_dict_results_ks)\n",
    "# \n",
    "# print('\\n--- OOT GERAL (CONS) ---')\n",
    "# oot_inc = df_results_ks_inc[df_results_ks_inc['BASE'] == 'OOT GERAL (CONS)']\n",
    "# display(oot_inc.pivot_table(index='CONJ FEATURES', columns='MODELO', values='KS'))\n",
    "\n",
    "print('KS Incremental SKIP (comentado para acelerar)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v6_md_shap",
   "metadata": {},
   "source": [
    "## 11. Feature Selection (SHAP TreeExplainer)\n",
    "\n",
    "Mede contribuicao real de cada feature para a predicao de FPD, capturando interacoes.\n",
    "- Treinar LGBM dedicado (sem cadastro) → SHAP values via TreeExplainer\n",
    "- Ranking global por mean(|SHAP|) → selecionar features >= 90% cumulativo\n",
    "- Features SHAP aplicadas a AMBOS modelos (LR + LGBM)"
   ]
  },
  {
   "cell_type": "code",
   "id": "v6_101",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 11.1 TREINAR LGBM PARA SHAP + CALCULAR SHAP VALUES\n",
    "# =============================================================================\n",
    "feats_cadastro_shap = [x for x in X_train_final.columns if 'var_' in x and _var_num(x) <= 25]\n",
    "feats_cadastro_shap += [c for c in ['STATUSRF', 'UF', 'REGIAO', 'DIAS_VAR_12'] if c in X_train_final.columns]\n",
    "\n",
    "feats_to_use = [c for c in X_train_final.columns if c not in feats_cadastro_shap and c not in ['NUM_CPF', 'SAFRA']]\n",
    "X_shap = X_train_final[feats_to_use].copy()\n",
    "\n",
    "num_shap = [n for n in X_shap.select_dtypes(include=['int32','int64','float32','float64']).columns]\n",
    "cat_shap = [c for c in X_shap.select_dtypes(include=['object','category']).columns]\n",
    "\n",
    "prep_shap = ColumnTransformer([\n",
    "    ('num', Pipeline([('imputer', SimpleImputer(strategy='median'))]), num_shap),\n",
    "    ('cat', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', CountEncoder(normalize=True, handle_unknown=0, handle_missing=0)),\n",
    "    ]), cat_shap),\n",
    "], remainder='drop')\n",
    "\n",
    "pipe_shap = Pipeline([\n",
    "    ('prep', prep_shap),\n",
    "    ('model', LGBMClassifier(\n",
    "        objective='binary', boosting_type='gbdt', learning_rate=0.05,\n",
    "        n_estimators=300, max_depth=7, colsample_bytree=0.8, subsample=0.8,\n",
    "        random_state=42, n_jobs=-1, verbosity=-1,\n",
    "    )),\n",
    "])\n",
    "pipe_shap.fit(X_shap, y_train_final)\n",
    "\n",
    "# SHAP values\n",
    "X_shap_transformed = pipe_shap.named_steps['prep'].transform(X_shap)\n",
    "try:\n",
    "    transformed_names = pipe_shap.named_steps['prep'].get_feature_names_out()\n",
    "except Exception:\n",
    "    transformed_names = num_shap + cat_shap\n",
    "\n",
    "explainer = shap.TreeExplainer(pipe_shap.named_steps['model'])\n",
    "shap_values = explainer.shap_values(X_shap_transformed)\n",
    "shap_vals = shap_values[1] if isinstance(shap_values, list) else shap_values\n",
    "print(f'SHAP values: {shap_vals.shape}')\n",
    "\n",
    "# Ranking\n",
    "_original_features_ordered = num_shap + cat_shap\n",
    "\n",
    "def _map_transformed_to_original(name):\n",
    "    raw = name.split('__', 1)[-1] if '__' in name else name\n",
    "    try:\n",
    "        idx = int(raw)\n",
    "        if 0 <= idx < len(_original_features_ordered):\n",
    "            return _original_features_ordered[idx]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return raw\n",
    "\n",
    "mean_abs_shap = np.abs(shap_vals).mean(axis=0)\n",
    "df_shap_ranking = pd.DataFrame({\n",
    "    'feature_transformed': list(transformed_names),\n",
    "    'mean_abs_shap': mean_abs_shap,\n",
    "}).sort_values('mean_abs_shap', ascending=False).reset_index(drop=True)\n",
    "\n",
    "df_shap_ranking['feature'] = df_shap_ranking['feature_transformed'].apply(_map_transformed_to_original)\n",
    "\n",
    "def get_book_label(feat):\n",
    "    if feat.startswith('REC_'): return 'Recarga (REC_)'\n",
    "    elif feat.startswith('PAG_'): return 'Pagamento (PAG_)'\n",
    "    elif feat.startswith('FAT_'): return 'Faturamento (FAT_)'\n",
    "    return 'Base (Telco+Score)'\n",
    "\n",
    "df_shap_ranking['book'] = df_shap_ranking['feature'].apply(get_book_label)\n",
    "total_shap = df_shap_ranking['mean_abs_shap'].sum()\n",
    "df_shap_ranking['pct_importance'] = df_shap_ranking['mean_abs_shap'] / total_shap\n",
    "df_shap_ranking['cumulative_pct'] = df_shap_ranking['pct_importance'].cumsum()\n",
    "df_shap_ranking['rank'] = range(1, len(df_shap_ranking) + 1)\n",
    "\n",
    "print(f'\\nTop 15 Features (SHAP):')\n",
    "print(df_shap_ranking[['rank','feature','book','mean_abs_shap','pct_importance']].head(15).to_string(index=False))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v6_102",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 11.2 SELECIONAR FEATURES 90% CUMULATIVO + EXPORT\n",
    "# =============================================================================\n",
    "CUMULATIVE_THRESHOLD = 0.90\n",
    "\n",
    "selected_mask = df_shap_ranking['cumulative_pct'] <= CUMULATIVE_THRESHOLD\n",
    "if not selected_mask.all():\n",
    "    first_over = selected_mask[~selected_mask].index[0]\n",
    "    selected_mask.iloc[:first_over + 1] = True\n",
    "final_set_features = df_shap_ranking[selected_mask]['feature'].unique().tolist()\n",
    "\n",
    "print(f'Features selecionadas (SHAP >= 90%): {len(final_set_features)}')\n",
    "print(f'Importancia capturada: {df_shap_ranking[selected_mask][\"pct_importance\"].sum():.1%}')\n",
    "\n",
    "# Export SHAP artifacts\n",
    "shap_dir = '/tmp/shap_artifacts_v6'\n",
    "os.makedirs(shap_dir, exist_ok=True)\n",
    "df_shap_ranking.to_csv(f'{shap_dir}/shap_feature_ranking.csv', index=False)\n",
    "with open(f'{shap_dir}/selected_features_shap.pkl', 'wb') as f:\n",
    "    pickle.dump(final_set_features, f)\n",
    "\n",
    "with mlflow.start_run(run_name='SHAP_Feature_Selection_v6'):\n",
    "    mlflow.set_tag('task', 'feature_selection')\n",
    "    mlflow.log_param('n_features_total', len(feats_to_use))\n",
    "    mlflow.log_param('n_features_selected', len(final_set_features))\n",
    "    mlflow.log_artifact(f'{shap_dir}/shap_feature_ranking.csv', 'feature_selection')\n",
    "    mlflow.log_artifact(f'{shap_dir}/selected_features_shap.pkl', 'feature_selection')\n",
    "\n",
    "print(f'Artifacts salvos em {shap_dir}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v6_md_final",
   "metadata": {},
   "source": [
    "## 12. Modelos Finais com Features SHAP\n",
    "\n",
    "Treinar AMBOS modelos (LR L1 + LGBM) com as features selecionadas pelo SHAP.\n",
    "Avaliar em 13 splits. Determinar melhor modelo por KS OOT."
   ]
  },
  {
   "cell_type": "code",
   "id": "v6_111",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 12. TREINO FINAL AMBOS MODELOS COM FEATURES SHAP + AVALIACAO\n",
    "# =============================================================================\n",
    "dimensions = ['NUM_CPF', 'SAFRA']\n",
    "final_features_with_dims = final_set_features + [d for d in dimensions if d not in final_set_features]\n",
    "\n",
    "X_tr_final_fs = X_train_final[final_features_with_dims]\n",
    "X_oos_fs = X_oos_agg[final_features_with_dims]\n",
    "X_oot_fs = X_oot_agg[final_features_with_dims]\n",
    "\n",
    "models_to_train = ['LR', 'LGBM']\n",
    "list_results_fs = []\n",
    "best_model_name = None\n",
    "best_model_pipeline = None\n",
    "best_ks_oot = 0\n",
    "pipelines_final = {}\n",
    "\n",
    "for model_name in models_to_train:\n",
    "    run_label = f\"Final_{model_name}_SHAP_v6\"\n",
    "    with mlflow.start_run(run_name=run_label) as run:\n",
    "        pipe = update_pipeline(X_tr_final_fs, name_model=model_name)\n",
    "        pipe.fit(X_tr_final_fs, y_train_final)\n",
    "        pipelines_final[model_name] = pipe\n",
    "\n",
    "        mlflow.log_param('model_type', model_name)\n",
    "        mlflow.log_param('n_features', len(final_set_features))\n",
    "        mlflow.log_param('feature_selection', 'SHAP_TreeExplainer_90pct')\n",
    "        model_params = pipe.named_steps['model'].get_params()\n",
    "        for k, v in model_params.items():\n",
    "            if isinstance(v, (int, float, str, bool)):\n",
    "                mlflow.log_param(f'model__{k}', v)\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f'MODELO FINAL: {model_name} ({len(final_set_features)} features SHAP)')\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        map_data = generate_map_step_data(X_tr_final_fs, y_train_final, X_oos_fs, y_oos_agg, X_oot_fs, y_oot_agg)\n",
    "        for key in dict_safras:\n",
    "            X_f, y_f = filter_xy_by_safra(map_data[key]['X'], map_data[key]['Y'], dict_safras[key])\n",
    "            auc, ks = evaluation_auc_ks(X_f, y_f, pipe, key)\n",
    "            safe_key = _sanitize_mlflow_key(key)\n",
    "            mlflow.log_metric(f'AUC_{safe_key}', auc)\n",
    "            mlflow.log_metric(f'KS_{safe_key}', ks)\n",
    "            list_results_fs.append({'MODEL': model_name, 'BASE': key, 'AUC': auc, 'KS': ks})\n",
    "\n",
    "            if key == 'OOT GERAL (CONS)' and ks > best_ks_oot:\n",
    "                best_ks_oot = ks\n",
    "                best_model_name = model_name\n",
    "                best_model_pipeline = pipe\n",
    "\n",
    "        mlflow.sklearn.log_model(pipe, f\"model_final_{model_name.lower()}_shap\")\n",
    "        print(f'MLflow Run ID ({model_name}): {run.info.run_id}')\n",
    "\n",
    "df_results_fs = pd.DataFrame(list_results_fs)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f'MELHOR MODELO (KS OOT): {best_model_name} — KS={best_ks_oot:.5f}')\n",
    "print(f\"{'='*60}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v6_md_swap",
   "metadata": {},
   "source": [
    "## 13. Swap Analysis — Modelo vs FPD Real\n",
    "\n",
    "Compara o ranking do **melhor modelo** com o ranking real (flag FPD) na **mesma populacao** (metodologia v5 corrigida).\n",
    "\n",
    "- **Ranking Oracle**: FPD=1 primeiro, desempate por score modelo\n",
    "- **Ranking Modelo**: Ordenar por P(FPD=1) descendente\n",
    "- **Overlap**: Acertos — maus reais capturados pelo modelo\n",
    "- **Swap-in**: No top X% do MODELO mas NAO do ORACLE (falsos alarmes)\n",
    "- **Swap-out**: No top X% do ORACLE mas NAO do MODELO (maus que escapam)"
   ]
  },
  {
   "cell_type": "code",
   "id": "v6_121",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 13. SWAP ANALYSIS CORRIGIDO — MODELO vs FPD REAL (mesma populacao)\n",
    "# =============================================================================\n",
    "print(f'Modelo: {best_model_name}')\n",
    "print('=' * 60)\n",
    "\n",
    "# Scores para AMBOS modelos no OOT\n",
    "scores_oot_best = best_model_pipeline.predict_proba(X_oot_fs)[:, 1]\n",
    "scores_oot_lr = pipelines_final['LR'].predict_proba(X_oot_fs)[:, 1]\n",
    "scores_oot_lgbm = pipelines_final['LGBM'].predict_proba(X_oot_fs)[:, 1]\n",
    "\n",
    "df_swap = pd.DataFrame({\n",
    "    'SAFRA': X_oot_fs['SAFRA'].values,\n",
    "    'score': scores_oot_best,\n",
    "    'score_lr': scores_oot_lr,\n",
    "    'score_lgbm': scores_oot_lgbm,\n",
    "    'FPD': y_oot_agg.values,\n",
    "})\n",
    "\n",
    "total_bad = (df_swap['FPD'] == 1).sum()\n",
    "total_pop = len(df_swap)\n",
    "print(f'Populacao OOT: {total_pop:,} | Maus (FPD=1): {total_bad:,} ({total_bad/total_pop:.2%})')\n",
    "\n",
    "swap_results = []\n",
    "for cutoff in SWAP_CUTOFFS:\n",
    "    n_top = int(total_pop * cutoff)\n",
    "    model_top_idx = df_swap.nlargest(n_top, 'score').index\n",
    "    df_swap['_oracle_rank'] = df_swap['FPD'] * 1e6 + df_swap['score']\n",
    "    oracle_top_idx = df_swap.nlargest(n_top, '_oracle_rank').index\n",
    "\n",
    "    overlap_idx = model_top_idx.intersection(oracle_top_idx)\n",
    "    swap_in_idx = model_top_idx.difference(oracle_top_idx)\n",
    "    swap_out_idx = oracle_top_idx.difference(model_top_idx)\n",
    "\n",
    "    bad_captured = (df_swap.loc[model_top_idx, 'FPD'] == 1).sum()\n",
    "    capture_rate = bad_captured / total_bad if total_bad > 0 else 0\n",
    "\n",
    "    result = {\n",
    "        'cutoff': f'{cutoff:.0%}', 'n_top': n_top,\n",
    "        'overlap': len(overlap_idx), 'swap_in': len(swap_in_idx), 'swap_out': len(swap_out_idx),\n",
    "        'overlap_pct': len(overlap_idx) / n_top * 100,\n",
    "        'swap_in_pct': len(swap_in_idx) / n_top * 100,\n",
    "        'swap_out_pct': len(swap_out_idx) / n_top * 100,\n",
    "        'capture_rate': capture_rate * 100,\n",
    "        'default_rate_model': df_swap.loc[model_top_idx, 'FPD'].mean() * 100,\n",
    "        'default_rate_oracle': df_swap.loc[oracle_top_idx, 'FPD'].mean() * 100,\n",
    "    }\n",
    "    swap_results.append(result)\n",
    "    print(f\"\\nTop {cutoff:.0%} (n={n_top:,}):\")\n",
    "    print(f\"  Overlap: {result['overlap']:>6,} ({result['overlap_pct']:.1f}%)\")\n",
    "    print(f\"  Swap-in: {result['swap_in']:>6,} ({result['swap_in_pct']:.1f}%)\")\n",
    "    print(f\"  Swap-out:{result['swap_out']:>6,} ({result['swap_out_pct']:.1f}%)\")\n",
    "    print(f\"  Capture: {result['capture_rate']:.1f}%\")\n",
    "\n",
    "df_swap.drop(columns=['_oracle_rank'], inplace=True)\n",
    "df_swap_results = pd.DataFrame(swap_results)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v6_md_decis",
   "metadata": {},
   "source": [
    "## 14. Analise por Decis + PSI\n",
    "\n",
    "- **Decis**: 10 faixas de score — bad rate e lift por faixa (ambos modelos)\n",
    "- **PSI**: Estabilidade da distribuicao de scores entre safras\n",
    "  - PSI < 0.10: Estavel | 0.10-0.25: Atencao | > 0.25: Critico"
   ]
  },
  {
   "cell_type": "code",
   "id": "v6_131",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 14. ANALISE POR DECIS + PSI (AMBOS MODELOS)\n",
    "# =============================================================================\n",
    "\n",
    "def decile_analysis(y_true, y_score, n_bins=10):\n",
    "    df_d = pd.DataFrame({'y': y_true, 'score': y_score})\n",
    "    df_d = df_d.sort_values('score', ascending=False).reset_index(drop=True)\n",
    "    df_d['decil'] = pd.qcut(df_d['score'], n_bins, labels=False, duplicates='drop')\n",
    "    df_d['decil'] = df_d['decil'].max() - df_d['decil'] + 1\n",
    "    result = df_d.groupby('decil').agg(\n",
    "        n=('y', 'count'), n_bad=('y', 'sum'),\n",
    "        score_min=('score', 'min'), score_max=('score', 'max'),\n",
    "        score_mean=('score', 'mean'),\n",
    "    ).sort_index().reset_index()\n",
    "    result['n_good'] = result['n'] - result['n_bad']\n",
    "    result['bad_rate'] = result['n_bad'] / result['n'] * 100\n",
    "    result['pct_pop'] = result['n'] / result['n'].sum() * 100\n",
    "    result['pct_bad'] = result['n_bad'] / result['n_bad'].sum() * 100\n",
    "    result['cum_bad_pct'] = result['pct_bad'].cumsum()\n",
    "    result['cum_pop_pct'] = result['pct_pop'].cumsum()\n",
    "    avg_bad_rate = result['n_bad'].sum() / result['n'].sum() * 100\n",
    "    result['lift'] = result['bad_rate'] / avg_bad_rate\n",
    "    return result\n",
    "\n",
    "def calc_psi(expected, actual, bins=10):\n",
    "    breakpoints = np.linspace(0, 1, bins + 1)\n",
    "    exp_pct = np.histogram(expected, bins=breakpoints)[0] / len(expected)\n",
    "    act_pct = np.histogram(actual, bins=breakpoints)[0] / len(actual)\n",
    "    exp_pct = np.where(exp_pct == 0, 0.0001, exp_pct)\n",
    "    act_pct = np.where(act_pct == 0, 0.0001, act_pct)\n",
    "    return float(np.sum((act_pct - exp_pct) * np.log(act_pct / exp_pct)))\n",
    "\n",
    "# Decis ambos modelos\n",
    "dec_oot_best = decile_analysis(y_oot_agg.values, scores_oot_best)\n",
    "dec_oot_lr = decile_analysis(y_oot_agg.values, scores_oot_lr)\n",
    "dec_oot_lgbm = decile_analysis(y_oot_agg.values, scores_oot_lgbm)\n",
    "\n",
    "print('=== Decis OOT — Best Model ===')\n",
    "print(dec_oot_best[['decil','n','n_bad','bad_rate','cum_bad_pct','lift']].to_string(index=False))\n",
    "\n",
    "# PSI (ambos modelos)\n",
    "scores_train_lr = pipelines_final['LR'].predict_proba(X_tr_final_fs)[:, 1]\n",
    "scores_train_lgbm = pipelines_final['LGBM'].predict_proba(X_tr_final_fs)[:, 1]\n",
    "\n",
    "safras_oot_list = sorted(df_swap['SAFRA'].unique())\n",
    "psi_results = []\n",
    "\n",
    "for model_name, s_train, s_oot in [('LR', scores_train_lr, scores_oot_lr),\n",
    "                                      ('LGBM', scores_train_lgbm, scores_oot_lgbm)]:\n",
    "    psi_val = calc_psi(s_train, s_oot)\n",
    "    status = 'OK' if psi_val < 0.1 else 'ATENCAO' if psi_val < 0.25 else 'CRITICO'\n",
    "    psi_results.append({'modelo': model_name, 'comparacao': 'Train vs OOT', 'psi': psi_val, 'status': status})\n",
    "    print(f'\\nPSI {model_name} Train vs OOT: {psi_val:.4f} [{status}]')\n",
    "\n",
    "    if len(safras_oot_list) >= 2:\n",
    "        s1 = df_swap.loc[df_swap['SAFRA']==safras_oot_list[0], f'score_{model_name.lower()}'].values\n",
    "        s2 = df_swap.loc[df_swap['SAFRA']==safras_oot_list[1], f'score_{model_name.lower()}'].values\n",
    "        psi_12 = calc_psi(s1, s2)\n",
    "        status = 'OK' if psi_12 < 0.1 else 'ATENCAO' if psi_12 < 0.25 else 'CRITICO'\n",
    "        psi_results.append({'modelo': model_name, 'comparacao': 'OOT1 vs OOT2', 'psi': psi_12, 'status': status})\n",
    "        print(f'PSI {model_name} OOT1 vs OOT2: {psi_12:.4f} [{status}]')\n",
    "\n",
    "df_psi = pd.DataFrame(psi_results)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v6_md_viz",
   "metadata": {},
   "source": [
    "## 15. Visualizacoes (26 graficos)\n",
    "\n",
    "### Panel 1 — Performance (8 plots): KS, ROC, PR, Score Dist, Confusion Matrix\n",
    "### Panel 2 — Stability (8 plots): Decile, Lift, PSI, Swap, Calibracao\n",
    "### Panel 3 — Business (8 plots): Model Comparison, Coefs LR, Odds Ratios, Risk Bands\n",
    "### SHAP Standalone (2 plots): Beeswarm + Pareto\n",
    "### KS Incremental (1 plot): Contribuicao marginal por fonte"
   ]
  },
  {
   "cell_type": "code",
   "id": "v6_201",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 15.1 PANEL 1 — PERFORMANCE (8 plots, 2x4)\n",
    "# =============================================================================\n",
    "def get_feature_color(feat):\n",
    "    if feat.startswith('REC_') or '__REC_' in feat: return BOOK_COLORS['Recarga (REC_)']\n",
    "    elif feat.startswith('PAG_') or '__PAG_' in feat: return BOOK_COLORS['Pagamento (PAG_)']\n",
    "    elif feat.startswith('FAT_') or '__FAT_' in feat: return BOOK_COLORS['Faturamento (FAT_)']\n",
    "    return BOOK_COLORS['Base (Telco+Score)']\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(32, 14))\n",
    "fig.suptitle('Panel 1 — Performance: LR L1 vs LGBM (OOT)', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Helper: KS curve data\n",
    "def _ks_curve_data(y, s):\n",
    "    d = pd.DataFrame({'y': y, 's': s}).sort_values('s')\n",
    "    d['cg'] = (1-d['y']).cumsum()/(1-d['y']).sum()\n",
    "    d['cb'] = d['y'].cumsum()/d['y'].sum()\n",
    "    d['ks'] = np.abs(d['cb']-d['cg'])\n",
    "    return d\n",
    "\n",
    "# 1. KS LR\n",
    "ax = axes[0,0]\n",
    "d = _ks_curve_data(y_oot_agg.values, scores_oot_lr)\n",
    "x = np.linspace(0,1,len(d))\n",
    "ax.plot(x, d['cg'].values, label='Bons', color=COLORS['blue'])\n",
    "ax.plot(x, d['cb'].values, label='Maus', color=COLORS['red'])\n",
    "ks_lr = d['ks'].max()\n",
    "ax.set_title(f'1. KS Curve LR | KS={ks_lr:.4f}', fontweight='bold')\n",
    "ax.legend(fontsize=8); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. KS LGBM\n",
    "ax = axes[0,1]\n",
    "d = _ks_curve_data(y_oot_agg.values, scores_oot_lgbm)\n",
    "x = np.linspace(0,1,len(d))\n",
    "ax.plot(x, d['cg'].values, label='Bons', color=COLORS['blue'])\n",
    "ax.plot(x, d['cb'].values, label='Maus', color=COLORS['red'])\n",
    "ks_lgbm = d['ks'].max()\n",
    "ax.set_title(f'2. KS Curve LGBM | KS={ks_lgbm:.4f}', fontweight='bold')\n",
    "ax.legend(fontsize=8); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. ROC Dual\n",
    "ax = axes[0,2]\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_oot_agg, scores_oot_lr)\n",
    "fpr_lgbm, tpr_lgbm, _ = roc_curve(y_oot_agg, scores_oot_lgbm)\n",
    "auc_lr = roc_auc_score(y_oot_agg, scores_oot_lr)\n",
    "auc_lgbm = roc_auc_score(y_oot_agg, scores_oot_lgbm)\n",
    "ax.plot(fpr_lr, tpr_lr, color=COLORS['orange'], linewidth=2, label=f'LR AUC={auc_lr:.4f}')\n",
    "ax.plot(fpr_lgbm, tpr_lgbm, color=COLORS['blue'], linewidth=2, label=f'LGBM AUC={auc_lgbm:.4f}')\n",
    "ax.plot([0,1],[0,1],'k--',alpha=0.3)\n",
    "ax.set_title('3. ROC Curve (Dual)', fontweight='bold'); ax.legend(fontsize=9); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. PR Dual\n",
    "ax = axes[0,3]\n",
    "prec_lr, rec_lr, _ = precision_recall_curve(y_oot_agg, scores_oot_lr)\n",
    "prec_lgbm, rec_lgbm, _ = precision_recall_curve(y_oot_agg, scores_oot_lgbm)\n",
    "ax.plot(rec_lr, prec_lr, color=COLORS['orange'], linewidth=2, label='LR')\n",
    "ax.plot(rec_lgbm, prec_lgbm, color=COLORS['blue'], linewidth=2, label='LGBM')\n",
    "ax.axhline(y=y_oot_agg.mean(), color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_title('4. Precision-Recall (Dual)', fontweight='bold'); ax.legend(fontsize=9); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Score Dist LR\n",
    "ax = axes[1,0]\n",
    "ax.hist(scores_oot_lr[y_oot_agg.values==0], bins=50, alpha=0.6, label='Bons', color=COLORS['green'], density=True)\n",
    "ax.hist(scores_oot_lr[y_oot_agg.values==1], bins=50, alpha=0.6, label='Maus', color=COLORS['red'], density=True)\n",
    "ax.set_title('5. Score Dist LR', fontweight='bold'); ax.legend(fontsize=8); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Score Dist LGBM\n",
    "ax = axes[1,1]\n",
    "ax.hist(scores_oot_lgbm[y_oot_agg.values==0], bins=50, alpha=0.6, label='Bons', color=COLORS['green'], density=True)\n",
    "ax.hist(scores_oot_lgbm[y_oot_agg.values==1], bins=50, alpha=0.6, label='Maus', color=COLORS['red'], density=True)\n",
    "ax.set_title('6. Score Dist LGBM', fontweight='bold'); ax.legend(fontsize=8); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. CM LR\n",
    "ax = axes[1,2]\n",
    "y_pred_lr = (scores_oot_lr >= 0.5).astype(int)\n",
    "cm_lr = confusion_matrix(y_oot_agg, y_pred_lr)\n",
    "ax.imshow(cm_lr, interpolation='nearest', cmap=plt.cm.Oranges)\n",
    "ax.set_title('7. CM LR (t=0.5)', fontweight='bold')\n",
    "ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
    "ax.set_xticklabels(['Bom','Mau']); ax.set_yticklabels(['Bom','Mau'])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax.text(j, i, f'{cm_lr[i,j]:,}', ha='center', va='center',\n",
    "                color='white' if cm_lr[i,j] > cm_lr.max()/2 else 'black', fontsize=11)\n",
    "\n",
    "# 8. CM LGBM\n",
    "ax = axes[1,3]\n",
    "y_pred_lgbm = (scores_oot_lgbm >= 0.5).astype(int)\n",
    "cm_lgbm = confusion_matrix(y_oot_agg, y_pred_lgbm)\n",
    "ax.imshow(cm_lgbm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.set_title('8. CM LGBM (t=0.5)', fontweight='bold')\n",
    "ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
    "ax.set_xticklabels(['Bom','Mau']); ax.set_yticklabels(['Bom','Mau'])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax.text(j, i, f'{cm_lgbm[i,j]:,}', ha='center', va='center',\n",
    "                color='white' if cm_lgbm[i,j] > cm_lgbm.max()/2 else 'black', fontsize=11)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(f'{OUTPUT_DIR_V6}/panel1_performance_8plots.png', dpi=DPI, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Panel 1 salvo (8 graficos)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v6_211",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 15.2 PANEL 2 — STABILITY (8 plots, 2x4)\n",
    "# =============================================================================\n",
    "fig, axes = plt.subplots(2, 4, figsize=(32, 14))\n",
    "fig.suptitle(f'Panel 2 — Estabilidade & Swap ({best_model_name} best)', fontsize=18, fontweight='bold')\n",
    "\n",
    "# 9. Decile Bad Rate (best model)\n",
    "ax = axes[0,0]\n",
    "bars = ax.bar(dec_oot_best['decil'], dec_oot_best['bad_rate'], color=COLORS['red'], alpha=0.8, edgecolor='white')\n",
    "avg_br = dec_oot_best['n_bad'].sum() / dec_oot_best['n'].sum() * 100\n",
    "ax.axhline(y=avg_br, color='gray', linestyle='--', alpha=0.7, label=f'Media={avg_br:.1f}%')\n",
    "ax.set_title(f'9. Bad Rate por Decil ({best_model_name})', fontweight='bold')\n",
    "ax.set_xlabel('Decil (1=pior)'); ax.set_ylabel('Bad Rate (%)')\n",
    "ax.legend(fontsize=8); ax.grid(True, alpha=0.3, axis='y')\n",
    "for bar, val in zip(bars, dec_oot_best['bad_rate']):\n",
    "    ax.text(bar.get_x()+bar.get_width()/2, val+0.3, f'{val:.1f}%', ha='center', fontsize=7)\n",
    "\n",
    "# 10. Lift Dual\n",
    "ax = axes[0,1]\n",
    "ax.bar(np.arange(len(dec_oot_lr))-0.2, dec_oot_lr['lift'], 0.4, label='LR', color=COLORS['orange'], alpha=0.8)\n",
    "ax.bar(np.arange(len(dec_oot_lgbm))+0.2, dec_oot_lgbm['lift'], 0.4, label='LGBM', color=COLORS['blue'], alpha=0.8)\n",
    "ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.7)\n",
    "ax.set_title('10. Lift por Decil (Dual)', fontweight='bold')\n",
    "ax.set_xlabel('Decil'); ax.set_ylabel('Lift'); ax.legend(fontsize=8); ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 11. Cumulative Capture Dual\n",
    "ax = axes[0,2]\n",
    "ax.plot(dec_oot_lr['cum_pop_pct'], dec_oot_lr['cum_bad_pct'], 'o-', color=COLORS['orange'], linewidth=2, label='LR')\n",
    "ax.plot(dec_oot_lgbm['cum_pop_pct'], dec_oot_lgbm['cum_bad_pct'], 's-', color=COLORS['blue'], linewidth=2, label='LGBM')\n",
    "ax.plot([0,100],[0,100],'k--',alpha=0.3, label='Aleatorio')\n",
    "ax.set_title('11. Curva de Captura (Dual)', fontweight='bold')\n",
    "ax.set_xlabel('% Pop'); ax.set_ylabel('% Maus Capturados')\n",
    "ax.legend(fontsize=8); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 12. KS por Base Dual\n",
    "ax = axes[0,3]\n",
    "oos_oot_lr = df_results_fs[(df_results_fs['MODEL']=='LR') & (df_results_fs['BASE'].str.contains('OOS|OOT'))]\n",
    "oos_oot_lgbm = df_results_fs[(df_results_fs['MODEL']=='LGBM') & (df_results_fs['BASE'].str.contains('OOS|OOT'))]\n",
    "x_pos = np.arange(len(oos_oot_lr))\n",
    "ax.bar(x_pos-0.2, oos_oot_lr['KS'].values, 0.4, label='LR', color=COLORS['orange'], alpha=0.8)\n",
    "ax.bar(x_pos+0.2, oos_oot_lgbm['KS'].values, 0.4, label='LGBM', color=COLORS['blue'], alpha=0.8)\n",
    "ax.set_xticks(x_pos); ax.set_xticklabels(oos_oot_lr['BASE'].values, rotation=45, ha='right', fontsize=6)\n",
    "ax.axhline(y=0.20, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_title('12. KS por Base (Dual)', fontweight='bold'); ax.set_ylabel('KS')\n",
    "ax.legend(fontsize=8); ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 13. Score Boxplot OOT\n",
    "ax = axes[1,0]\n",
    "data_bp = []\n",
    "labels_bp = []\n",
    "for safra in safras_oot_list:\n",
    "    data_bp.append(df_swap.loc[df_swap['SAFRA']==safra, 'score_lr'].values)\n",
    "    labels_bp.append(f'LR-{safra}')\n",
    "    data_bp.append(df_swap.loc[df_swap['SAFRA']==safra, 'score_lgbm'].values)\n",
    "    labels_bp.append(f'LGBM-{safra}')\n",
    "bp = ax.boxplot(data_bp, labels=labels_bp, patch_artist=True)\n",
    "cols_bp = [COLORS['orange'], COLORS['blue']] * len(safras_oot_list)\n",
    "for patch, c in zip(bp['boxes'], cols_bp):\n",
    "    patch.set_facecolor(c); patch.set_alpha(0.6)\n",
    "ax.set_title('13. Score por SAFRA OOT', fontweight='bold')\n",
    "ax.tick_params(axis='x', rotation=45); ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 14. PSI Bar\n",
    "ax = axes[1,1]\n",
    "psi_colors = [COLORS['green'] if r['psi']<0.1 else COLORS['orange'] if r['psi']<0.25 else COLORS['red']\n",
    "              for r in psi_results]\n",
    "labels_psi = [f\"{r['modelo']}: {r['comparacao']}\" for r in psi_results]\n",
    "ax.barh(range(len(psi_results)), [r['psi'] for r in psi_results], color=psi_colors, edgecolor='white')\n",
    "ax.set_yticks(range(len(psi_results))); ax.set_yticklabels(labels_psi, fontsize=8)\n",
    "ax.axvline(x=0.10, color='orange', linestyle='--', alpha=0.5, label='Atencao')\n",
    "ax.axvline(x=0.25, color='red', linestyle='--', alpha=0.5, label='Critico')\n",
    "ax.set_title('14. PSI (Dual)', fontweight='bold'); ax.set_xlabel('PSI')\n",
    "ax.legend(fontsize=7); ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 15. Swap Stacked Bar\n",
    "ax = axes[1,2]\n",
    "x_sw = range(len(df_swap_results))\n",
    "ax.bar(x_sw, df_swap_results['overlap_pct'], label='Overlap', color=COLORS['green'], alpha=0.8)\n",
    "ax.bar(x_sw, df_swap_results['swap_in_pct'], bottom=df_swap_results['overlap_pct'],\n",
    "       label='Swap-in', color=COLORS['orange'], alpha=0.8)\n",
    "ax.bar(x_sw, df_swap_results['swap_out_pct'],\n",
    "       bottom=df_swap_results['overlap_pct']+df_swap_results['swap_in_pct'],\n",
    "       label='Swap-out', color=COLORS['red'], alpha=0.8)\n",
    "ax.set_xticks(x_sw); ax.set_xticklabels(df_swap_results['cutoff'], fontsize=9)\n",
    "ax.set_title(f'15. Swap Analysis ({best_model_name})', fontweight='bold')\n",
    "ax.set_xlabel('Top %'); ax.set_ylabel('% do Top')\n",
    "ax.legend(fontsize=7); ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 16. Calibration Dual\n",
    "ax = axes[1,3]\n",
    "for name, scores, color in [('LR', scores_oot_lr, COLORS['orange']), ('LGBM', scores_oot_lgbm, COLORS['blue'])]:\n",
    "    df_cal = pd.DataFrame({'y': y_oot_agg.values, 'score': scores})\n",
    "    df_cal['bin'] = pd.qcut(df_cal['score'], 10, duplicates='drop')\n",
    "    cal = df_cal.groupby('bin', observed=True).agg(pred=('score','mean'), obs=('y','mean')).reset_index()\n",
    "    ax.plot(cal['pred'], cal['obs'], 'o-', color=color, markersize=6, label=name)\n",
    "ax.plot([0, 0.5], [0, 0.5], 'k--', alpha=0.3, label='Perfeito')\n",
    "ax.set_title('16. Calibracao (Dual)', fontweight='bold')\n",
    "ax.set_xlabel('Score Predito'); ax.set_ylabel('Bad Rate Obs')\n",
    "ax.legend(fontsize=8); ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(f'{OUTPUT_DIR_V6}/panel2_stability_8plots.png', dpi=DPI, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Panel 2 salvo (8 graficos)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v6_221",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 15.3 PANEL 3 — BUSINESS (8 plots, 2x4)\n",
    "# =============================================================================\n",
    "fig, axes = plt.subplots(2, 4, figsize=(32, 14))\n",
    "fig.suptitle('Panel 3 — Business Intelligence: LR L1 vs LGBM', fontsize=18, fontweight='bold')\n",
    "\n",
    "# 17. Model Comparison Dashboard\n",
    "ax = axes[0,0]\n",
    "metrics_comp = {\n",
    "    'AUC': [auc_lr, auc_lgbm],\n",
    "    'KS': [ks_lr, ks_lgbm],\n",
    "    'Gini': [(2*auc_lr-1)*100, (2*auc_lgbm-1)*100],\n",
    "}\n",
    "x_m = np.arange(len(metrics_comp))\n",
    "w = 0.35\n",
    "vals_lr = [v[0] for v in metrics_comp.values()]\n",
    "vals_lgbm = [v[1] for v in metrics_comp.values()]\n",
    "ax.bar(x_m-w/2, vals_lr, w, label='LR', color=COLORS['orange'], alpha=0.8)\n",
    "ax.bar(x_m+w/2, vals_lgbm, w, label='LGBM', color=COLORS['blue'], alpha=0.8)\n",
    "ax.set_xticks(x_m); ax.set_xticklabels(metrics_comp.keys())\n",
    "ax.set_title('17. Model Comparison', fontweight='bold')\n",
    "ax.legend(fontsize=9); ax.grid(True, alpha=0.3, axis='y')\n",
    "for i, (vl, vg) in enumerate(zip(vals_lr, vals_lgbm)):\n",
    "    ax.text(i-w/2, vl+0.005, f'{vl:.3f}', ha='center', fontsize=8)\n",
    "    ax.text(i+w/2, vg+0.005, f'{vg:.3f}', ha='center', fontsize=8)\n",
    "\n",
    "# 18. LR Top 20 Coefs\n",
    "ax = axes[0,1]\n",
    "top20_coefs = df_coefs.head(20)\n",
    "colors_coef = ['#F44336' if c > 0 else '#2196F3' for c in top20_coefs['coef']]\n",
    "ax.barh(range(len(top20_coefs)-1,-1,-1), top20_coefs['coef'].values, color=colors_coef, alpha=0.85)\n",
    "ax.set_yticks(range(len(top20_coefs)-1,-1,-1))\n",
    "ax.set_yticklabels(top20_coefs['feature'].values, fontsize=7)\n",
    "ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "ax.set_title('18. LR Top 20 Coeficientes', fontweight='bold')\n",
    "ax.set_xlabel('Coeficiente'); ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 19. LR Odds Ratios (top 20)\n",
    "ax = axes[0,2]\n",
    "top20_or = df_coefs.head(20).copy()\n",
    "top20_or['or_display'] = top20_or['odds_ratio'].clip(upper=5)\n",
    "colors_or = [COLORS['red'] if o > 1 else COLORS['green'] for o in top20_or['odds_ratio']]\n",
    "ax.barh(range(len(top20_or)-1,-1,-1), top20_or['or_display'].values, color=colors_or, alpha=0.85)\n",
    "ax.set_yticks(range(len(top20_or)-1,-1,-1))\n",
    "ax.set_yticklabels(top20_or['feature'].values, fontsize=7)\n",
    "ax.axvline(x=1.0, color='black', linewidth=1, linestyle='--')\n",
    "ax.set_title('19. LR Odds Ratios (Top 20)', fontweight='bold')\n",
    "ax.set_xlabel('Odds Ratio'); ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 20. Approval Rate vs FPD Rate\n",
    "ax = axes[0,3]\n",
    "pcts = np.linspace(0.1, 1.0, 19)\n",
    "approval_fpd_lr = []\n",
    "approval_fpd_lgbm = []\n",
    "for p in pcts:\n",
    "    pctl = np.clip((1-p)*100, 0, 100)\n",
    "    thresh_lr = np.percentile(scores_oot_lr, pctl)\n",
    "    approved_lr = y_oot_agg.values[scores_oot_lr <= thresh_lr]\n",
    "    approval_fpd_lr.append(approved_lr.mean()*100 if len(approved_lr)>0 else 0)\n",
    "    thresh_lgbm = np.percentile(scores_oot_lgbm, pctl)\n",
    "    approved_lgbm = y_oot_agg.values[scores_oot_lgbm <= thresh_lgbm]\n",
    "    approval_fpd_lgbm.append(approved_lgbm.mean()*100 if len(approved_lgbm)>0 else 0)\n",
    "ax.plot(pcts*100, approval_fpd_lr, 'o-', color=COLORS['orange'], label='LR', markersize=4)\n",
    "ax.plot(pcts*100, approval_fpd_lgbm, 's-', color=COLORS['blue'], label='LGBM', markersize=4)\n",
    "ax.set_title('20. Approval Rate vs FPD Rate', fontweight='bold')\n",
    "ax.set_xlabel('% Aprovados'); ax.set_ylabel('FPD Rate (%)')\n",
    "ax.legend(fontsize=9); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 21. Risk Bands (best model)\n",
    "ax = axes[1,0]\n",
    "df_risk = pd.DataFrame({'score': scores_oot_best, 'FPD': y_oot_agg.values})\n",
    "df_risk['band'] = pd.cut(df_risk['score'], bins=[0,0.1,0.2,0.3,0.5,1.0],\n",
    "                          labels=['Muito Baixo','Baixo','Medio','Alto','Muito Alto'])\n",
    "risk_stats = df_risk.groupby('band', observed=True).agg(n=('FPD','count'), bad_rate=('FPD','mean')).reset_index()\n",
    "risk_stats['bad_rate'] = risk_stats['bad_rate'] * 100\n",
    "bar_colors = [COLORS['green'], COLORS['teal'], COLORS['orange'], COLORS['red'], COLORS['purple']]\n",
    "bars_r = ax.bar(range(len(risk_stats)), risk_stats['bad_rate'], color=bar_colors[:len(risk_stats)], alpha=0.8)\n",
    "ax.set_xticks(range(len(risk_stats))); ax.set_xticklabels(risk_stats['band'], rotation=30, ha='right', fontsize=8)\n",
    "ax.set_title(f'21. Risk Bands ({best_model_name})', fontweight='bold')\n",
    "ax.set_ylabel('Bad Rate (%)'); ax.grid(True, alpha=0.3, axis='y')\n",
    "for bar, val, n in zip(bars_r, risk_stats['bad_rate'], risk_stats['n']):\n",
    "    ax.text(bar.get_x()+bar.get_width()/2, val+0.3, f'{val:.1f}%\\n(n={n:,})', ha='center', fontsize=7)\n",
    "\n",
    "# 22. Feature Importance LGBM by Book (top 20)\n",
    "ax = axes[1,1]\n",
    "lgbm_final_model = pipelines_final['LGBM'].named_steps['model']\n",
    "try:\n",
    "    fn_lgbm_final = pipelines_final['LGBM'].named_steps['prep'].get_feature_names_out()\n",
    "except Exception:\n",
    "    nf_f = [n for n in X_tr_final_fs.select_dtypes(include=['int32','int64','float32','float64']).columns if n != 'SAFRA']\n",
    "    cf_f = [c for c in X_tr_final_fs.select_dtypes(include=['object','category']).columns if c != 'NUM_CPF']\n",
    "    fn_lgbm_final = nf_f + cf_f\n",
    "df_imp_final = pd.DataFrame({'feature': fn_lgbm_final, 'importance': lgbm_final_model.feature_importances_})\n",
    "df_imp_final = df_imp_final.sort_values('importance', ascending=False).head(20)\n",
    "colors_imp = [get_feature_color(f.split('__')[-1] if '__' in f else f) for f in df_imp_final['feature']]\n",
    "ax.barh(range(len(df_imp_final)-1,-1,-1), df_imp_final['importance'].values, color=colors_imp, alpha=0.85)\n",
    "ax.set_yticks(range(len(df_imp_final)-1,-1,-1))\n",
    "ax.set_yticklabels(df_imp_final['feature'].values, fontsize=7)\n",
    "ax.set_title('22. Feature Importance LGBM (Top 20)', fontweight='bold')\n",
    "ax.set_xlabel('Importance'); ax.grid(True, alpha=0.3, axis='x')\n",
    "legend_elements = [Patch(facecolor=c, label=l) for l, c in BOOK_COLORS.items()]\n",
    "ax.legend(handles=legend_elements, loc='lower right', fontsize=7)\n",
    "\n",
    "# 23. AUC por Split Dual\n",
    "ax = axes[1,2]\n",
    "groups = {'TREINO': {'LR': [], 'LGBM': []}, 'OOS': {'LR': [], 'LGBM': []}, 'OOT': {'LR': [], 'LGBM': []}}\n",
    "for _, row in df_results_fs.iterrows():\n",
    "    if 'TREINO' in row['BASE'] and 'CONS' not in row['BASE']:\n",
    "        groups['TREINO'][row['MODEL']].append(row['AUC'])\n",
    "    elif 'OOS' in row['BASE'] and 'CONS' not in row['BASE']:\n",
    "        groups['OOS'][row['MODEL']].append(row['AUC'])\n",
    "    elif 'OOT' in row['BASE'] and 'CONS' not in row['BASE']:\n",
    "        groups['OOT'][row['MODEL']].append(row['AUC'])\n",
    "x_g = np.arange(3)\n",
    "lr_means = [np.mean(groups[g]['LR']) if groups[g]['LR'] else 0 for g in ['TREINO','OOS','OOT']]\n",
    "lgbm_means = [np.mean(groups[g]['LGBM']) if groups[g]['LGBM'] else 0 for g in ['TREINO','OOS','OOT']]\n",
    "ax.bar(x_g-0.2, lr_means, 0.4, label='LR', color=COLORS['orange'], alpha=0.8)\n",
    "ax.bar(x_g+0.2, lgbm_means, 0.4, label='LGBM', color=COLORS['blue'], alpha=0.8)\n",
    "ax.set_xticks(x_g); ax.set_xticklabels(['TREINO','OOS','OOT'])\n",
    "ax.axhline(y=0.65, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_title('23. AUC Medio por Split (Dual)', fontweight='bold'); ax.set_ylabel('AUC')\n",
    "ax.legend(fontsize=9); ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 24. Capture Rate Dual\n",
    "ax = axes[1,3]\n",
    "pcts_cap = np.linspace(0.01, 1.0, 100)\n",
    "cap_lr = []; cap_lgbm = []\n",
    "for p in pcts_cap:\n",
    "    n = int(total_pop * p)\n",
    "    top_lr = df_swap.nlargest(n, 'score_lr').index\n",
    "    top_lgbm = df_swap.nlargest(n, 'score_lgbm').index\n",
    "    cap_lr.append((df_swap.loc[top_lr, 'FPD'] == 1).sum() / total_bad * 100)\n",
    "    cap_lgbm.append((df_swap.loc[top_lgbm, 'FPD'] == 1).sum() / total_bad * 100)\n",
    "ax.plot(pcts_cap*100, cap_lr, color=COLORS['orange'], linewidth=2, label='LR')\n",
    "ax.plot(pcts_cap*100, cap_lgbm, color=COLORS['blue'], linewidth=2, label='LGBM')\n",
    "ax.plot([0,100],[0,100],'k--',alpha=0.3)\n",
    "ax.set_title('24. Capture Rate (Dual)', fontweight='bold')\n",
    "ax.set_xlabel('% Pop Avaliada'); ax.set_ylabel('% Maus Capturados')\n",
    "ax.legend(fontsize=9); ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(f'{OUTPUT_DIR_V6}/panel3_business_8plots.png', dpi=DPI, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Panel 3 salvo (8 graficos)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v6_231",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 15.4 SHAP VISUALIZACOES STANDALONE\n",
    "# =============================================================================\n",
    "\n",
    "# 25. SHAP Beeswarm (top 40)\n",
    "fig, ax = plt.subplots(figsize=(14, 16))\n",
    "shap.summary_plot(shap_vals, X_shap_transformed,\n",
    "                  feature_names=list(transformed_names), max_display=40, show=False)\n",
    "plt.title('25. SHAP Summary Plot - Top 40 Features (FPD)', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR_V6}/shap_beeswarm_top40.png', dpi=DPI, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 26. Pareto Cumulative\n",
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "x_range = range(1, len(df_shap_ranking) + 1)\n",
    "colors_cum = [get_feature_color(f) for f in df_shap_ranking['feature']]\n",
    "ax.bar(x_range, df_shap_ranking['pct_importance'].values, color=colors_cum, alpha=0.7, width=1.0)\n",
    "ax2_p = ax.twinx()\n",
    "ax2_p.plot(x_range, df_shap_ranking['cumulative_pct'].values, color='red', linewidth=2)\n",
    "ax2_p.axhline(y=CUMULATIVE_THRESHOLD, color='red', linestyle='--', alpha=0.5)\n",
    "n_90 = (df_shap_ranking['cumulative_pct'] <= CUMULATIVE_THRESHOLD).sum()\n",
    "ax2_p.axvline(x=n_90, color='green', linestyle='--', alpha=0.7)\n",
    "ax2_p.annotate(f'{n_90} features = 90%', xy=(n_90, 0.90), xytext=(n_90+20, 0.80),\n",
    "              arrowprops=dict(arrowstyle='->', color='green'), fontsize=12, fontweight='bold', color='green')\n",
    "ax.set_xlabel('Feature (rank SHAP)'); ax.set_ylabel('Importancia Individual (%)')\n",
    "ax2_p.set_ylabel('Cumulativo (%)')\n",
    "ax.set_title(f'26. Pareto — {n_90} features capturam 90% importancia SHAP', fontsize=14, fontweight='bold')\n",
    "legend_elems = [Patch(facecolor=c, label=l) for l, c in BOOK_COLORS.items()]\n",
    "legend_elems.append(Line2D([0],[0], color='red', linewidth=2, label='Cumulativo'))\n",
    "ax.legend(handles=legend_elems, loc='center right', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR_V6}/shap_pareto_cumulative.png', dpi=DPI, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('SHAP plots salvos (2 graficos)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v6_241",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 15.5 KS INCREMENTAL VISUALIZATION\n",
    "# COMENTADO — depende de df_results_ks_inc (secao 10.2)\n",
    "# =============================================================================\n",
    "# fig, ax = plt.subplots(figsize=(14, 7))\n",
    "# \n",
    "# oot_ks_inc = df_results_ks_inc[df_results_ks_inc['BASE'] == 'OOT GERAL (CONS)']\n",
    "# steps_order = ['Score1', 'Score1+Score2', '+Cadastro', '+Telco', '+Recarga', '+Pagamento', '+Faturamento']\n",
    "# \n",
    "# for model_name, color, marker in [('LR', COLORS['orange'], 'o'), ('LGBM', COLORS['blue'], 's')]:\n",
    "#     model_data = oot_ks_inc[oot_ks_inc['MODELO'] == model_name]\n",
    "#     ks_vals = []\n",
    "#     for step in steps_order:\n",
    "#         row = model_data[model_data['CONJ FEATURES'] == step]\n",
    "#         ks_vals.append(row['KS'].values[0] if len(row) > 0 else 0)\n",
    "#     ax.plot(range(len(steps_order)), ks_vals, f'{marker}-', color=color, linewidth=2.5,\n",
    "#             markersize=10, label=model_name)\n",
    "#     for i, v in enumerate(ks_vals):\n",
    "#         ax.annotate(f'{v:.3f}', xy=(i, v), xytext=(0, 10), textcoords='offset points',\n",
    "#                    ha='center', fontsize=9, fontweight='bold', color=color)\n",
    "# \n",
    "# ax.set_xticks(range(len(steps_order)))\n",
    "# ax.set_xticklabels(steps_order, rotation=30, ha='right', fontsize=10)\n",
    "# ax.set_xlabel('Conjunto de Features (incremental)', fontsize=11)\n",
    "# ax.set_ylabel('KS (OOT)', fontsize=11)\n",
    "# ax.set_title('KS Incremental por Fonte de Dados — LR L1 vs LGBM (OOT)', fontsize=14, fontweight='bold')\n",
    "# ax.legend(fontsize=12)\n",
    "# ax.grid(True, alpha=0.3)\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f'{OUTPUT_DIR_V6}/ks_incremental_dual.png', dpi=DPI, bbox_inches='tight')\n",
    "# plt.show()\n",
    "# print('KS Incremental plot salvo')\n",
    "\n",
    "print('KS Incremental viz SKIP (comentado)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v6_md_export",
   "metadata": {},
   "source": [
    "## 16. Export & MLflow Registry\n",
    "\n",
    "- Log todas visualizacoes + CSVs no MLflow\n",
    "- Export AMBOS modelos via `export_model()`\n",
    "- Promover melhor modelo para Production"
   ]
  },
  {
   "cell_type": "code",
   "id": "v6_250",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 16.1 LOG VISUALIZACOES NO MLFLOW\n",
    "# =============================================================================\n",
    "import glob as glob_mod\n",
    "\n",
    "with mlflow.start_run(run_name='Final_Visualizations_v6') as run_viz:\n",
    "    mlflow.set_tag('task', 'visualization')\n",
    "    mlflow.set_tag('version', 'v6')\n",
    "\n",
    "    for fig_path in glob_mod.glob(f'{OUTPUT_DIR_V6}/*.png'):\n",
    "        mlflow.log_artifact(fig_path, 'plots_v6')\n",
    "\n",
    "    # Salvar CSVs\n",
    "    df_swap_results.to_csv(f'{OUTPUT_DIR_V6}/swap_analysis_results.csv', index=False)\n",
    "    dec_oot_best.to_csv(f'{OUTPUT_DIR_V6}/decile_analysis_oot.csv', index=False)\n",
    "    df_psi.to_csv(f'{OUTPUT_DIR_V6}/psi_results.csv', index=False)\n",
    "    df_results_fs.to_csv(f'{OUTPUT_DIR_V6}/model_comparison_results.csv', index=False)\n",
    "    if 'df_results_ks_inc' in dir():\n",
    "        df_results_ks_inc.to_csv(f'{OUTPUT_DIR_V6}/ks_incremental_results.csv', index=False)\n",
    "\n",
    "    for csv_path in glob_mod.glob(f'{OUTPUT_DIR_V6}/*.csv'):\n",
    "        mlflow.log_artifact(csv_path, 'analysis_v6')\n",
    "\n",
    "    # Metricas OOT\n",
    "    mlflow.log_metric('ks_oot_lr', ks_lr)\n",
    "    mlflow.log_metric('ks_oot_lgbm', ks_lgbm)\n",
    "    mlflow.log_metric('auc_oot_lr', auc_lr)\n",
    "    mlflow.log_metric('auc_oot_lgbm', auc_lgbm)\n",
    "    mlflow.log_metric('gini_oot_lr', (2*auc_lr-1)*100)\n",
    "    mlflow.log_metric('gini_oot_lgbm', (2*auc_lgbm-1)*100)\n",
    "\n",
    "    psi_best = [r for r in psi_results if r['modelo']==best_model_name and r['comparacao']=='Train vs OOT']\n",
    "    if psi_best:\n",
    "        mlflow.log_metric('psi_train_vs_oot', psi_best[0]['psi'])\n",
    "\n",
    "    print(f'MLflow Run ID (viz): {run_viz.info.run_id}')\n",
    "    print(f'Visualizacoes + CSVs logados no MLflow')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v6_261",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 16.2 EXPORT AMBOS MODELOS VIA MLFLOW REGISTRY\n",
    "# =============================================================================\n",
    "sys.path.insert(0, '/lakehouse/default/Files/projeto-final/5-treinamento-modelos')\n",
    "from export_model import export_model, promote_to_production\n",
    "\n",
    "feature_names_export = [f for f in final_set_features if f not in ['NUM_CPF', 'SAFRA']]\n",
    "X_oot_exp = X_oot_fs.drop(columns=['NUM_CPF', 'SAFRA'], errors='ignore')\n",
    "X_oos_exp = X_oos_fs.drop(columns=['NUM_CPF', 'SAFRA'], errors='ignore')\n",
    "\n",
    "# Metricas comuns\n",
    "def _compute_export_metrics(pipe, model_label):\n",
    "    ks_oot = ks_stat(y_oot_agg, pipe.predict_proba(X_oot_fs)[:, 1])\n",
    "    auc_oot = roc_auc_score(y_oot_agg, pipe.predict_proba(X_oot_fs)[:, 1])\n",
    "    ks_oos = ks_stat(y_oos_agg, pipe.predict_proba(X_oos_fs)[:, 1])\n",
    "    auc_oos = roc_auc_score(y_oos_agg, pipe.predict_proba(X_oos_fs)[:, 1])\n",
    "    m = {\n",
    "        'ks_oot': ks_oot, 'auc_oot': auc_oot,\n",
    "        'ks_oos': ks_oos, 'auc_oos': auc_oos,\n",
    "        'gini_oot': (2*auc_oot-1)*100, 'gini_oos': (2*auc_oos-1)*100,\n",
    "        'n_features': len(feature_names_export), 'version': 'v6',\n",
    "    }\n",
    "    # OOS 202501 especifico (para validacao_deploy)\n",
    "    X_501, y_501 = filter_xy_by_safra(X_oos_fs, y_oos_agg, [202501])\n",
    "    if len(y_501) > 0:\n",
    "        p = pipe.predict_proba(X_501)[:, 1]\n",
    "        m['ks_oos_202501'] = ks_stat(y_501, p)\n",
    "        m['auc_oos_202501'] = roc_auc_score(y_501, p)\n",
    "        m['gini_oos_202501'] = (2*m['auc_oos_202501']-1)*100\n",
    "    return m\n",
    "\n",
    "# Export LR\n",
    "print('=== Export LR L1 ===')\n",
    "metrics_lr = _compute_export_metrics(pipelines_final['LR'], 'lr')\n",
    "result_lr = export_model(\n",
    "    pipeline=pipelines_final['LR'], model_name='lr_l1_v6',\n",
    "    X_test=X_oot_exp, y_test=y_oot_agg,\n",
    "    feature_names=feature_names_export, metrics_dict=metrics_lr,\n",
    ")\n",
    "print(f\"  Registered: {result_lr['registered_name']}\")\n",
    "print(f\"  KS OOT: {metrics_lr['ks_oot']:.5f}, AUC OOT: {metrics_lr['auc_oot']:.5f}\")\n",
    "\n",
    "# Export LGBM\n",
    "print('\\n=== Export LGBM ===')\n",
    "metrics_lgbm = _compute_export_metrics(pipelines_final['LGBM'], 'lgbm')\n",
    "result_lgbm = export_model(\n",
    "    pipeline=pipelines_final['LGBM'], model_name='lgbm_baseline_v6',\n",
    "    X_test=X_oot_exp, y_test=y_oot_agg,\n",
    "    feature_names=feature_names_export, metrics_dict=metrics_lgbm,\n",
    ")\n",
    "print(f\"  Registered: {result_lgbm['registered_name']}\")\n",
    "print(f\"  KS OOT: {metrics_lgbm['ks_oot']:.5f}, AUC OOT: {metrics_lgbm['auc_oot']:.5f}\")\n",
    "\n",
    "# Promover melhor modelo para Production\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f'Promovendo melhor modelo ({best_model_name}) para Production...')\n",
    "if best_model_name == 'LR':\n",
    "    best_result = result_lr\n",
    "else:\n",
    "    best_result = result_lgbm\n",
    "version = promote_to_production(best_result['registered_name'])\n",
    "print(f\"Modelo {best_result['registered_name']} v{version} em Production!\")\n",
    "print(f\"{'='*60}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v6_271",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 17. RESUMO FINAL\n",
    "# =============================================================================\n",
    "print('=' * 70)\n",
    "print('  MODELO BASELINE v6 — RESUMO FINAL')\n",
    "print('=' * 70)\n",
    "print(f'\\n  Melhor Modelo: {best_model_name}')\n",
    "print(f'  KS OOT:  {best_ks_oot:.5f}')\n",
    "if best_model_name == 'LR':\n",
    "    best_metrics = metrics_lr\n",
    "else:\n",
    "    best_metrics = metrics_lgbm\n",
    "print(f'  AUC OOT: {best_metrics[\"auc_oot\"]:.5f}')\n",
    "print(f'  Gini OOT: {best_metrics[\"gini_oot\"]:.1f}%')\n",
    "print(f'  KS OOS:  {best_metrics[\"ks_oos\"]:.5f}')\n",
    "print(f'  Features SHAP: {len(final_set_features)}')\n",
    "print(f'\\n  Comparacao:')\n",
    "print(f'    LR L1:  KS={metrics_lr[\"ks_oot\"]:.5f}  AUC={metrics_lr[\"auc_oot\"]:.5f}  Gini={metrics_lr[\"gini_oot\"]:.1f}%')\n",
    "print(f'    LGBM:   KS={metrics_lgbm[\"ks_oot\"]:.5f}  AUC={metrics_lgbm[\"auc_oot\"]:.5f}  Gini={metrics_lgbm[\"gini_oot\"]:.1f}%')\n",
    "print(f'\\n  PSI:')\n",
    "for r in psi_results:\n",
    "    print(f'    {r[\"modelo\"]} {r[\"comparacao\"]}: {r[\"psi\"]:.4f} [{r[\"status\"]}]')\n",
    "print(f'\\n  Visualizacoes: 26 graficos em {OUTPUT_DIR_V6}/')\n",
    "print(f'  MLflow: Ambos modelos registrados, {best_model_name} em Production')\n",
    "print(f'\\n  Modelos exportados:')\n",
    "print(f'    LR:   {result_lr[\"registered_name\"]}')\n",
    "print(f'    LGBM: {result_lgbm[\"registered_name\"]}')\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print('  v6 completo — dual model + 26 viz + swap corrigido + SHAP + PSI')\n",
    "print(f'{\"=\"*70}')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}