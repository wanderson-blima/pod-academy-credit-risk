{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analise Swap e Visualizacoes — Modelo Baseline FPD\n",
    "\n",
    "**Story**: HD-3.3 | **Epic**: EPIC-HD-001 | **Entregavel**: C/D\n",
    "- Swap analysis entre SAFRAs 202502 vs 202503\n",
    "- 8 tipos de visualizacao exportadas em dpi=150\n",
    "- Modelo: Logistic Regression (L1) + LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, \"/lakehouse/default/Files/projeto-final\")\n",
    "from config.pipeline_config import GOLD_BASE, SAFRAS\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, roc_auc_score, confusion_matrix,\n",
    "    precision_recall_curve, classification_report\n",
    ")\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "OUTPUT_DIR = \"/lakehouse/default/Files/projeto-final/docs/analytics\"\n",
    "DPI = 150\n",
    "\n",
    "# Paleta profissional\n",
    "COLORS = {\n",
    "    \"blue\": \"#2196F3\",\n",
    "    \"orange\": \"#FF9800\",\n",
    "    \"green\": \"#4CAF50\",\n",
    "    \"red\": \"#F44336\",\n",
    "    \"purple\": \"#9C27B0\",\n",
    "    \"gray\": \"#607D8B\"\n",
    "}\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette([COLORS[\"blue\"], COLORS[\"orange\"], COLORS[\"green\"], COLORS[\"red\"]])\n",
    "\n",
    "print(\"Setup completo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Dados e Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota: Este notebook deve ser executado APOS o modelo baseline\n",
    "# As variaveis abaixo devem estar no escopo do notebook principal\n",
    "# Se executando standalone, carregar do .pkl:\n",
    "\n",
    "# import joblib\n",
    "# pipeline_LR = joblib.load(\"artifacts/logistic_regression_l1_*.pkl\")\n",
    "# pipeline_LGBM = joblib.load(\"artifacts/lgbm_baseline_*.pkl\")\n",
    "\n",
    "# Gerar scores para cada SAFRA OOT\n",
    "try:\n",
    "    scores_oot1_lr = pipeline_LR.predict_proba(X_oot1)[:, 1]\n",
    "    scores_oot2_lr = pipeline_LR.predict_proba(X_oot2)[:, 1]\n",
    "    scores_oot1_lgbm = pipeline_LGBM.predict_proba(X_oot1)[:, 1]\n",
    "    scores_oot2_lgbm = pipeline_LGBM.predict_proba(X_oot2)[:, 1]\n",
    "    print(f\"OOT1 (202502): {len(scores_oot1_lr):,} registros\")\n",
    "    print(f\"OOT2 (202503): {len(scores_oot2_lr):,} registros\")\n",
    "except NameError:\n",
    "    print(\"AVISO: Variaveis do modelo nao encontradas no escopo.\")\n",
    "    print(\"Execute o notebook modelo_baseline primeiro ou carregue do .pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Grafico 1 — Curva KS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ks_curve(y_true, y_scores, title, ax):\n",
    "    \"\"\"Plota curva KS (Kolmogorov-Smirnov) comparando distribuicoes acumuladas.\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    ks_stat = max(tpr - fpr)\n",
    "    ks_idx = np.argmax(tpr - fpr)\n",
    "\n",
    "    x_axis = np.linspace(0, 1, len(tpr))\n",
    "    ax.plot(x_axis, tpr, label='Bad (TPR)', color=COLORS[\"red\"], linewidth=1.5)\n",
    "    ax.plot(x_axis, fpr, label='Good (FPR)', color=COLORS[\"blue\"], linewidth=1.5)\n",
    "    ax.fill_between(x_axis, fpr, tpr, alpha=0.1, color=COLORS[\"green\"])\n",
    "    ax.axvline(x=ks_idx / len(tpr), color=COLORS[\"gray\"], linestyle='--', alpha=0.7)\n",
    "    ax.set_title(f\"{title}\\nKS = {ks_stat:.4f}\", fontsize=11)\n",
    "    ax.set_xlabel(\"Population %\")\n",
    "    ax.set_ylabel(\"Cumulative %\")\n",
    "    ax.legend(loc='lower right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    return ks_stat\n",
    "\n",
    "\n",
    "try:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    ks1 = plot_ks_curve(y_oot1, scores_oot1_lr, \"LR — OOT1 (202502)\", axes[0, 0])\n",
    "    ks2 = plot_ks_curve(y_oot2, scores_oot2_lr, \"LR — OOT2 (202503)\", axes[0, 1])\n",
    "    ks3 = plot_ks_curve(y_oot1, scores_oot1_lgbm, \"LGBM — OOT1 (202502)\", axes[1, 0])\n",
    "    ks4 = plot_ks_curve(y_oot2, scores_oot2_lgbm, \"LGBM — OOT2 (202503)\", axes[1, 1])\n",
    "    plt.suptitle(\"Curvas KS por Modelo e SAFRA OOT\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/fig_ks_curves.png\", dpi=DPI, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"KS LR:   OOT1={ks1:.4f}, OOT2={ks2:.4f}\")\n",
    "    print(f\"KS LGBM: OOT1={ks3:.4f}, OOT2={ks4:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao gerar curvas KS: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Grafico 2 — Distribuicao de Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    plot_configs = [\n",
    "        (scores_oot1_lr,   y_oot1, \"LR — OOT1 (202502)\",   axes[0, 0]),\n",
    "        (scores_oot2_lr,   y_oot2, \"LR — OOT2 (202503)\",   axes[0, 1]),\n",
    "        (scores_oot1_lgbm, y_oot1, \"LGBM — OOT1 (202502)\", axes[1, 0]),\n",
    "        (scores_oot2_lgbm, y_oot2, \"LGBM — OOT2 (202503)\", axes[1, 1]),\n",
    "    ]\n",
    "\n",
    "    for scores, y_true, title, ax in plot_configs:\n",
    "        mask_0 = y_true == 0\n",
    "        mask_1 = y_true == 1\n",
    "        ax.hist(scores[mask_0], bins=50, alpha=0.6, label=\"FPD=0 (Good)\",\n",
    "                color=COLORS[\"blue\"], density=True, edgecolor='white', linewidth=0.5)\n",
    "        ax.hist(scores[mask_1], bins=50, alpha=0.6, label=\"FPD=1 (Bad)\",\n",
    "                color=COLORS[\"red\"], density=True, edgecolor='white', linewidth=0.5)\n",
    "        ax.set_title(title, fontsize=11)\n",
    "        ax.set_xlabel(\"Score (P(FPD=1))\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(\"Distribuicao de Scores por FPD (0 vs 1)\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/fig_score_distribution.png\", dpi=DPI, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Distribuicao de scores salva.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao gerar distribuicao de scores: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Grafico 3 — ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # --- LR ---\n",
    "    fpr_1, tpr_1, _ = roc_curve(y_oot1, scores_oot1_lr)\n",
    "    fpr_2, tpr_2, _ = roc_curve(y_oot2, scores_oot2_lr)\n",
    "    auc_1 = roc_auc_score(y_oot1, scores_oot1_lr)\n",
    "    auc_2 = roc_auc_score(y_oot2, scores_oot2_lr)\n",
    "    axes[0].plot(fpr_1, tpr_1, label=f\"OOT1 (AUC={auc_1:.4f})\", color=COLORS[\"blue\"], linewidth=1.5)\n",
    "    axes[0].plot(fpr_2, tpr_2, label=f\"OOT2 (AUC={auc_2:.4f})\", color=COLORS[\"orange\"], linewidth=1.5)\n",
    "    axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.4, label=\"Random\")\n",
    "    axes[0].set_title(\"ROC — Logistic Regression (L1)\", fontsize=12)\n",
    "    axes[0].set_xlabel(\"False Positive Rate\")\n",
    "    axes[0].set_ylabel(\"True Positive Rate\")\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # --- LGBM ---\n",
    "    fpr_3, tpr_3, _ = roc_curve(y_oot1, scores_oot1_lgbm)\n",
    "    fpr_4, tpr_4, _ = roc_curve(y_oot2, scores_oot2_lgbm)\n",
    "    auc_3 = roc_auc_score(y_oot1, scores_oot1_lgbm)\n",
    "    auc_4 = roc_auc_score(y_oot2, scores_oot2_lgbm)\n",
    "    axes[1].plot(fpr_3, tpr_3, label=f\"OOT1 (AUC={auc_3:.4f})\", color=COLORS[\"blue\"], linewidth=1.5)\n",
    "    axes[1].plot(fpr_4, tpr_4, label=f\"OOT2 (AUC={auc_4:.4f})\", color=COLORS[\"orange\"], linewidth=1.5)\n",
    "    axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.4, label=\"Random\")\n",
    "    axes[1].set_title(\"ROC — LightGBM (GBDT)\", fontsize=12)\n",
    "    axes[1].set_xlabel(\"False Positive Rate\")\n",
    "    axes[1].set_ylabel(\"True Positive Rate\")\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(\"Curvas ROC por Modelo — OOT1 vs OOT2\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/fig_roc_curves.png\", dpi=DPI, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"AUC LR:   OOT1={auc_1:.4f}, OOT2={auc_2:.4f}\")\n",
    "    print(f\"AUC LGBM: OOT1={auc_3:.4f}, OOT2={auc_4:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao gerar curvas ROC: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Grafico 4 — Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(y_true, y_scores):\n",
    "    \"\"\"Encontra threshold que maximiza KS (TPR - FPR).\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    ks_values = tpr - fpr\n",
    "    best_idx = np.argmax(ks_values)\n",
    "    return thresholds[best_idx]\n",
    "\n",
    "\n",
    "try:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "    configs = [\n",
    "        (y_oot1, scores_oot1_lr,   \"LR — OOT1 (202502)\",   axes[0, 0]),\n",
    "        (y_oot2, scores_oot2_lr,   \"LR — OOT2 (202503)\",   axes[0, 1]),\n",
    "        (y_oot1, scores_oot1_lgbm, \"LGBM — OOT1 (202502)\", axes[1, 0]),\n",
    "        (y_oot2, scores_oot2_lgbm, \"LGBM — OOT2 (202503)\", axes[1, 1]),\n",
    "    ]\n",
    "\n",
    "    for y_true, y_scores, title, ax in configs:\n",
    "        threshold = find_optimal_threshold(y_true, y_scores)\n",
    "        y_pred = (y_scores >= threshold).astype(int)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        sns.heatmap(\n",
    "            cm, annot=True, fmt=\",d\", cmap=\"Blues\", ax=ax,\n",
    "            xticklabels=[\"Predicted 0\", \"Predicted 1\"],\n",
    "            yticklabels=[\"Actual 0\", \"Actual 1\"],\n",
    "            linewidths=0.5, linecolor='white'\n",
    "        )\n",
    "        ax.set_title(f\"{title}\\n(threshold={threshold:.4f})\", fontsize=11)\n",
    "\n",
    "    plt.suptitle(\"Confusion Matrix — Threshold Otimo (max KS)\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/fig_confusion_matrix.png\", dpi=DPI, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Confusion matrices salvas.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao gerar confusion matrices: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Grafico 5 — Feature Importance (LGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Extrair feature importances do LGBM (ultimo step do pipeline)\n",
    "    lgbm_model = pipeline_LGBM.named_steps.get(\n",
    "        \"lgbmclassifier\",\n",
    "        pipeline_LGBM.steps[-1][1]\n",
    "    )\n",
    "    importances = lgbm_model.feature_importances_\n",
    "\n",
    "    # Nomes das features — usar X_oot1 ou feature_names se disponiveis\n",
    "    try:\n",
    "        feat_names = list(X_oot1.columns)\n",
    "    except AttributeError:\n",
    "        feat_names = [f\"feature_{i}\" for i in range(len(importances))]\n",
    "\n",
    "    # Top 30\n",
    "    fi_df = pd.DataFrame({\"feature\": feat_names, \"importance\": importances})\n",
    "    fi_df = fi_df.sort_values(\"importance\", ascending=False).head(30)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 12))\n",
    "    ax.barh(\n",
    "        fi_df[\"feature\"].values[::-1],\n",
    "        fi_df[\"importance\"].values[::-1],\n",
    "        color=COLORS[\"blue\"],\n",
    "        edgecolor='white',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "    ax.set_xlabel(\"Importance (split)\", fontsize=11)\n",
    "    ax.set_title(\"Top 30 Feature Importance — LightGBM\", fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "    # Anotar com prefix de origem\n",
    "    for i, (feat, imp) in enumerate(zip(fi_df[\"feature\"].values[::-1], fi_df[\"importance\"].values[::-1])):\n",
    "        prefix = feat.split(\"_\")[0] if \"_\" in feat else \"\"\n",
    "        if prefix in [\"REC\", \"PAG\", \"FAT\"]:\n",
    "            color_map = {\"REC\": COLORS[\"green\"], \"PAG\": COLORS[\"orange\"], \"FAT\": COLORS[\"purple\"]}\n",
    "            ax.get_children()[i].set_color(color_map[prefix])\n",
    "\n",
    "    # Legenda manual para prefixes\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=COLORS[\"green\"],  label='REC_ (Recarga)'),\n",
    "        Patch(facecolor=COLORS[\"orange\"], label='PAG_ (Pagamento)'),\n",
    "        Patch(facecolor=COLORS[\"purple\"], label='FAT_ (Faturamento)'),\n",
    "        Patch(facecolor=COLORS[\"blue\"],   label='Cadastro/Telco/Score'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='lower right', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/fig_feature_importance_lgbm.png\", dpi=DPI, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nTop 10 features:\")\n",
    "    print(fi_df.head(10).to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao gerar feature importance: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Grafico 6 — KS por SAFRA (Estabilidade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ks(y_true, y_scores):\n",
    "    \"\"\"Calcula estatistica KS.\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    return max(tpr - fpr)\n",
    "\n",
    "\n",
    "try:\n",
    "    # SAFRAs de treino: 202410-202501, OOT1: 202502, OOT2: 202503\n",
    "    # Nota: para treino, usar scores in-sample; para OOT, usar out-of-sample\n",
    "    # Se X_train_by_safra nao estiver disponivel, usar apenas OOT\n",
    "\n",
    "    safra_labels = []\n",
    "    ks_lr_values = []\n",
    "    ks_lgbm_values = []\n",
    "\n",
    "    # Tentar carregar scores por safra de treino (se disponiveis)\n",
    "    try:\n",
    "        train_safras_data = [\n",
    "            (\"202410\", X_train_202410, y_train_202410),\n",
    "            (\"202411\", X_train_202411, y_train_202411),\n",
    "            (\"202412\", X_train_202412, y_train_202412),\n",
    "            (\"202501\", X_train_202501, y_train_202501),\n",
    "        ]\n",
    "        for safra, X_s, y_s in train_safras_data:\n",
    "            s_lr = pipeline_LR.predict_proba(X_s)[:, 1]\n",
    "            s_lgbm = pipeline_LGBM.predict_proba(X_s)[:, 1]\n",
    "            safra_labels.append(safra)\n",
    "            ks_lr_values.append(compute_ks(y_s, s_lr))\n",
    "            ks_lgbm_values.append(compute_ks(y_s, s_lgbm))\n",
    "    except NameError:\n",
    "        # Safras de treino nao disponiveis individualmente — usar score in-sample global\n",
    "        try:\n",
    "            s_train_lr = pipeline_LR.predict_proba(X_train)[:, 1]\n",
    "            s_train_lgbm = pipeline_LGBM.predict_proba(X_train)[:, 1]\n",
    "            safra_labels.append(\"Train\\n(202410-202501)\")\n",
    "            ks_lr_values.append(compute_ks(y_train, s_train_lr))\n",
    "            ks_lgbm_values.append(compute_ks(y_train, s_train_lgbm))\n",
    "        except NameError:\n",
    "            pass\n",
    "\n",
    "    # OOT safras (sempre disponiveis)\n",
    "    safra_labels.append(\"OOT1\\n(202502)\")\n",
    "    ks_lr_values.append(compute_ks(y_oot1, scores_oot1_lr))\n",
    "    ks_lgbm_values.append(compute_ks(y_oot1, scores_oot1_lgbm))\n",
    "\n",
    "    safra_labels.append(\"OOT2\\n(202503)\")\n",
    "    ks_lr_values.append(compute_ks(y_oot2, scores_oot2_lr))\n",
    "    ks_lgbm_values.append(compute_ks(y_oot2, scores_oot2_lgbm))\n",
    "\n",
    "    # Plotar evolucao de KS\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    x = range(len(safra_labels))\n",
    "\n",
    "    ax.plot(x, ks_lr_values, 'o-', label=\"Logistic Regression (L1)\",\n",
    "            color=COLORS[\"blue\"], linewidth=2, markersize=8)\n",
    "    ax.plot(x, ks_lgbm_values, 's-', label=\"LightGBM (GBDT)\",\n",
    "            color=COLORS[\"orange\"], linewidth=2, markersize=8)\n",
    "\n",
    "    # Benchmark KS = 33.1% (baseline LR OOT)\n",
    "    ax.axhline(y=0.331, color=COLORS[\"red\"], linestyle='--', alpha=0.6,\n",
    "               label=\"Benchmark KS = 33.1%\")\n",
    "\n",
    "    # Anotacoes de valor\n",
    "    for i, (v_lr, v_lgbm) in enumerate(zip(ks_lr_values, ks_lgbm_values)):\n",
    "        ax.annotate(f\"{v_lr:.3f}\", (i, v_lr), textcoords=\"offset points\",\n",
    "                    xytext=(0, 10), ha='center', fontsize=9, color=COLORS[\"blue\"])\n",
    "        ax.annotate(f\"{v_lgbm:.3f}\", (i, v_lgbm), textcoords=\"offset points\",\n",
    "                    xytext=(0, -15), ha='center', fontsize=9, color=COLORS[\"orange\"])\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(safra_labels, fontsize=10)\n",
    "    ax.set_ylabel(\"KS Statistic\", fontsize=11)\n",
    "    ax.set_title(\"Estabilidade do KS por SAFRA\", fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, max(max(ks_lr_values), max(ks_lgbm_values)) * 1.25)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/fig_ks_stability.png\", dpi=DPI, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nKS por SAFRA:\")\n",
    "    for s, klr, klgbm in zip(safra_labels, ks_lr_values, ks_lgbm_values):\n",
    "        print(f\"  {s.replace(chr(10), ' ')}: LR={klr:.4f}, LGBM={klgbm:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao gerar grafico de estabilidade KS: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Grafico 7 — Swap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_analysis(y_true_1, scores_1, y_true_2, scores_2, thresholds=[0.05, 0.10, 0.20, 0.30]):\n",
    "    \"\"\"Swap analysis: aplica regras de corte da SAFRA 1 na SAFRA 2.\n",
    "\n",
    "    Compara taxa de FPD usando:\n",
    "    - Regras proprias da SAFRA 1\n",
    "    - Regras da SAFRA 1 aplicadas na SAFRA 2 (swap)\n",
    "    - Regras proprias da SAFRA 2\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for thresh in thresholds:\n",
    "        cutoff_1 = np.percentile(scores_1, (1 - thresh) * 100)\n",
    "        cutoff_2 = np.percentile(scores_2, (1 - thresh) * 100)\n",
    "\n",
    "        # Safra 1 rules\n",
    "        approved_1 = scores_1 <= cutoff_1\n",
    "        fpd_rate_1 = y_true_1[approved_1].mean() if approved_1.sum() > 0 else 0\n",
    "\n",
    "        # Safra 2 with Safra 1 rules (swap)\n",
    "        approved_swap = scores_2 <= cutoff_1\n",
    "        fpd_rate_swap = y_true_2[approved_swap].mean() if approved_swap.sum() > 0 else 0\n",
    "\n",
    "        # Safra 2 own rules\n",
    "        approved_2 = scores_2 <= cutoff_2\n",
    "        fpd_rate_2 = y_true_2[approved_2].mean() if approved_2.sum() > 0 else 0\n",
    "\n",
    "        results.append({\n",
    "            \"threshold\": f\"{thresh * 100:.0f}%\",\n",
    "            \"cutoff_safra1\": cutoff_1,\n",
    "            \"cutoff_safra2\": cutoff_2,\n",
    "            \"approved_pct_safra1\": approved_1.mean(),\n",
    "            \"fpd_rate_safra1\": fpd_rate_1,\n",
    "            \"approved_pct_swap\": approved_swap.mean(),\n",
    "            \"fpd_rate_swap\": fpd_rate_swap,\n",
    "            \"approved_pct_safra2\": approved_2.mean(),\n",
    "            \"fpd_rate_safra2\": fpd_rate_2,\n",
    "            \"delta_fpd\": fpd_rate_swap - fpd_rate_2\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "try:\n",
    "    swap_lr = swap_analysis(y_oot1.values, scores_oot1_lr, y_oot2.values, scores_oot2_lr)\n",
    "    swap_lgbm = swap_analysis(y_oot1.values, scores_oot1_lgbm, y_oot2.values, scores_oot2_lgbm)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    x = range(len(swap_lr))\n",
    "    width = 0.25\n",
    "\n",
    "    # LR\n",
    "    axes[0].bar([i - width for i in x], swap_lr[\"fpd_rate_safra1\"], width,\n",
    "               label=\"OOT1 (202502)\", color=COLORS[\"blue\"])\n",
    "    axes[0].bar([i for i in x], swap_lr[\"fpd_rate_swap\"], width,\n",
    "               label=\"Swap (regra OOT1->OOT2)\", color=COLORS[\"orange\"])\n",
    "    axes[0].bar([i + width for i in x], swap_lr[\"fpd_rate_safra2\"], width,\n",
    "               label=\"OOT2 (202503)\", color=COLORS[\"green\"])\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(swap_lr[\"threshold\"])\n",
    "    axes[0].set_title(\"Swap Analysis — Logistic Regression\", fontsize=12)\n",
    "    axes[0].set_ylabel(\"Taxa FPD\")\n",
    "    axes[0].set_xlabel(\"Rejection Threshold\")\n",
    "    axes[0].legend(fontsize=9)\n",
    "    axes[0].grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "    # LGBM\n",
    "    axes[1].bar([i - width for i in x], swap_lgbm[\"fpd_rate_safra1\"], width,\n",
    "               label=\"OOT1 (202502)\", color=COLORS[\"blue\"])\n",
    "    axes[1].bar([i for i in x], swap_lgbm[\"fpd_rate_swap\"], width,\n",
    "               label=\"Swap (regra OOT1->OOT2)\", color=COLORS[\"orange\"])\n",
    "    axes[1].bar([i + width for i in x], swap_lgbm[\"fpd_rate_safra2\"], width,\n",
    "               label=\"OOT2 (202503)\", color=COLORS[\"green\"])\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(swap_lgbm[\"threshold\"])\n",
    "    axes[1].set_title(\"Swap Analysis — LightGBM\", fontsize=12)\n",
    "    axes[1].set_ylabel(\"Taxa FPD\")\n",
    "    axes[1].set_xlabel(\"Rejection Threshold\")\n",
    "    axes[1].legend(fontsize=9)\n",
    "    axes[1].grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "    plt.suptitle(\"Swap Analysis: OOT1 (202502) vs OOT2 (202503)\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/fig_swap_analysis.png\", dpi=DPI, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n--- Swap LR ---\")\n",
    "    print(swap_lr.to_string(index=False))\n",
    "    print(\"\\n--- Swap LGBM ---\")\n",
    "    print(swap_lgbm.to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao gerar swap analysis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Grafico 8 — Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # --- LR ---\n",
    "    prec_1, rec_1, _ = precision_recall_curve(y_oot1, scores_oot1_lr)\n",
    "    prec_2, rec_2, _ = precision_recall_curve(y_oot2, scores_oot2_lr)\n",
    "    baseline_1 = y_oot1.mean()\n",
    "    baseline_2 = y_oot2.mean()\n",
    "\n",
    "    axes[0].plot(rec_1, prec_1, label=f\"OOT1 (202502)\", color=COLORS[\"blue\"], linewidth=1.5)\n",
    "    axes[0].plot(rec_2, prec_2, label=f\"OOT2 (202503)\", color=COLORS[\"orange\"], linewidth=1.5)\n",
    "    axes[0].axhline(y=baseline_1, color=COLORS[\"blue\"], linestyle=':', alpha=0.4,\n",
    "                    label=f\"Baseline OOT1 ({baseline_1:.3f})\")\n",
    "    axes[0].axhline(y=baseline_2, color=COLORS[\"orange\"], linestyle=':', alpha=0.4,\n",
    "                    label=f\"Baseline OOT2 ({baseline_2:.3f})\")\n",
    "    axes[0].set_title(\"Precision-Recall — Logistic Regression (L1)\", fontsize=12)\n",
    "    axes[0].set_xlabel(\"Recall\")\n",
    "    axes[0].set_ylabel(\"Precision\")\n",
    "    axes[0].legend(fontsize=9)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_xlim([0, 1])\n",
    "    axes[0].set_ylim([0, 1])\n",
    "\n",
    "    # --- LGBM ---\n",
    "    prec_3, rec_3, _ = precision_recall_curve(y_oot1, scores_oot1_lgbm)\n",
    "    prec_4, rec_4, _ = precision_recall_curve(y_oot2, scores_oot2_lgbm)\n",
    "\n",
    "    axes[1].plot(rec_3, prec_3, label=f\"OOT1 (202502)\", color=COLORS[\"blue\"], linewidth=1.5)\n",
    "    axes[1].plot(rec_4, prec_4, label=f\"OOT2 (202503)\", color=COLORS[\"orange\"], linewidth=1.5)\n",
    "    axes[1].axhline(y=baseline_1, color=COLORS[\"blue\"], linestyle=':', alpha=0.4,\n",
    "                    label=f\"Baseline OOT1 ({baseline_1:.3f})\")\n",
    "    axes[1].axhline(y=baseline_2, color=COLORS[\"orange\"], linestyle=':', alpha=0.4,\n",
    "                    label=f\"Baseline OOT2 ({baseline_2:.3f})\")\n",
    "    axes[1].set_title(\"Precision-Recall — LightGBM (GBDT)\", fontsize=12)\n",
    "    axes[1].set_xlabel(\"Recall\")\n",
    "    axes[1].set_ylabel(\"Precision\")\n",
    "    axes[1].legend(fontsize=9)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_xlim([0, 1])\n",
    "    axes[1].set_ylim([0, 1])\n",
    "\n",
    "    plt.suptitle(\"Precision-Recall Curves — OOT1 vs OOT2\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/fig_precision_recall.png\", dpi=DPI, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Precision-Recall curves salvas.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao gerar Precision-Recall curves: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resumo de Metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    def compute_metrics(y_true, y_scores, model_name, safra_name):\n",
    "        \"\"\"Calcula metricas completas para um par (modelo, safra).\"\"\"\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "        ks = max(tpr - fpr)\n",
    "        auc = roc_auc_score(y_true, y_scores)\n",
    "        best_idx = np.argmax(tpr - fpr)\n",
    "        best_thresh = thresholds[best_idx]\n",
    "        y_pred = (y_scores >= best_thresh).astype(int)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "        return {\n",
    "            \"Modelo\": model_name,\n",
    "            \"SAFRA\": safra_name,\n",
    "            \"N\": len(y_true),\n",
    "            \"FPD_Rate\": y_true.mean(),\n",
    "            \"KS\": ks,\n",
    "            \"AUC\": auc,\n",
    "            \"Threshold\": best_thresh,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1\": f1,\n",
    "            \"TP\": tp,\n",
    "            \"FP\": fp,\n",
    "            \"TN\": tn,\n",
    "            \"FN\": fn\n",
    "        }\n",
    "\n",
    "    rows = [\n",
    "        compute_metrics(y_oot1, scores_oot1_lr,   \"LR (L1)\",       \"OOT1 (202502)\"),\n",
    "        compute_metrics(y_oot2, scores_oot2_lr,   \"LR (L1)\",       \"OOT2 (202503)\"),\n",
    "        compute_metrics(y_oot1, scores_oot1_lgbm, \"LightGBM\",      \"OOT1 (202502)\"),\n",
    "        compute_metrics(y_oot2, scores_oot2_lgbm, \"LightGBM\",      \"OOT2 (202503)\"),\n",
    "    ]\n",
    "\n",
    "    # Incluir treino se disponivel\n",
    "    try:\n",
    "        s_train_lr = pipeline_LR.predict_proba(X_train)[:, 1]\n",
    "        s_train_lgbm = pipeline_LGBM.predict_proba(X_train)[:, 1]\n",
    "        rows.insert(0, compute_metrics(y_train, s_train_lr,   \"LR (L1)\",  \"Train (202410-202501)\"))\n",
    "        rows.insert(1, compute_metrics(y_train, s_train_lgbm, \"LightGBM\", \"Train (202410-202501)\"))\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    df_metrics = pd.DataFrame(rows)\n",
    "\n",
    "    # Formatar para exibicao\n",
    "    display_cols = [\"Modelo\", \"SAFRA\", \"N\", \"FPD_Rate\", \"KS\", \"AUC\",\n",
    "                    \"Threshold\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "    df_display = df_metrics[display_cols].copy()\n",
    "    for col in [\"FPD_Rate\", \"KS\", \"AUC\", \"Threshold\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"]:\n",
    "        df_display[col] = df_display[col].map(\"{:.4f}\".format)\n",
    "    df_display[\"N\"] = df_display[\"N\"].map(\"{:,}\".format)\n",
    "\n",
    "    print(\"=\" * 120)\n",
    "    print(\"RESUMO DE METRICAS — Modelo Baseline FPD\")\n",
    "    print(\"=\" * 120)\n",
    "    print(df_display.to_string(index=False))\n",
    "    print(\"=\" * 120)\n",
    "\n",
    "    # Salvar como CSV\n",
    "    df_metrics.to_csv(f\"{OUTPUT_DIR}/metrics_summary.csv\", index=False)\n",
    "    print(f\"\\nResumo salvo em: {OUTPUT_DIR}/metrics_summary.csv\")\n",
    "\n",
    "    # Swap analysis resumo\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SWAP ANALYSIS RESUMO\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nLogistic Regression — Delta FPD (swap - safra2):\")\n",
    "    for _, row in swap_lr.iterrows():\n",
    "        delta_pct = row['delta_fpd'] * 100\n",
    "        status = \"ESTAVEL\" if abs(delta_pct) < 1.0 else (\"PIORA\" if delta_pct > 0 else \"MELHORA\")\n",
    "        print(f\"  Threshold {row['threshold']}: delta={delta_pct:+.2f}pp [{status}]\")\n",
    "\n",
    "    print(f\"\\nLightGBM — Delta FPD (swap - safra2):\")\n",
    "    for _, row in swap_lgbm.iterrows():\n",
    "        delta_pct = row['delta_fpd'] * 100\n",
    "        status = \"ESTAVEL\" if abs(delta_pct) < 1.0 else (\"PIORA\" if delta_pct > 0 else \"MELHORA\")\n",
    "        print(f\"  Threshold {row['threshold']}: delta={delta_pct:+.2f}pp [{status}]\")\n",
    "\n",
    "    # Graficos exportados\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"GRAFICOS EXPORTADOS\")\n",
    "    print(\"=\" * 80)\n",
    "    exported = [\n",
    "        \"fig_ks_curves.png\",\n",
    "        \"fig_score_distribution.png\",\n",
    "        \"fig_roc_curves.png\",\n",
    "        \"fig_confusion_matrix.png\",\n",
    "        \"fig_feature_importance_lgbm.png\",\n",
    "        \"fig_ks_stability.png\",\n",
    "        \"fig_swap_analysis.png\",\n",
    "        \"fig_precision_recall.png\",\n",
    "        \"metrics_summary.csv\"\n",
    "    ]\n",
    "    for f in exported:\n",
    "        print(f\"  {OUTPUT_DIR}/{f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao gerar resumo de metricas: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  },
  "microsoft": {
   "language": "python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}