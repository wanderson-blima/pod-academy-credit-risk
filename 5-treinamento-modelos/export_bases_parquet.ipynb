{
 "cells": [
  {
   "cell_type": "code",
   "id": "eb_000",
   "metadata": {},
   "source": [
    "%%configure\n",
    "\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.driver.maxResultSize\": \"8g\",\n",
    "        \"spark.driver.memory\": \"54g\",\n",
    "        \"spark.driver.cores\": 8,\n",
    "        \"spark.executor.instances\": 0,\n",
    "        \"spark.sql.execution.arrow.pyspark.enabled\": \"true\",\n",
    "        \"spark.sql.execution.arrow.pyspark.selfDestruct.enabled\": \"true\"\n",
    "    }\n",
    "}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "eb_001",
   "metadata": {},
   "source": [
    "# Export Bases — Treino, OOS e OOT para Lakehouse (Parquet)\n",
    "\n",
    "Notebook de entrega que exporta as bases utilizadas no treinamento do modelo v6 para o Lakehouse em formato **Parquet**, garantindo rastreabilidade e reprodutibilidade.\n",
    "\n",
    "### Bases exportadas\n",
    "| Base | Descricao | SAFRAs | Uso |\n",
    "|------|-----------|--------|-----|\n",
    "| **treino** | Amostra estratificada 25% (train + val) | 202410-202501 | Treino e validacao do modelo |\n",
    "| **oos** | Out-of-Sample (75% restante) | 202410-202501 | Validacao fora da amostra |\n",
    "| **oot** | Out-of-Time | 202502-202503 | Teste temporal (producao simulada) |\n",
    "\n",
    "### Destinos\n",
    "- **Parquet**: `/lakehouse/default/Files/projeto-final/datasets/v6/`\n",
    "- **Delta Tables**: `Gold.feature_store.{treino_v6, oos_v6, oot_v6}`\n",
    "\n",
    "### Pipeline\n",
    "1. Leitura do Gold Feature Store (5-layer memory optimization)\n",
    "2. Limpeza de dados (mesmas 7 funcoes do modelo v6)\n",
    "3. Split temporal + amostragem estratificada 25%\n",
    "4. Export Parquet + Delta (particionado por SAFRA)\n",
    "5. Validacao de integridade (contagens, schema, missing)"
   ]
  },
  {
   "cell_type": "code",
   "id": "eb_010",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 1. IMPORTS E CONFIGURACAO\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType, IntegerType, DoubleType, LongType\n",
    "\n",
    "import sys; sys.path.insert(0, '/lakehouse/default/Files/projeto-final')\n",
    "from config.pipeline_config import (\n",
    "    PATH_FEATURE_STORE, SAFRAS, LEAKAGE_BLACKLIST, GOLD_BASE\n",
    ")\n",
    "\n",
    "# Destinos de export\n",
    "PARQUET_DIR = \"/lakehouse/default/Files/projeto-final/datasets/v6\"\n",
    "DELTA_BASE = f\"{GOLD_BASE}/Tables/feature_store\"\n",
    "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
    "\n",
    "print('Imports OK')\n",
    "print(f'Parquet dir: {PARQUET_DIR}')\n",
    "print(f'Delta base:  {DELTA_BASE}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "eb_md_data",
   "metadata": {},
   "source": [
    "## 2. Leitura do Gold Feature Store\n",
    "\n",
    "Mesma estrategia de 5 camadas do modelo v6 para contornar o limite de `spark.driver.maxResultSize`."
   ]
  },
  {
   "cell_type": "code",
   "id": "eb_020",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 2. LEITURA OTIMIZADA DO GOLD FEATURE STORE (Spark -> Pandas)\n",
    "# =============================================================================\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.selfDestruct.enabled\", \"true\")\n",
    "\n",
    "print(f'Lendo feature store de: {PATH_FEATURE_STORE}\\n')\n",
    "\n",
    "df_spark = spark.read.format(\"delta\").load(PATH_FEATURE_STORE)\n",
    "n_original = df_spark.count()\n",
    "print(f'Original: {n_original:,} rows x {len(df_spark.columns)} cols')\n",
    "\n",
    "# Drop colunas audit + leakage no Spark\n",
    "cols_audit = ['_execution_id', '_data_inclusao', '_data_alteracao_silver', 'DT_PROCESSAMENTO']\n",
    "cols_drop = [c for c in cols_audit + LEAKAGE_BLACKLIST if c in df_spark.columns]\n",
    "if cols_drop:\n",
    "    df_spark = df_spark.drop(*cols_drop)\n",
    "    print(f'Drop {len(cols_drop)} colunas (audit+leakage): {cols_drop}')\n",
    "\n",
    "# Filtrar FLAG_INSTALACAO == 1 no Spark\n",
    "n_reprovados = 0\n",
    "if 'FLAG_INSTALACAO' in df_spark.columns:\n",
    "    n_reprovados = df_spark.filter(F.col('FLAG_INSTALACAO') == 0).count()\n",
    "    df_spark = df_spark.filter(F.col('FLAG_INSTALACAO') == 1).drop('FLAG_INSTALACAO')\n",
    "    n_pos = n_original - n_reprovados\n",
    "    print(f'FLAG_INSTALACAO: {n_original:,} -> {n_pos:,} ({n_reprovados:,} reprovados removidos)')\n",
    "else:\n",
    "    n_pos = n_original\n",
    "\n",
    "# Cast tipos via .select() (plano flat)\n",
    "cast_exprs = []\n",
    "n_double, n_long = 0, 0\n",
    "for field in df_spark.schema.fields:\n",
    "    if isinstance(field.dataType, DoubleType):\n",
    "        cast_exprs.append(F.col(field.name).cast(FloatType()).alias(field.name))\n",
    "        n_double += 1\n",
    "    elif isinstance(field.dataType, LongType):\n",
    "        cast_exprs.append(F.col(field.name).cast(IntegerType()).alias(field.name))\n",
    "        n_long += 1\n",
    "    else:\n",
    "        cast_exprs.append(F.col(field.name))\n",
    "df_spark = df_spark.select(*cast_exprs)\n",
    "print(f'Cast tipos: {n_double} Double->Float, {n_long} Long->Int')\n",
    "\n",
    "# Conversao chunked por SAFRA\n",
    "safras_disponiveis = sorted([row.SAFRA for row in df_spark.select('SAFRA').distinct().collect()])\n",
    "print(f'\\nSAFRAs: {safras_disponiveis} | Colunas: {len(df_spark.columns)}')\n",
    "print('Convertendo por SAFRA...')\n",
    "\n",
    "chunks = []\n",
    "for safra in safras_disponiveis:\n",
    "    chunk = df_spark.filter(F.col('SAFRA') == safra).toPandas()\n",
    "    mem_mb = chunk.memory_usage(deep=True).sum() / 1e6\n",
    "    print(f'  SAFRA {safra}: {len(chunk):,} rows | {mem_mb:.0f} MB')\n",
    "    chunks.append(chunk)\n",
    "    gc.collect()\n",
    "\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks\n",
    "gc.collect()\n",
    "\n",
    "print(f'\\nDataset carregado: {df.shape}')\n",
    "print(f'Memory: {df.memory_usage(deep=True).sum() / 1e9:.2f} GB')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "eb_md_clean",
   "metadata": {},
   "source": [
    "## 3. Limpeza de Dados\n",
    "\n",
    "Mesmas 7 funcoes de limpeza do modelo v6 para garantir que as bases exportadas correspondam **exatamente** ao que foi usado no treinamento."
   ]
  },
  {
   "cell_type": "code",
   "id": "eb_030",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 3. FUNCOES DE LIMPEZA (identicas ao modelo v6)\n",
    "# =============================================================================\n",
    "\n",
    "def clean_empty_keys(df):\n",
    "    return df.dropna(subset=['NUM_CPF', 'SAFRA'])\n",
    "\n",
    "def convert_cep3_uf_regiao(df):\n",
    "    cep_map = {\n",
    "        '01':('SP','SUDESTE'),'02':('SP','SUDESTE'),'03':('SP','SUDESTE'),\n",
    "        '04':('SP','SUDESTE'),'05':('SP','SUDESTE'),'06':('SP','SUDESTE'),\n",
    "        '07':('SP','SUDESTE'),'08':('SP','SUDESTE'),'09':('SP','SUDESTE'),\n",
    "        '20':('RJ','SUDESTE'),'21':('RJ','SUDESTE'),'22':('RJ','SUDESTE'),\n",
    "        '23':('RJ','SUDESTE'),'24':('RJ','SUDESTE'),'29':('ES','SUDESTE'),\n",
    "        '30':('MG','SUDESTE'),'31':('MG','SUDESTE'),'32':('MG','SUDESTE'),\n",
    "        '33':('MG','SUDESTE'),'34':('MG','SUDESTE'),'35':('MG','SUDESTE'),\n",
    "        '36':('MG','SUDESTE'),'37':('MG','SUDESTE'),'38':('MG','SUDESTE'),\n",
    "        '39':('MG','SUDESTE'),\n",
    "        '40':('BA','NORDESTE'),'41':('BA','NORDESTE'),'42':('BA','NORDESTE'),\n",
    "        '43':('BA','NORDESTE'),'44':('BA','NORDESTE'),'45':('BA','NORDESTE'),\n",
    "        '46':('BA','NORDESTE'),'47':('BA','NORDESTE'),'48':('BA','NORDESTE'),\n",
    "        '49':('SE','NORDESTE'),\n",
    "        '50':('PE','NORDESTE'),'51':('PE','NORDESTE'),'52':('PE','NORDESTE'),\n",
    "        '53':('PE','NORDESTE'),'54':('PE','NORDESTE'),'55':('PE','NORDESTE'),\n",
    "        '56':('AL','NORDESTE'),'57':('AL','NORDESTE'),\n",
    "        '58':('PB','NORDESTE'),'59':('RN','NORDESTE'),\n",
    "        '60':('CE','NORDESTE'),'61':('CE','NORDESTE'),'62':('CE','NORDESTE'),\n",
    "        '63':('PI','NORDESTE'),'64':('PI','NORDESTE'),'65':('MA','NORDESTE'),\n",
    "        '66':('PA','NORTE'),'67':('PA','NORTE'),'68':('AC','NORTE'),\n",
    "        '69':('AM','NORTE'),'77':('TO','NORTE'),\n",
    "        '70':('DF','CENTRO-OESTE'),'71':('DF','CENTRO-OESTE'),\n",
    "        '72':('GO','CENTRO-OESTE'),'73':('GO','CENTRO-OESTE'),\n",
    "        '74':('GO','CENTRO-OESTE'),'75':('GO','CENTRO-OESTE'),\n",
    "        '76':('GO','CENTRO-OESTE'),\n",
    "        '78':('MT','CENTRO-OESTE'),'79':('MS','CENTRO-OESTE'),\n",
    "        '80':('PR','SUL'),'81':('PR','SUL'),'82':('PR','SUL'),\n",
    "        '83':('PR','SUL'),'84':('PR','SUL'),'85':('PR','SUL'),\n",
    "        '86':('PR','SUL'),'87':('PR','SUL'),\n",
    "        '88':('SC','SUL'),'89':('SC','SUL'),\n",
    "        '90':('RS','SUL'),'91':('RS','SUL'),'92':('RS','SUL'),\n",
    "        '93':('RS','SUL'),'94':('RS','SUL'),'95':('RS','SUL'),\n",
    "        '96':('RS','SUL'),'97':('RS','SUL'),'98':('RS','SUL'),'99':('RS','SUL'),\n",
    "    }\n",
    "    if 'CEP_3_digitos' not in df.columns:\n",
    "        return df\n",
    "    cep2 = df['CEP_3_digitos'].astype(str).str[:2]\n",
    "    mapped = cep2.map(cep_map)\n",
    "    df['UF'] = mapped.apply(lambda x: x[0] if isinstance(x, tuple) else 'OUTROS')\n",
    "    df['REGIAO'] = mapped.apply(lambda x: x[1] if isinstance(x, tuple) else 'OUTROS')\n",
    "    return df.drop(columns=['CEP_3_digitos'])\n",
    "\n",
    "def adjust_and_drop_date_cols(df):\n",
    "    if 'var_12' in df.columns:\n",
    "        df['var_12'] = pd.to_datetime(df['var_12'], format='%d/%m/%Y', errors='coerce')\n",
    "    df['DATA_REF_SAFRA'] = pd.to_datetime(df['SAFRA'].astype(str), format='%Y%m')\n",
    "    if 'var_12' in df.columns:\n",
    "        df['DIAS_VAR_12'] = (df['DATA_REF_SAFRA'] - df['var_12']).dt.days\n",
    "    if 'PAG_DT_PRIMEIRA_FATURA' in df.columns:\n",
    "        df['PAG_DT_PRIMEIRA_FATURA'] = pd.to_datetime(df['PAG_DT_PRIMEIRA_FATURA'], errors='coerce')\n",
    "        df['PAG_DIAS_DESDE_PRIMEIRA_FATURA'] = (df['DATA_REF_SAFRA'] - df['PAG_DT_PRIMEIRA_FATURA']).dt.days\n",
    "    date_cols = df.select_dtypes(include=['datetime64', 'datetimetz']).columns.tolist()\n",
    "    date_cols.append('DATA_REF_SAFRA')\n",
    "    return df.drop(columns=[c for c in date_cols if c in df.columns])\n",
    "\n",
    "def remove_high_missing(df, threshold=0.75):\n",
    "    null_pct = df.isnull().mean()\n",
    "    cols_to_drop = null_pct[null_pct >= threshold].index.tolist()\n",
    "    print(f'  High missing (>= {threshold:.0%}): {len(cols_to_drop)} colunas removidas')\n",
    "    return df.drop(columns=cols_to_drop)\n",
    "\n",
    "def remove_low_cardinality(df):\n",
    "    low_card = [c for c in df.columns if df[c].nunique() <= 1]\n",
    "    print(f'  Low cardinality (== 1): {len(low_card)} colunas removidas')\n",
    "    return df.drop(columns=low_card)\n",
    "\n",
    "def remove_high_correlation(df, threshold=0.8, safras_train=None):\n",
    "    if safras_train is not None:\n",
    "        df_corr_base = df[df['SAFRA'].isin(safras_train)]\n",
    "    else:\n",
    "        df_corr_base = df\n",
    "    df_sample = df_corr_base.groupby(['SAFRA', 'FPD'], group_keys=False).apply(\n",
    "        lambda x: x.sample(frac=0.25, random_state=42))\n",
    "    num_cols = df_sample.select_dtypes(include=['int32','int64','float32','float64']).columns\n",
    "    num_cols = [c for c in num_cols if c != 'FPD']\n",
    "    corr_matrix = df_sample[num_cols].corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = []\n",
    "    while True:\n",
    "        max_corr = upper.max().max()\n",
    "        if max_corr < threshold:\n",
    "            break\n",
    "        col_to_drop = upper.max().sort_values(ascending=False).index[0]\n",
    "        to_drop.append(col_to_drop)\n",
    "        upper = upper.drop(index=col_to_drop, columns=col_to_drop)\n",
    "    print(f'  High correlation (> {threshold}): {len(to_drop)} colunas removidas')\n",
    "    return df.drop(columns=to_drop)\n",
    "\n",
    "def remove_misused_columns(df):\n",
    "    misused = ['PROD', 'flag_mig2', 'FAT_VLR_FPD', 'FAT_FLAG_MIG2_AQUISICAO']\n",
    "    existing = [c for c in misused if c in df.columns]\n",
    "    if existing:\n",
    "        print(f'  Misused columns removed: {existing}')\n",
    "    return df.drop(columns=existing, errors='ignore')\n",
    "\n",
    "print('Funcoes de limpeza carregadas')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "eb_031",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 3.2 APLICAR LIMPEZAS\n",
    "# =============================================================================\n",
    "safras_train_val = SAFRAS[:4]\n",
    "\n",
    "print('Aplicando limpezas...')\n",
    "print(f'Shape original: {df.shape}')\n",
    "\n",
    "df = clean_empty_keys(df)\n",
    "df = convert_cep3_uf_regiao(df)\n",
    "df = adjust_and_drop_date_cols(df)\n",
    "df = remove_high_missing(df)\n",
    "df = remove_low_cardinality(df)\n",
    "df = remove_high_correlation(df, threshold=0.8, safras_train=safras_train_val)\n",
    "df = remove_misused_columns(df)\n",
    "\n",
    "print(f'Shape apos limpezas: {df.shape}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "eb_md_split",
   "metadata": {},
   "source": [
    "## 4. Split Temporal + Amostragem Estratificada\n",
    "\n",
    "Mesma logica do modelo v6:\n",
    "- **Treino (sample)**: 25% estratificado por (SAFRA, FPD) das SAFRAs 202410-202501\n",
    "- **OOS**: 75% restante das SAFRAs 202410-202501\n",
    "- **OOT**: SAFRAs 202502-202503 (100%)"
   ]
  },
  {
   "cell_type": "code",
   "id": "eb_040",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 4. SPLIT TEMPORAL + AMOSTRAGEM ESTRATIFICADA\n",
    "# =============================================================================\n",
    "safras_ord = sorted(df['SAFRA'].unique())\n",
    "safras_train_oos = safras_ord[:4]  # 202410-202501\n",
    "safras_oot = safras_ord[4:]        # 202502-202503\n",
    "\n",
    "df_4_safras = df[df['SAFRA'].isin(safras_train_oos)]\n",
    "df_oot_full = df[df['SAFRA'].isin(safras_oot)]\n",
    "\n",
    "# Amostragem estratificada 25%\n",
    "df_sample = df_4_safras.groupby(['SAFRA', 'FPD'], group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=0.25, random_state=42))\n",
    "df_oos = df_4_safras.drop(df_sample.index)\n",
    "\n",
    "df_treino = df_sample.reset_index(drop=True).drop_duplicates()\n",
    "df_oos = df_oos.reset_index(drop=True).drop_duplicates()\n",
    "df_oot = df_oot_full.reset_index(drop=True).drop_duplicates()\n",
    "\n",
    "del df, df_4_safras, df_oot_full, df_sample\n",
    "gc.collect()\n",
    "\n",
    "print(f'Treino (sample 25%): {df_treino.shape}')\n",
    "print(f'OOS (75% restante):  {df_oos.shape}')\n",
    "print(f'OOT (temporal):      {df_oot.shape}')\n",
    "\n",
    "# Volumetria detalhada\n",
    "for name, data in [('Treino', df_treino), ('OOS', df_oos), ('OOT', df_oot)]:\n",
    "    print(f'\\n--- {name} ---')\n",
    "    total = len(data)\n",
    "    for safra in sorted(data['SAFRA'].unique()):\n",
    "        s = data[data['SAFRA'] == safra]\n",
    "        n_bad = (s['FPD'] == 1).sum()\n",
    "        n_good = (s['FPD'] == 0).sum()\n",
    "        print(f'  SAFRA {safra}: {len(s):>8,} rows | FPD=0: {n_good:>7,} | FPD=1: {n_bad:>6,} ({n_bad/len(s):.2%})')\n",
    "    n_bad_total = (data['FPD'] == 1).sum()\n",
    "    print(f'  TOTAL:       {total:>8,} rows | FPD=1: {n_bad_total:>6,} ({n_bad_total/total:.2%})')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "eb_md_export",
   "metadata": {},
   "source": [
    "## 5. Export para Lakehouse (Parquet + Delta)\n",
    "\n",
    "Converte cada base de Pandas para Spark e grava em:\n",
    "1. **Parquet**: `/lakehouse/default/Files/projeto-final/datasets/v6/` — formato de entrega\n",
    "2. **Delta Table**: `Gold.feature_store.{treino_v6, oos_v6, oot_v6}` — acesso estruturado\n",
    "\n",
    "Todas as tabelas sao particionadas por `SAFRA` para otimizar leituras parciais."
   ]
  },
  {
   "cell_type": "code",
   "id": "eb_050",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 5. EXPORT PARA LAKEHOUSE — PARQUET + DELTA\n",
    "# =============================================================================\n",
    "bases = {\n",
    "    'treino_v6': df_treino,\n",
    "    'oos_v6': df_oos,\n",
    "    'oot_v6': df_oot,\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, df_base in bases.items():\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Exportando: {name} ({df_base.shape[0]:,} rows x {df_base.shape[1]} cols)')\n",
    "    print(f'{\"=\"*60}')\n",
    "\n",
    "    # Converter Pandas -> Spark\n",
    "    df_spark = spark.createDataFrame(df_base)\n",
    "\n",
    "    # --- 5.1 Parquet ---\n",
    "    parquet_path = f'{PARQUET_DIR}/{name}'\n",
    "    df_spark.write.mode('overwrite').partitionBy('SAFRA').parquet(parquet_path)\n",
    "    print(f'  Parquet salvo: {parquet_path}')\n",
    "\n",
    "    # --- 5.2 Delta Table ---\n",
    "    delta_path = f'{DELTA_BASE}/{name}'\n",
    "    df_spark.write.mode('overwrite').partitionBy('SAFRA').format('delta').save(delta_path)\n",
    "    print(f'  Delta salvo:   {delta_path}')\n",
    "\n",
    "    # Registrar contagens\n",
    "    results.append({\n",
    "        'base': name,\n",
    "        'rows': df_base.shape[0],\n",
    "        'cols': df_base.shape[1],\n",
    "        'safras': sorted(df_base['SAFRA'].unique().tolist()),\n",
    "        'fpd_rate': (df_base['FPD'] == 1).mean() * 100,\n",
    "        'parquet_path': parquet_path,\n",
    "        'delta_path': delta_path,\n",
    "    })\n",
    "\n",
    "    print(f'  OK!')\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print('Export concluido!')\n",
    "print(f'{\"=\"*60}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "eb_md_valid",
   "metadata": {},
   "source": [
    "## 6. Validacao de Integridade\n",
    "\n",
    "Recarrega cada base exportada e verifica:\n",
    "- Contagem de registros (deve bater com original)\n",
    "- Schema (colunas e tipos)\n",
    "- FPD rate (deve bater com original)"
   ]
  },
  {
   "cell_type": "code",
   "id": "eb_060",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 6. VALIDACAO DE INTEGRIDADE\n",
    "# =============================================================================\n",
    "print('Validando bases exportadas...\\n')\n",
    "\n",
    "all_ok = True\n",
    "\n",
    "for res in results:\n",
    "    name = res['base']\n",
    "    expected_rows = res['rows']\n",
    "    expected_cols = res['cols']\n",
    "\n",
    "    # Ler Parquet de volta\n",
    "    df_check_parquet = spark.read.parquet(res['parquet_path'])\n",
    "    n_parquet = df_check_parquet.count()\n",
    "    cols_parquet = len(df_check_parquet.columns)\n",
    "\n",
    "    # Ler Delta de volta\n",
    "    df_check_delta = spark.read.format('delta').load(res['delta_path'])\n",
    "    n_delta = df_check_delta.count()\n",
    "\n",
    "    # Checks\n",
    "    ok_rows_pq = n_parquet == expected_rows\n",
    "    ok_rows_dt = n_delta == expected_rows\n",
    "    ok_cols = cols_parquet == expected_cols\n",
    "\n",
    "    status = 'PASS' if (ok_rows_pq and ok_rows_dt and ok_cols) else 'FAIL'\n",
    "    if status == 'FAIL':\n",
    "        all_ok = False\n",
    "\n",
    "    print(f'--- {name} [{status}] ---')\n",
    "    print(f'  Esperado:    {expected_rows:>10,} rows x {expected_cols} cols')\n",
    "    print(f'  Parquet:     {n_parquet:>10,} rows x {cols_parquet} cols {\"OK\" if ok_rows_pq else \"FALHA\"}')\n",
    "    print(f'  Delta:       {n_delta:>10,} rows {\"OK\" if ok_rows_dt else \"FALHA\"}')\n",
    "    print(f'  FPD rate:    {res[\"fpd_rate\"]:.2f}%')\n",
    "    print(f'  SAFRAs:      {res[\"safras\"]}')\n",
    "    print()\n",
    "\n",
    "if all_ok:\n",
    "    print('=== TODAS AS VALIDACOES PASSARAM ===')\n",
    "else:\n",
    "    print('!!! ALGUMA VALIDACAO FALHOU — VERIFICAR ACIMA !!!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "eb_070",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# 7. RESUMO FINAL\n",
    "# =============================================================================\n",
    "print('=' * 70)\n",
    "print('  EXPORT BASES v6 — RESUMO')\n",
    "print('=' * 70)\n",
    "\n",
    "print(f'\\n  {\"Base\":<15} {\"Rows\":>10} {\"Cols\":>6} {\"FPD%\":>7} {\"SAFRAs\"}')\n",
    "print(f'  {\"-\"*55}')\n",
    "for res in results:\n",
    "    safras_str = ', '.join(str(s) for s in res['safras'])\n",
    "    print(f'  {res[\"base\"]:<15} {res[\"rows\"]:>10,} {res[\"cols\"]:>6} {res[\"fpd_rate\"]:>6.2f}% {safras_str}')\n",
    "\n",
    "total_rows = sum(r['rows'] for r in results)\n",
    "print(f'  {\"-\"*55}')\n",
    "print(f'  {\"TOTAL\":<15} {total_rows:>10,}')\n",
    "\n",
    "print(f'\\n  Parquet: {PARQUET_DIR}/')\n",
    "print(f'  Delta:   Gold.feature_store.treino_v6 / oos_v6 / oot_v6')\n",
    "print(f'\\n  Particao: SAFRA')\n",
    "print(f'  Limpezas: 7 funcoes (identicas modelo v6)')\n",
    "print(f'  Split: 25% estratificado (SAFRA x FPD) | random_state=42')\n",
    "\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print('  Bases prontas para entrega do projeto')\n",
    "print(f'{\"=\"*70}')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}