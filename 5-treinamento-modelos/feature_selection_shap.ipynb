{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%configure\n",
    "\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.driver.maxResultSize\": \"8g\",\n",
    "        \"spark.driver.memory\": \"54g\",\n",
    "        \"spark.driver.cores\": 8,\n",
    "        \"spark.executor.instances\": 0,\n",
    "        \"spark.sql.execution.arrow.pyspark.enabled\": \"true\",\n",
    "        \"spark.sql.execution.arrow.pyspark.selfDestruct.enabled\": \"true\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection com SHAP — Credit Risk FPD\n",
    "\n",
    "**Objetivo**: Identificar as features mais relevantes para predizer FPD usando SHAP\n",
    "(SHapley Additive exPlanations), gerando rankings por book e visualizacoes para\n",
    "apresentacao.\n",
    "\n",
    "### Metodologia\n",
    "\n",
    "1. **Treinar LGBM baseline** com todas as features disponveis\n",
    "2. **SHAP TreeExplainer** — calcula contribuicao real de cada feature (captura interacoes)\n",
    "3. **Rankings por book**: REC_ (Recarga), PAG_ (Pagamento), FAT_ (Faturamento), Base\n",
    "4. **Selecao**: Features que acumulam 90% da importancia SHAP total\n",
    "5. **Comparacao** com metodo atual (IV + L1 + correlacao)\n",
    "\n",
    "### Por que SHAP?\n",
    "\n",
    "- **IV** e univariado — nao captura interacoes entre features\n",
    "- **LGBM feature_importances_** e enviesado por cardinalidade e numero de splits\n",
    "- **SHAP** mede a contribuicao marginal real (teoria dos jogos), captura direcao e interacoes\n",
    "- **TreeExplainer** e O(TLD) — rapido, nao re-prediz (~5 min para nosso dataset)\n",
    "\n",
    "### Impacto\n",
    "\n",
    "- Tempo extra de execucao: ~5-8 min (negligivel vs pipeline total)\n",
    "- Entregaveis: 6 graficos de apresentacao + lista de features selecionadas"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 1. IMPORTS E CONFIGURACAO\n",
    "# =============================================================================\n",
    "import gc\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import warnings\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from category_encoders import CountEncoder\n",
    "from scipy.stats import ks_2samp\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType, IntegerType, DoubleType, LongType\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Config centralizado\n",
    "import sys; sys.path.insert(0, '/lakehouse/default/Files/projeto-final')\n",
    "from config.pipeline_config import (\n",
    "    PATH_FEATURE_STORE, EXPERIMENT_NAME, SAFRAS,\n",
    "    LEAKAGE_BLACKLIST, TARGET_COLUMNS\n",
    ")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%H:%M:%S')\n",
    "logger = logging.getLogger('feature_selection_shap')\n",
    "\n",
    "# Diretorio de artefatos\n",
    "ARTIFACTS_DIR = '/tmp/shap_feature_selection'\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 2. MLFLOW SETUP\n",
    "# =============================================================================\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "print(f'MLflow experiment: {EXPERIMENT_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 3. LEITURA OTIMIZADA DO GOLD FEATURE STORE\n",
    "# =============================================================================\n",
    "logger.info('Carregando feature store...')\n",
    "\n",
    "# ---- 3.1 Arrow habilitado via %%configure ----\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', 'true')\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.selfDestruct.enabled', 'true')\n",
    "\n",
    "df_spark = spark.read.format('delta').load(PATH_FEATURE_STORE)\n",
    "n_original = df_spark.count()\n",
    "print(f'Original: {n_original:,} rows x {len(df_spark.columns)} cols')\n",
    "\n",
    "# ---- 3.2 Drop audit + leakage ----\n",
    "cols_audit = ['_execution_id', '_data_inclusao', '_data_alteracao_silver', 'DT_PROCESSAMENTO']\n",
    "cols_drop = [c for c in cols_audit + LEAKAGE_BLACKLIST if c in df_spark.columns]\n",
    "if cols_drop:\n",
    "    df_spark = df_spark.drop(*cols_drop)\n",
    "    print(f'Drop {len(cols_drop)} colunas (audit+leakage): {cols_drop}')\n",
    "\n",
    "# ---- 3.3 Filtrar FLAG_INSTALACAO == 1 ----\n",
    "if 'FLAG_INSTALACAO' in df_spark.columns:\n",
    "    n_reprovados = df_spark.filter(F.col('FLAG_INSTALACAO') == 0).count()\n",
    "    df_spark = df_spark.filter(F.col('FLAG_INSTALACAO') == 1).drop('FLAG_INSTALACAO')\n",
    "    n_pos = n_original - n_reprovados\n",
    "    print(f'FLAG_INSTALACAO: {n_original:,} -> {n_pos:,} ({n_reprovados:,} reprovados removidos)')\n",
    "\n",
    "# ---- 3.4 Cast tipos via .select() ----\n",
    "cast_exprs = []\n",
    "for field in df_spark.schema.fields:\n",
    "    if isinstance(field.dataType, DoubleType):\n",
    "        cast_exprs.append(F.col(field.name).cast(FloatType()).alias(field.name))\n",
    "    elif isinstance(field.dataType, LongType):\n",
    "        cast_exprs.append(F.col(field.name).cast(IntegerType()).alias(field.name))\n",
    "    else:\n",
    "        cast_exprs.append(F.col(field.name))\n",
    "df_spark = df_spark.select(*cast_exprs)\n",
    "\n",
    "# ---- 3.5 Conversao chunked por SAFRA ----\n",
    "safras_disponiveis = sorted([row.SAFRA for row in df_spark.select('SAFRA').distinct().collect()])\n",
    "print(f'SAFRAs: {safras_disponiveis} | Colunas: {len(df_spark.columns)}')\n",
    "\n",
    "chunks = []\n",
    "for safra in safras_disponiveis:\n",
    "    chunk = df_spark.filter(F.col('SAFRA') == safra).toPandas()\n",
    "    print(f'  SAFRA {safra}: {len(chunk):,} rows')\n",
    "    chunks.append(chunk)\n",
    "    gc.collect()\n",
    "\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks\n",
    "gc.collect()\n",
    "\n",
    "print(f'\\nDataset: {df.shape}')\n",
    "print(f'Memory: {df.memory_usage(deep=True).sum() / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 4. LIMPEZA BASICA (mesmo criterio do v3)\n",
    "# =============================================================================\n",
    "print(f'Shape pre-limpeza: {df.shape}')\n",
    "\n",
    "# 4.1 Remove chaves vazias\n",
    "df = df.dropna(subset=['NUM_CPF', 'SAFRA'])\n",
    "\n",
    "# 4.2 Remove missing > 75%\n",
    "null_pct = df.isnull().mean()\n",
    "cols_high_missing = null_pct[null_pct >= 0.75].index.tolist()\n",
    "df = df.drop(columns=cols_high_missing)\n",
    "print(f'High missing (>= 75%): {len(cols_high_missing)} colunas removidas')\n",
    "\n",
    "# 4.3 Remove low cardinality (1 unico valor)\n",
    "low_card = [c for c in df.columns if df[c].nunique() <= 1]\n",
    "df = df.drop(columns=low_card)\n",
    "print(f'Low cardinality (== 1): {len(low_card)} colunas removidas')\n",
    "\n",
    "# 4.4 Remove leakage conhecidos\n",
    "misused = ['PROD', 'flag_mig2', 'FAT_VLR_FPD', 'FAT_FLAG_MIG2_AQUISICAO']\n",
    "existing_misused = [c for c in misused if c in df.columns]\n",
    "if existing_misused:\n",
    "    df = df.drop(columns=existing_misused)\n",
    "    print(f'Leakage removidos: {existing_misused}')\n",
    "\n",
    "print(f'Shape pos-limpeza: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 5. SPLIT TEMPORAL + PREPARACAO X/Y\n",
    "# =============================================================================\n",
    "NON_FEATURE_COLS = [\n",
    "    'NUM_CPF', 'SAFRA', 'FPD', 'TARGET_SCORE_01', 'TARGET_SCORE_02',\n",
    "    'DATADENASCIMENTO',\n",
    "]\n",
    "\n",
    "safras_train = [202410, 202411, 202412, 202501]\n",
    "safras_oot = [202502, 202503]\n",
    "\n",
    "df_train_full = df[df['SAFRA'].isin(safras_train)].copy()\n",
    "df_oot_full = df[df['SAFRA'].isin(safras_oot)].copy()\n",
    "\n",
    "# Drop NaN no target\n",
    "df_train_full = df_train_full.dropna(subset=['FPD'])\n",
    "df_oot_full = df_oot_full.dropna(subset=['FPD'])\n",
    "\n",
    "# Amostra estratificada 25% para treino (acelera SHAP)\n",
    "df_sample = df_train_full.groupby(['SAFRA', 'FPD'], group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=0.25, random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Separar X/y\n",
    "feature_cols = [c for c in df_sample.columns if c not in NON_FEATURE_COLS]\n",
    "X_train = df_sample[feature_cols].copy()\n",
    "y_train = df_sample['FPD'].astype(int).copy()\n",
    "\n",
    "X_oot = df_oot_full[feature_cols].copy()\n",
    "y_oot = df_oot_full['FPD'].astype(int).copy()\n",
    "\n",
    "# Liberar memoria\n",
    "del df, df_train_full, df_oot_full\n",
    "gc.collect()\n",
    "\n",
    "print(f'X_train (25% sample): {X_train.shape}')\n",
    "print(f'y_train FPD rate: {y_train.mean():.4f}')\n",
    "print(f'X_oot: {X_oot.shape}')\n",
    "print(f'y_oot FPD rate: {y_oot.mean():.4f}')\n",
    "print(f'Features: {len(feature_cols)}')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 6. CLASSIFICAR FEATURES POR BOOK (PREFIXO)\n",
    "# =============================================================================\n",
    "def classify_features_by_book(columns):\n",
    "    \"\"\"Classifica features por fonte/book baseado no prefixo.\"\"\"\n",
    "    groups = {'Base (Cadastro+Telco)': [], 'Recarga (REC_)': [],\n",
    "              'Pagamento (PAG_)': [], 'Faturamento (FAT_)': []}\n",
    "    for col in columns:\n",
    "        if col.startswith('REC_'):\n",
    "            groups['Recarga (REC_)'].append(col)\n",
    "        elif col.startswith('PAG_'):\n",
    "            groups['Pagamento (PAG_)'].append(col)\n",
    "        elif col.startswith('FAT_'):\n",
    "            groups['Faturamento (FAT_)'].append(col)\n",
    "        else:\n",
    "            groups['Base (Cadastro+Telco)'].append(col)\n",
    "    return groups\n",
    "\n",
    "feature_groups = classify_features_by_book(feature_cols)\n",
    "\n",
    "# Mapa de cores por book\n",
    "BOOK_COLORS = {\n",
    "    'Base (Cadastro+Telco)': '#607D8B',\n",
    "    'Recarga (REC_)': '#2196F3',\n",
    "    'Pagamento (PAG_)': '#FF9800',\n",
    "    'Faturamento (FAT_)': '#9C27B0',\n",
    "}\n",
    "\n",
    "def get_feature_color(feat_name):\n",
    "    \"\"\"Retorna cor baseada no prefixo da feature.\"\"\"\n",
    "    if feat_name.startswith('REC_') or '__REC_' in feat_name:\n",
    "        return BOOK_COLORS['Recarga (REC_)']\n",
    "    elif feat_name.startswith('PAG_') or '__PAG_' in feat_name:\n",
    "        return BOOK_COLORS['Pagamento (PAG_)']\n",
    "    elif feat_name.startswith('FAT_') or '__FAT_' in feat_name:\n",
    "        return BOOK_COLORS['Faturamento (FAT_)']\n",
    "    return BOOK_COLORS['Base (Cadastro+Telco)']\n",
    "\n",
    "print('Feature Groups:')\n",
    "for group, cols in feature_groups.items():\n",
    "    print(f'  {group}: {len(cols)} features')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 7. TREINAR LGBM BASELINE (TODAS AS FEATURES)\n",
    "# =============================================================================\n",
    "logger.info('Treinando LGBM baseline para SHAP...')\n",
    "\n",
    "num_features = [n for n in X_train.select_dtypes(include=['int32','int64','float32','float64']).columns]\n",
    "cat_features = [c for c in X_train.select_dtypes(include=['object','category']).columns]\n",
    "\n",
    "num_pipe = Pipeline([('imputer', SimpleImputer(strategy='median'))])\n",
    "cat_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', CountEncoder(normalize=True, handle_unknown=0, handle_missing=0)),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipe, num_features),\n",
    "    ('cat', cat_pipe, cat_features),\n",
    "], remainder='drop')\n",
    "\n",
    "lgbm_model = LGBMClassifier(\n",
    "    objective='binary', boosting_type='gbdt',\n",
    "    learning_rate=0.05, n_estimators=300, max_depth=7,\n",
    "    colsample_bytree=0.8, subsample=0.8,\n",
    "    random_state=42, n_jobs=-1, verbosity=-1,\n",
    ")\n",
    "\n",
    "pipeline_lgbm = Pipeline([('prep', preprocessor), ('model', lgbm_model)])\n",
    "pipeline_lgbm.fit(X_train, y_train)\n",
    "\n",
    "# Metricas baseline\n",
    "def ks_stat(y_true, y_score):\n",
    "    pos = y_score[y_true == 1]\n",
    "    neg = y_score[y_true == 0]\n",
    "    return ks_2samp(pos, neg).statistic\n",
    "\n",
    "scores_train = pipeline_lgbm.predict_proba(X_train)[:, 1]\n",
    "scores_oot = pipeline_lgbm.predict_proba(X_oot)[:, 1]\n",
    "\n",
    "auc_train = roc_auc_score(y_train, scores_train)\n",
    "ks_train = ks_stat(y_train, scores_train)\n",
    "auc_oot = roc_auc_score(y_oot, scores_oot)\n",
    "ks_oot = ks_stat(y_oot, scores_oot)\n",
    "\n",
    "print(f'LGBM Baseline (todas features):')\n",
    "print(f'  Train — AUC: {auc_train:.4f}, KS: {ks_train:.4f}')\n",
    "print(f'  OOT   — AUC: {auc_oot:.4f}, KS: {ks_oot:.4f}')\n",
    "print(f'  Features: {len(num_features)} num + {len(cat_features)} cat = {len(num_features)+len(cat_features)}')\n",
    "\n",
    "logger.info('LGBM baseline treinado')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 8. SHAP TREE EXPLAINER\n",
    "# =============================================================================\n",
    "logger.info('Calculando SHAP values (TreeExplainer)...')\n",
    "\n",
    "# Transformar X para formato pos-preprocessamento\n",
    "X_train_transformed = pipeline_lgbm.named_steps['prep'].transform(X_train)\n",
    "\n",
    "# Nomes das features pos-transformacao\n",
    "try:\n",
    "    transformed_names = pipeline_lgbm.named_steps['prep'].get_feature_names_out()\n",
    "except Exception:\n",
    "    transformed_names = num_features + cat_features\n",
    "\n",
    "# SHAP TreeExplainer (rapido para tree-based models)\n",
    "explainer = shap.TreeExplainer(pipeline_lgbm.named_steps['model'])\n",
    "shap_values = explainer.shap_values(X_train_transformed)\n",
    "\n",
    "# Para classificacao binaria, shap_values pode ser lista [class_0, class_1]\n",
    "if isinstance(shap_values, list):\n",
    "    shap_vals = shap_values[1]  # classe positiva (FPD=1)\n",
    "else:\n",
    "    shap_vals = shap_values\n",
    "\n",
    "print(f'SHAP values shape: {shap_vals.shape}')\n",
    "print(f'Features transformadas: {len(transformed_names)}')\n",
    "\n",
    "logger.info('SHAP values calculados')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 9. RANKING GLOBAL — mean(|SHAP|)\n",
    "# =============================================================================\n",
    "mean_abs_shap = np.abs(shap_vals).mean(axis=0)\n",
    "\n",
    "df_shap_ranking = pd.DataFrame({\n",
    "    'feature_transformed': list(transformed_names),\n",
    "    'mean_abs_shap': mean_abs_shap,\n",
    "}).sort_values('mean_abs_shap', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Mapear de volta para nome original (remover prefixo num__/cat__)\n",
    "df_shap_ranking['feature'] = df_shap_ranking['feature_transformed'].apply(\n",
    "    lambda x: x.split('__', 1)[-1] if '__' in x else x\n",
    ")\n",
    "\n",
    "# Classificar por book\n",
    "def get_book_label(feat):\n",
    "    if feat.startswith('REC_'):\n",
    "        return 'Recarga (REC_)'\n",
    "    elif feat.startswith('PAG_'):\n",
    "        return 'Pagamento (PAG_)'\n",
    "    elif feat.startswith('FAT_'):\n",
    "        return 'Faturamento (FAT_)'\n",
    "    return 'Base (Cadastro+Telco)'\n",
    "\n",
    "df_shap_ranking['book'] = df_shap_ranking['feature'].apply(get_book_label)\n",
    "\n",
    "# Importancia cumulativa\n",
    "total_shap = df_shap_ranking['mean_abs_shap'].sum()\n",
    "df_shap_ranking['pct_importance'] = df_shap_ranking['mean_abs_shap'] / total_shap\n",
    "df_shap_ranking['cumulative_pct'] = df_shap_ranking['pct_importance'].cumsum()\n",
    "df_shap_ranking['rank'] = range(1, len(df_shap_ranking) + 1)\n",
    "\n",
    "print('Top 30 Features (SHAP):')\n",
    "print(df_shap_ranking[['rank', 'feature', 'book', 'mean_abs_shap', 'pct_importance', 'cumulative_pct']].head(30).to_string(index=False))\n",
    "\n",
    "# Contribuicao por book\n",
    "book_contribution = df_shap_ranking.groupby('book')['mean_abs_shap'].sum()\n",
    "book_contribution_pct = (book_contribution / total_shap * 100).sort_values(ascending=False)\n",
    "\n",
    "print(f'\\nContribuicao por Book:')\n",
    "for book, pct in book_contribution_pct.items():\n",
    "    n_feats = len(df_shap_ranking[df_shap_ranking['book'] == book])\n",
    "    print(f'  {book}: {pct:.1f}% ({n_feats} features)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizacoes para Apresentacao"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 10. GRAFICO 1 — SHAP Summary Plot (Beeswarm) Global\n",
    "# =============================================================================\n",
    "fig, ax = plt.subplots(figsize=(12, 14))\n",
    "shap.summary_plot(\n",
    "    shap_vals, X_train_transformed,\n",
    "    feature_names=list(transformed_names),\n",
    "    max_display=40, show=False,\n",
    ")\n",
    "plt.title('SHAP Summary Plot — Top 40 Features (FPD)', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{ARTIFACTS_DIR}/shap_summary_beeswarm.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Grafico 1 salvo: shap_summary_beeswarm.png')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 11. GRAFICO 2 — SHAP Bar Plot Top 30 (colorido por Book)\n",
    "# =============================================================================\n",
    "top30 = df_shap_ranking.head(30)\n",
    "colors = [get_feature_color(f) for f in top30['feature']]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "bars = ax.barh(\n",
    "    range(len(top30) - 1, -1, -1),\n",
    "    top30['mean_abs_shap'].values,\n",
    "    color=colors, alpha=0.85, edgecolor='white', linewidth=0.5,\n",
    ")\n",
    "ax.set_yticks(range(len(top30) - 1, -1, -1))\n",
    "ax.set_yticklabels(top30['feature'].values, fontsize=9)\n",
    "ax.set_xlabel('mean(|SHAP value|)', fontsize=11)\n",
    "ax.set_title('Top 30 Features por Importancia SHAP — Colorido por Book', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Anotar percentual cumulativo\n",
    "for i, (_, row) in enumerate(top30.iterrows()):\n",
    "    ax.text(row['mean_abs_shap'] + max(top30['mean_abs_shap']) * 0.01,\n",
    "            len(top30) - 1 - i,\n",
    "            f\"{row['pct_importance']:.1%}\",\n",
    "            va='center', fontsize=8, color='#333')\n",
    "\n",
    "# Legenda\n",
    "legend_elements = [Patch(facecolor=c, label=l) for l, c in BOOK_COLORS.items()]\n",
    "ax.legend(handles=legend_elements, loc='lower right', fontsize=10)\n",
    "ax.grid(True, alpha=0.2, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{ARTIFACTS_DIR}/shap_top30_by_book.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Grafico 2 salvo: shap_top30_by_book.png')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 12. GRAFICO 3 — Top 15 Features por Book (4 subplots)\n",
    "# =============================================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "fig.suptitle('Top 15 Features por Book — Importancia SHAP', fontsize=16, fontweight='bold')\n",
    "\n",
    "book_names = ['Base (Cadastro+Telco)', 'Recarga (REC_)', 'Pagamento (PAG_)', 'Faturamento (FAT_)']\n",
    "\n",
    "for idx, book_name in enumerate(book_names):\n",
    "    ax = axes[idx // 2][idx % 2]\n",
    "    book_data = df_shap_ranking[df_shap_ranking['book'] == book_name].head(15)\n",
    "\n",
    "    if book_data.empty:\n",
    "        ax.set_title(f'{book_name} — sem features')\n",
    "        continue\n",
    "\n",
    "    color = BOOK_COLORS[book_name]\n",
    "    ax.barh(\n",
    "        range(len(book_data) - 1, -1, -1),\n",
    "        book_data['mean_abs_shap'].values,\n",
    "        color=color, alpha=0.85, edgecolor='white', linewidth=0.5,\n",
    "    )\n",
    "    ax.set_yticks(range(len(book_data) - 1, -1, -1))\n",
    "    ax.set_yticklabels(book_data['feature'].values, fontsize=8)\n",
    "    ax.set_xlabel('mean(|SHAP value|)', fontsize=9)\n",
    "\n",
    "    total_book = df_shap_ranking[df_shap_ranking['book'] == book_name]['mean_abs_shap'].sum()\n",
    "    pct_global = total_book / total_shap * 100\n",
    "    n_total = len(df_shap_ranking[df_shap_ranking['book'] == book_name])\n",
    "    ax.set_title(f'{book_name}\\n{n_total} features | {pct_global:.1f}% da importancia global',\n",
    "                fontsize=11, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.2, axis='x')\n",
    "\n",
    "    # Anotar valores\n",
    "    for i, (_, row) in enumerate(book_data.iterrows()):\n",
    "        ax.text(row['mean_abs_shap'] + max(book_data['mean_abs_shap']) * 0.02,\n",
    "                len(book_data) - 1 - i,\n",
    "                f\"{row['mean_abs_shap']:.4f}\",\n",
    "                va='center', fontsize=7, color='#555')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(f'{ARTIFACTS_DIR}/shap_top15_per_book.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Grafico 3 salvo: shap_top15_per_book.png')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 13. GRAFICO 4 — Contribuicao Agregada por Book (Stacked Bar)\n",
    "# =============================================================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Contribuicao Agregada por Book', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 13a. Pie chart\n",
    "ax1 = axes[0]\n",
    "book_pcts = []\n",
    "book_labels = []\n",
    "book_colors_list = []\n",
    "for book_name in book_names:\n",
    "    total_book = df_shap_ranking[df_shap_ranking['book'] == book_name]['mean_abs_shap'].sum()\n",
    "    pct = total_book / total_shap * 100\n",
    "    book_pcts.append(pct)\n",
    "    n = len(df_shap_ranking[df_shap_ranking['book'] == book_name])\n",
    "    book_labels.append(f'{book_name}\\n({n} vars, {pct:.1f}%)')\n",
    "    book_colors_list.append(BOOK_COLORS[book_name])\n",
    "\n",
    "wedges, texts, autotexts = ax1.pie(\n",
    "    book_pcts, labels=None, colors=book_colors_list,\n",
    "    autopct='%1.1f%%', startangle=90, pctdistance=0.7,\n",
    "    textprops={'fontsize': 11, 'fontweight': 'bold'},\n",
    ")\n",
    "ax1.legend(book_labels, loc='center left', bbox_to_anchor=(-0.3, 0.5), fontsize=9)\n",
    "ax1.set_title('% Importancia SHAP por Book', fontsize=12)\n",
    "\n",
    "# 13b. Bar chart com top N features por book\n",
    "ax2 = axes[1]\n",
    "bottom = np.zeros(1)\n",
    "for book_name in book_names:\n",
    "    total_book = df_shap_ranking[df_shap_ranking['book'] == book_name]['mean_abs_shap'].sum()\n",
    "    ax2.bar(0, total_book, bottom=bottom, color=BOOK_COLORS[book_name],\n",
    "           label=book_name, edgecolor='white', linewidth=0.5, width=0.5)\n",
    "    # Anotar no meio da barra\n",
    "    mid = bottom[0] + total_book / 2\n",
    "    pct = total_book / total_shap * 100\n",
    "    if pct > 5:\n",
    "        ax2.text(0, mid, f'{pct:.1f}%', ha='center', va='center',\n",
    "                fontsize=11, fontweight='bold', color='white')\n",
    "    bottom += total_book\n",
    "\n",
    "ax2.set_ylabel('Sum mean(|SHAP|)', fontsize=11)\n",
    "ax2.set_title('Importancia SHAP Empilhada por Book', fontsize=12)\n",
    "ax2.set_xticks([0])\n",
    "ax2.set_xticklabels(['Todas Features'])\n",
    "ax2.legend(loc='upper right', fontsize=9)\n",
    "ax2.grid(True, alpha=0.2, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{ARTIFACTS_DIR}/shap_book_contribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Grafico 4 salvo: shap_book_contribution.png')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 14. GRAFICO 5 — SHAP Dependence Plots (Top 3 features)\n",
    "# =============================================================================\n",
    "top3_features = df_shap_ranking.head(3)['feature_transformed'].tolist()\n",
    "top3_names = df_shap_ranking.head(3)['feature'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle('SHAP Dependence Plots — Top 3 Features', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i, (feat_trans, feat_name) in enumerate(zip(top3_features, top3_names)):\n",
    "    ax = axes[i]\n",
    "    feat_idx = list(transformed_names).index(feat_trans)\n",
    "    shap.dependence_plot(\n",
    "        feat_idx, shap_vals, X_train_transformed,\n",
    "        feature_names=list(transformed_names),\n",
    "        ax=ax, show=False,\n",
    "    )\n",
    "    color = get_feature_color(feat_name)\n",
    "    ax.set_title(f'{feat_name}', fontsize=11, fontweight='bold', color=color)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "plt.savefig(f'{ARTIFACTS_DIR}/shap_dependence_top3.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Grafico 5 salvo: shap_dependence_top3.png')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 15. GRAFICO 6 — Curva Cumulativa de Importancia (Pareto)\n",
    "# =============================================================================\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "x_range = range(1, len(df_shap_ranking) + 1)\n",
    "colors_cum = [get_feature_color(f) for f in df_shap_ranking['feature']]\n",
    "\n",
    "ax.bar(x_range, df_shap_ranking['pct_importance'].values, color=colors_cum, alpha=0.7, width=1.0)\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(x_range, df_shap_ranking['cumulative_pct'].values, color='red', linewidth=2, label='Cumulativo')\n",
    "ax2.axhline(y=0.90, color='red', linestyle='--', alpha=0.5, label='90% threshold')\n",
    "\n",
    "# Encontrar ponto de corte 90%\n",
    "n_90 = (df_shap_ranking['cumulative_pct'] <= 0.90).sum()\n",
    "ax2.axvline(x=n_90, color='green', linestyle='--', alpha=0.7)\n",
    "ax2.annotate(f'{n_90} features\\n= 90% importancia',\n",
    "            xy=(n_90, 0.90), xytext=(n_90 + 20, 0.80),\n",
    "            arrowprops=dict(arrowstyle='->', color='green'),\n",
    "            fontsize=11, fontweight='bold', color='green')\n",
    "\n",
    "ax.set_xlabel('Feature (rank por SHAP importance)', fontsize=11)\n",
    "ax.set_ylabel('Importancia Individual (%)', fontsize=11)\n",
    "ax2.set_ylabel('Importancia Cumulativa (%)', fontsize=11)\n",
    "ax.set_title(f'Curva de Pareto — {n_90} features capturam 90% da importancia SHAP',\n",
    "            fontsize=14, fontweight='bold')\n",
    "\n",
    "# Legenda\n",
    "legend_elements = [Patch(facecolor=c, label=l) for l, c in BOOK_COLORS.items()]\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements.append(Line2D([0], [0], color='red', linewidth=2, label='Cumulativo'))\n",
    "ax.legend(handles=legend_elements, loc='center right', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{ARTIFACTS_DIR}/shap_pareto_cumulative.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Grafico 6 salvo: shap_pareto_cumulative.png')\n",
    "print(f'\\n>>> {n_90} features capturam 90% da importancia SHAP total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecao e Export"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 16. SELECAO FINAL — TOP FEATURES (90% CUMULATIVO)\n",
    "# =============================================================================\n",
    "CUMULATIVE_THRESHOLD = 0.90\n",
    "\n",
    "selected_mask = df_shap_ranking['cumulative_pct'] <= CUMULATIVE_THRESHOLD\n",
    "# Incluir a feature que cruza o threshold (para garantir >= 90%)\n",
    "if not selected_mask.all():\n",
    "    first_over = selected_mask[~selected_mask].index[0]\n",
    "    selected_mask.iloc[:first_over + 1] = True\n",
    "\n",
    "df_selected = df_shap_ranking[selected_mask].copy()\n",
    "selected_features_shap = df_selected['feature'].unique().tolist()\n",
    "\n",
    "print(f'Features selecionadas (SHAP >= {CUMULATIVE_THRESHOLD:.0%}): {len(selected_features_shap)}')\n",
    "print(f'Importancia cumulativa capturada: {df_selected[\"pct_importance\"].sum():.1%}')\n",
    "\n",
    "# Breakdown por book\n",
    "print(f'\\nBreakdown por book:')\n",
    "for book_name in book_names:\n",
    "    book_sel = df_selected[df_selected['book'] == book_name]\n",
    "    book_all = df_shap_ranking[df_shap_ranking['book'] == book_name]\n",
    "    print(f'  {book_name}: {len(book_sel)} / {len(book_all)} selecionadas')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 17. VALIDACAO — TREINAR LGBM COM FEATURES SELECIONADAS E COMPARAR\n",
    "# =============================================================================\n",
    "logger.info('Validando com features selecionadas...')\n",
    "\n",
    "# Features selecionadas que existem no X_train\n",
    "selected_in_X = [f for f in selected_features_shap if f in X_train.columns]\n",
    "\n",
    "X_train_sel = X_train[selected_in_X]\n",
    "X_oot_sel = X_oot[selected_in_X]\n",
    "\n",
    "num_sel = [n for n in X_train_sel.select_dtypes(include=['int32','int64','float32','float64']).columns]\n",
    "cat_sel = [c for c in X_train_sel.select_dtypes(include=['object','category']).columns]\n",
    "\n",
    "num_pipe_sel = Pipeline([('imputer', SimpleImputer(strategy='median'))])\n",
    "cat_pipe_sel = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', CountEncoder(normalize=True, handle_unknown=0, handle_missing=0)),\n",
    "])\n",
    "prep_sel = ColumnTransformer([\n",
    "    ('num', num_pipe_sel, num_sel),\n",
    "    ('cat', cat_pipe_sel, cat_sel),\n",
    "], remainder='drop')\n",
    "\n",
    "lgbm_sel = LGBMClassifier(\n",
    "    objective='binary', boosting_type='gbdt',\n",
    "    learning_rate=0.05, n_estimators=300, max_depth=7,\n",
    "    colsample_bytree=0.8, subsample=0.8,\n",
    "    random_state=42, n_jobs=-1, verbosity=-1,\n",
    ")\n",
    "pipe_sel = Pipeline([('prep', prep_sel), ('model', lgbm_sel)])\n",
    "pipe_sel.fit(X_train_sel, y_train)\n",
    "\n",
    "scores_train_sel = pipe_sel.predict_proba(X_train_sel)[:, 1]\n",
    "scores_oot_sel = pipe_sel.predict_proba(X_oot_sel)[:, 1]\n",
    "\n",
    "auc_train_sel = roc_auc_score(y_train, scores_train_sel)\n",
    "ks_train_sel = ks_stat(y_train, scores_train_sel)\n",
    "auc_oot_sel = roc_auc_score(y_oot, scores_oot_sel)\n",
    "ks_oot_sel = ks_stat(y_oot, scores_oot_sel)\n",
    "\n",
    "# Tabela comparativa\n",
    "n_all = len(num_features) + len(cat_features)\n",
    "n_sel = len(selected_in_X)\n",
    "\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f' COMPARACAO: TODAS FEATURES vs SHAP SELECIONADAS')\n",
    "print(f'{\"=\"*70}')\n",
    "print(f'{\"\":20s} {\"Todas (\" + str(n_all) + \")\":>18s} {\"SHAP (\" + str(n_sel) + \")\":>18s} {\"Delta\":>10s}')\n",
    "print(f'{\"-\"*70}')\n",
    "print(f'{\"AUC Train\":20s} {auc_train:18.4f} {auc_train_sel:18.4f} {auc_train_sel - auc_train:+10.4f}')\n",
    "print(f'{\"KS Train\":20s} {ks_train:18.4f} {ks_train_sel:18.4f} {ks_train_sel - ks_train:+10.4f}')\n",
    "print(f'{\"AUC OOT\":20s} {auc_oot:18.4f} {auc_oot_sel:18.4f} {auc_oot_sel - auc_oot:+10.4f}')\n",
    "print(f'{\"KS OOT\":20s} {ks_oot:18.4f} {ks_oot_sel:18.4f} {ks_oot_sel - ks_oot:+10.4f}')\n",
    "print(f'{\"Gini OOT\":20s} {(2*auc_oot-1):18.4f} {(2*auc_oot_sel-1):18.4f} {(2*auc_oot_sel-1)-(2*auc_oot-1):+10.4f}')\n",
    "print(f'{\"=\"*70}')\n",
    "print(f'Reducao de features: {n_all} -> {n_sel} ({(1-n_sel/n_all)*100:.0f}% menos)')\n",
    "\n",
    "logger.info('Validacao concluida')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 18. EXPORT — PICKLE + CSV + MLFLOW\n",
    "# =============================================================================\n",
    "\n",
    "# 18.1 Salvar lista de features como pickle\n",
    "pkl_path = f'{ARTIFACTS_DIR}/selected_features_shap.pkl'\n",
    "with open(pkl_path, 'wb') as f:\n",
    "    pickle.dump(selected_features_shap, f)\n",
    "print(f'Features pickle salvo: {pkl_path}')\n",
    "\n",
    "# 18.2 Salvar ranking completo como CSV\n",
    "csv_path = f'{ARTIFACTS_DIR}/shap_feature_ranking.csv'\n",
    "df_shap_ranking.to_csv(csv_path, index=False)\n",
    "print(f'Ranking CSV salvo: {csv_path}')\n",
    "\n",
    "# 18.3 Salvar features selecionadas como CSV\n",
    "csv_sel_path = f'{ARTIFACTS_DIR}/selected_features_list.csv'\n",
    "df_selected[['rank', 'feature', 'book', 'mean_abs_shap', 'pct_importance', 'cumulative_pct']].to_csv(csv_sel_path, index=False)\n",
    "print(f'Selected CSV salvo: {csv_sel_path}')\n",
    "\n",
    "# 18.4 Log no MLflow\n",
    "with mlflow.start_run(run_name='SHAP_Feature_Selection') as run:\n",
    "    mlflow.set_tag('task', 'feature_selection')\n",
    "    mlflow.set_tag('method', 'SHAP_TreeExplainer')\n",
    "    mlflow.log_param('n_features_total', n_all)\n",
    "    mlflow.log_param('n_features_selected', n_sel)\n",
    "    mlflow.log_param('cumulative_threshold', CUMULATIVE_THRESHOLD)\n",
    "    mlflow.log_param('reduction_pct', f'{(1-n_sel/n_all)*100:.0f}%')\n",
    "\n",
    "    # Metricas baseline vs selecionado\n",
    "    mlflow.log_metric('baseline_auc_oot', auc_oot)\n",
    "    mlflow.log_metric('baseline_ks_oot', ks_oot)\n",
    "    mlflow.log_metric('selected_auc_oot', auc_oot_sel)\n",
    "    mlflow.log_metric('selected_ks_oot', ks_oot_sel)\n",
    "    mlflow.log_metric('delta_auc_oot', auc_oot_sel - auc_oot)\n",
    "    mlflow.log_metric('delta_ks_oot', ks_oot_sel - ks_oot)\n",
    "\n",
    "    # Contribuicao por book\n",
    "    for book_name in book_names:\n",
    "        total_book = df_shap_ranking[df_shap_ranking['book'] == book_name]['mean_abs_shap'].sum()\n",
    "        pct = total_book / total_shap\n",
    "        safe_name = book_name.replace(' ', '_').replace('(', '').replace(')', '').replace('+', '')\n",
    "        mlflow.log_metric(f'book_pct_{safe_name}', round(pct, 4))\n",
    "\n",
    "    # Artefatos\n",
    "    for fname in os.listdir(ARTIFACTS_DIR):\n",
    "        fpath = f'{ARTIFACTS_DIR}/{fname}'\n",
    "        if fname.endswith('.png'):\n",
    "            mlflow.log_artifact(fpath, 'shap_plots')\n",
    "        else:\n",
    "            mlflow.log_artifact(fpath, 'feature_selection')\n",
    "\n",
    "    mlflow.sklearn.log_model(pipe_sel, 'model_lgbm_shap_selected')\n",
    "\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "print(f'\\nMLflow Run ID: {run_id}')\n",
    "print(f'Artefatos logados: {len(os.listdir(ARTIFACTS_DIR))} arquivos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusoes\n",
    "\n",
    "### Resultados\n",
    "\n",
    "- SHAP TreeExplainer identificou as features com maior contribuicao real para predizer FPD\n",
    "- A selecao por cumulativo 90% reduz significativamente o numero de features\n",
    "  sem perda relevante de performance (Delta KS e AUC devem ser proximos de zero)\n",
    "\n",
    "### Proximos Passos\n",
    "\n",
    "1. Importar `selected_features_shap.pkl` no `modelo_baseline_v3.ipynb`\n",
    "2. Re-treinar modelo final com features SHAP-selecionadas\n",
    "3. Comparar KS/AUC vs selecao IV+L1+corr atual\n",
    "4. Usar graficos por book na apresentacao (slide de Feature Importance)\n",
    "\n",
    "### Graficos Gerados\n",
    "\n",
    "| Arquivo | Descricao |\n",
    "|---------|----------|\n",
    "| `shap_summary_beeswarm.png` | Beeswarm plot global — direcao e magnitude |\n",
    "| `shap_top30_by_book.png` | Top 30 features coloridas por book |\n",
    "| `shap_top15_per_book.png` | Top 15 de cada book (4 subplots) |\n",
    "| `shap_book_contribution.png` | Pie + stacked bar de contribuicao por book |\n",
    "| `shap_dependence_top3.png` | Dependence plots das 3 features mais importantes |\n",
    "| `shap_pareto_cumulative.png` | Curva de Pareto com ponto de corte 90% |"
   ]
  }
 ]
}