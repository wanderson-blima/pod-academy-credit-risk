{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Modelo Baseline de Risco – Telecom\n",
    "\n",
    "Este notebook implementa um **modelo baseline de risco de inadimplência**, considerando o contexto de uma grande empresa de telecomunicacao.\n",
    "\n",
    "Abaixo, estao descritas as principais atividades implementadas para o desenvolvimento do modelo de risco de inadimplência.\n",
    "\n",
    "### Principais etapas aplicados na modelagem:\n",
    "- Amostragem de **25% por safra e FPD**, de forma estratificada \n",
    "- Separação em **Treino (2024-10 a 2024-12) e Validacao (2025-01) (Samples) X OOS (202-10 a 2025-01) X OOT (2025-02 e 2025-03)**\n",
    "- Modelos testados inicialmente: Logistic Regression e LightGBM\n",
    "- Ajuste de hiperparâmetros apenas no conjunto de validacao\n",
    "- Treinos finais dos modelos considerando bases de treino e validacao (Safras 2024-10 a 2025-01)\n",
    "- Metricas utilizadas na analise: AUC, KS, Precisao, Recall, F1-Score\n",
    "- Avaliação por safra (AUC e KS) -> Safras OOT1 (2025-02) e OOT2 (2025-03) e OOT Consolidada\n",
    "- Avaliação por safra tambem nas bases de treino e OOS, paea fins de validacao do processo\n",
    "- Analise da evolucao do KS com a inclusao incremental das variaveis de cada book/fonte\n",
    "- Uso do benchmark de 33.1% de KS como comparativo para performance do modelos nas safras OOT\n",
    "- Análise **Swap-in / Swap-out** e quantificacao de ganhos esperados com o modelo\n"
   ],
   "metadata": {},
   "id": "943fda1c"
  },
  {
   "cell_type": "code",
   "source": [
    "# Instalacao de pacote para uso de Target/CountEncoder\n",
    "%pip install scikit-learn==1.3.2\n",
    "%pip install category-encoders==2.6.3"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "2299d0e0-552d-4919-bf3e-cc122a92b991"
  },
  {
   "cell_type": "code",
   "source": [
    "# Importacao de bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "import warnings\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, \n",
    "    StratifiedKFold\n",
    ")\n",
    "from pyspark.sql import Window, Row\n",
    "from pyspark.sql.types import DateType, TimestampType\n",
    "from pyspark.sql import functions as F\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, FunctionTransformer\n",
    "from category_encoders import CountEncoder, TargetEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Config centralizado do pipeline\n",
    "import sys; sys.path.insert(0, \"/lakehouse/default/Files/projeto-final\")\n",
    "from config.pipeline_config import EXPERIMENT_NAME, SAFRAS"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "f56a5690"
  },
  {
   "cell_type": "code",
   "source": [
    "# Removendo warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "8da32ab7-ff5a-4004-ae63-2bb9bb002755"
  },
  {
   "cell_type": "code",
   "source": [
    "# Configuracao do MLflow experiment tracking\n",
    "# EXPERIMENT_NAME importado de config.pipeline_config\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# Habilitando autolog para sklearn e lightgbm\n",
    "mlflow.autolog(\n",
    "    log_models=True,\n",
    "    log_input_examples=False,\n",
    "    log_model_signatures=True,\n",
    "    silent=True\n",
    ")\n",
    "\n",
    "print(f\"MLflow experiment configurado: {EXPERIMENT_NAME}\")\n",
    "print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "30b7dc4e-0777-428c-8711-c813c31fcb59"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processo completo - (Spark)"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "1f2c3b73-4164-40ae-bc2d-8ca8f3e4a5f6"
  },
  {
   "cell_type": "code",
   "source": [
    "# Remove registros em que chaves (CPF e SAFRA) estao vazios\n",
    "def clean_empty_keys_data(df):\n",
    "    df = (\n",
    "        df\n",
    "        .filter(F.col(\"SAFRA\").isNotNull())\n",
    "        .filter(F.col(\"NUM_CPF\").isNotNull())   \n",
    "    )\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "fd2bc615-0f64-45ca-bdfc-f8dccaad75a6"
  },
  {
   "cell_type": "code",
   "source": [
    "# Remove colunas com alto percentual de valores faltantes\n",
    "def remove_high_missing_values(df, percent_missing=0.75):\n",
    "    total_rows = df.count()\n",
    "\n",
    "    null_ratio = (\n",
    "        df\n",
    "        .select([\n",
    "            (F.sum(F.col(c).isNull().cast(\"int\")) / total_rows).alias(c)\n",
    "            for c in df.columns\n",
    "        ])\n",
    "        .collect()[0]\n",
    "        .asDict()\n",
    "    )\n",
    "\n",
    "    cols_drop_nulls = [c for c, v in null_ratio.items() if v >= percent_missing]\n",
    "    df = df.drop(*cols_drop_nulls)\n",
    "    return df, null_ratio"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "da9afdff-26d6-4723-8d9e-87ab69e6d2ed"
  },
  {
   "cell_type": "code",
   "source": [
    "# Remove colunas com baixa cardinalidade\n",
    "def remove_low_cardinality_values(df):\n",
    "    low_card_cols = [\n",
    "        c for c in df.columns\n",
    "        if df.select(c).distinct().count() == 1\n",
    "    ]\n",
    "\n",
    "    df = df.drop(*low_card_cols)\n",
    "    return df,low_card_cols"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "0d59ee28-8793-42c9-94d5-b92a047b2d6e"
  },
  {
   "cell_type": "code",
   "source": [
    "# Funcao que faz a amostragem estratificada (SAFRA, FPD) de um df_spark de acordo com porcao definida em \"percent\"\n",
    "def split_stratified_data(df, percent=0.25):\n",
    "    window = Window.partitionBy(\"SAFRA\", \"FPD\").orderBy(F.rand(seed=42))\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"rn\", F.row_number().over(window)\n",
    "    )\n",
    "\n",
    "    counts = (\n",
    "        df\n",
    "        .groupBy(\"SAFRA\", \"FPD\")\n",
    "        .count()\n",
    "        .withColumn(\"cutoff\", (F.col(\"count\") * percent).cast(\"int\"))\n",
    "    )\n",
    "\n",
    "    df = df.join(counts, [\"SAFRA\", \"FPD\"], \"left\")\n",
    "\n",
    "    # Separando registros entre base amostrada e out-of-sample (teste)\n",
    "    df_sample = df.filter(F.col(\"rn\") <= F.col(\"cutoff\"))\n",
    "    df_oos = df.filter(F.col(\"rn\") > F.col(\"cutoff\"))\n",
    "\n",
    "    # Removendo colunas auxiliares geradas\n",
    "    df_sample = df_sample.drop(\"rn\", \"count\", \"cutoff\")\n",
    "    df_oos = df_oos.drop(\"rn\", \"count\", \"cutoff\")\n",
    "\n",
    "    return df_sample, df_oos"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "c49af35e-9190-40f3-ab6a-44b76bffbc49"
  },
  {
   "cell_type": "code",
   "source": [
    "# Funcao que ordena dados das safras\n",
    "def sort_periods(df):\n",
    "    safras_ord = (\n",
    "        df\n",
    "        .select(\"SAFRA\")\n",
    "        .distinct()\n",
    "        .orderBy(\"SAFRA\")\n",
    "        .rdd.flatMap(lambda x: x)\n",
    "        .collect()\n",
    "    )\n",
    "    return safras_ord"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "12051cf8-cc36-4ec7-9450-68809942a3aa"
  },
  {
   "cell_type": "code",
   "source": [
    "# Remove colunas com alta correlacao (acima de valor definido em thresh)\n",
    "def remove_high_correlation_data(df, thresh=0.8):\n",
    "\n",
    "    # Selecionando dados das 4 primeiras safras apenas (Dados das safras de treino + validacao, sem OOT)\n",
    "    safras_ord = sort_periods(df)[:4]\n",
    "    df_corr_base = df.filter(F.col(\"SAFRA\").isin(safras_ord))\n",
    "\n",
    "    # Selecionando variaveis numericas\n",
    "    num_cols_corr = [\n",
    "        c for c, t in df_corr_base.dtypes\n",
    "        if t in (\"int\", \"bigint\", \"double\", \"float\")\n",
    "        and c not in [\"FPD\"]\n",
    "    ]\n",
    "\n",
    "    df_corr_sample, _ = split_stratified_data(df_corr_base)\n",
    "    pdf_corr = df_corr_sample.select(num_cols_corr).toPandas()\n",
    "    corr_matrix = pdf_corr.corr().abs()\n",
    "\n",
    "    upper = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "\n",
    "    to_drop = []\n",
    "    while True:\n",
    "        max_corr = upper.max().max()\n",
    "        if max_corr < thresh:\n",
    "            break\n",
    "\n",
    "        col_to_drop = (\n",
    "            upper.max()\n",
    "            .sort_values(ascending=False)\n",
    "            .index[0]\n",
    "        )\n",
    "\n",
    "        to_drop.append(col_to_drop)\n",
    "\n",
    "        upper = upper.drop(index=col_to_drop, columns=col_to_drop)\n",
    "    \n",
    "    df = df.drop(*to_drop)\n",
    "    return df, to_drop"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "42c367c5-6798-4389-891c-8ccb2c94ab41"
  },
  {
   "cell_type": "code",
   "source": [
    "# Funcao para criacao de colunas numericas (datediff) baseadas nas de datas, e posteriormente a remocao destas\n",
    "def adjust_and_drop_date_cols(df):\n",
    "    # Transformando \"var_12\" em tipo data\n",
    "    df = df.withColumn(\n",
    "        \"var_12\",\n",
    "        F.to_timestamp(\"var_12\", \"dd/MM/yyyy\")\n",
    "    )\n",
    "\n",
    "    # Verificando colunas do tipo date/datetime\n",
    "    date_cols = [\n",
    "        f.name\n",
    "        for f in df.schema.fields\n",
    "        if isinstance(f.dataType, (DateType, TimestampType))\n",
    "    ]\n",
    "\n",
    "    # Criacao de coluna com data de ref baseada no valor da safra\n",
    "    df = df.withColumn(\n",
    "        \"DATA_REF_SAFRA\",\n",
    "        F.to_date(F.col(\"SAFRA\").cast(\"string\"), \"yyyyMM\")\n",
    "    )\n",
    "\n",
    "    # Inclusao de novos colunas considerando datediff de variaveis em formato datetime ate entao\n",
    "    df = df.withColumn(\"DIAS_VAR_12\", F.datediff(F.col(\"DATA_REF_SAFRA\"), F.col(\"var_12\")))\n",
    "    df = df.withColumn(\"PAG_DIAS_DESDE_PRIMEIRA_FATURA\", F.datediff(F.col(\"DATA_REF_SAFRA\"), F.col(\"PAG_DT_PRIMEIRA_FATURA\")))\n",
    "\n",
    "    date_cols.append(\"DATA_REF_SAFRA\")\n",
    "    df = df.drop(*date_cols)\n",
    "    return df, date_cols"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "a4249bd2-9def-4d3c-afe6-938f15d68bbd"
  },
  {
   "cell_type": "code",
   "source": [
    "# Remocao de colunas que nao devem ser usadas na modelagem\n",
    "# FAT_VLR_FPD removido do book_faturamento (leakage fix Story 1.1)\n",
    "# FAT_FLAG_MIG2_AQUISICAO removido do book_faturamento (duplicata)\n",
    "# Mantemos drop defensivo caso notebook rode com feature store antigo\n",
    "def remove_other_misused_columns(df):\n",
    "    misused_columns = [\"PROD\", \"flag_mig2\", \"FAT_VLR_FPD\", \"FAT_FLAG_MIG2_AQUISICAO\"]\n",
    "    existing = [c for c in misused_columns if c in df.columns]\n",
    "    if existing:\n",
    "        df = df.drop(columns=existing)\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "10410b5e-70fe-41e9-bbeb-3ce45460a758"
  },
  {
   "cell_type": "code",
   "source": [
    "cep_uf_regiao = [\n",
    "    # NORTE\n",
    "    (\"69\", \"AM\", \"NORTE\"), (\"68\", \"AC\", \"NORTE\"), (\"66\", \"PA\", \"NORTE\"),\n",
    "    (\"65\", \"PA\", \"NORTE\"), (\"67\", \"RO\", \"NORTE\"), (\"77\", \"TO\", \"NORTE\"),\n",
    "    (\"78\", \"MT\", \"NORTE\"), (\"79\", \"RR\", \"NORTE\"),\n",
    "\n",
    "    # NORDESTE\n",
    "    (\"60\", \"CE\", \"NORDESTE\"), (\"61\", \"DF\", \"CENTRO-OESTE\"),  # DF tratado aqui\n",
    "    (\"62\", \"PI\", \"NORDESTE\"), (\"63\", \"TO\", \"NORTE\"),\n",
    "    (\"64\", \"PI\", \"NORDESTE\"), (\"65\", \"MA\", \"NORDESTE\"),\n",
    "    (\"66\", \"PA\", \"NORTE\"), (\"67\", \"RO\", \"NORTE\"),\n",
    "    (\"68\", \"AC\", \"NORTE\"), (\"69\", \"AM\", \"NORTE\"),\n",
    "\n",
    "    (\"40\", \"BA\", \"NORDESTE\"), (\"41\", \"BA\", \"NORDESTE\"),\n",
    "    (\"42\", \"BA\", \"NORDESTE\"), (\"43\", \"BA\", \"NORDESTE\"),\n",
    "    (\"44\", \"BA\", \"NORDESTE\"), (\"45\", \"BA\", \"NORDESTE\"),\n",
    "    (\"46\", \"BA\", \"NORDESTE\"), (\"47\", \"BA\", \"NORDESTE\"),\n",
    "    (\"48\", \"BA\", \"NORDESTE\"),\n",
    "\n",
    "    (\"50\", \"PE\", \"NORDESTE\"), (\"51\", \"PE\", \"NORDESTE\"),\n",
    "    (\"52\", \"PE\", \"NORDESTE\"), (\"53\", \"PE\", \"NORDESTE\"),\n",
    "    (\"54\", \"PE\", \"NORDESTE\"), (\"55\", \"PE\", \"NORDESTE\"),\n",
    "\n",
    "    (\"56\", \"AL\", \"NORDESTE\"), (\"57\", \"AL\", \"NORDESTE\"),\n",
    "    (\"58\", \"PB\", \"NORDESTE\"), (\"59\", \"PB\", \"NORDESTE\"),\n",
    "\n",
    "    (\"20\", \"RJ\", \"SUDESTE\"), (\"21\", \"RJ\", \"SUDESTE\"),\n",
    "    (\"22\", \"RJ\", \"SUDESTE\"), (\"23\", \"RJ\", \"SUDESTE\"),\n",
    "    (\"24\", \"RJ\", \"SUDESTE\"),\n",
    "\n",
    "    (\"30\", \"MG\", \"SUDESTE\"), (\"31\", \"MG\", \"SUDESTE\"),\n",
    "    (\"32\", \"MG\", \"SUDESTE\"), (\"33\", \"MG\", \"SUDESTE\"),\n",
    "    (\"34\", \"MG\", \"SUDESTE\"), (\"35\", \"MG\", \"SUDESTE\"),\n",
    "    (\"36\", \"MG\", \"SUDESTE\"), (\"37\", \"MG\", \"SUDESTE\"),\n",
    "    (\"38\", \"MG\", \"SUDESTE\"), (\"39\", \"MG\", \"SUDESTE\"),\n",
    "\n",
    "    (\"01\", \"SP\", \"SUDESTE\"), (\"02\", \"SP\", \"SUDESTE\"),\n",
    "    (\"03\", \"SP\", \"SUDESTE\"), (\"04\", \"SP\", \"SUDESTE\"),\n",
    "    (\"05\", \"SP\", \"SUDESTE\"), (\"06\", \"SP\", \"SUDESTE\"),\n",
    "    (\"07\", \"SP\", \"SUDESTE\"), (\"08\", \"SP\", \"SUDESTE\"),\n",
    "    (\"09\", \"SP\", \"SUDESTE\"),\n",
    "\n",
    "    # SUL\n",
    "    (\"80\", \"PR\", \"SUL\"), (\"81\", \"PR\", \"SUL\"),\n",
    "    (\"82\", \"PR\", \"SUL\"), (\"83\", \"PR\", \"SUL\"),\n",
    "    (\"84\", \"PR\", \"SUL\"), (\"85\", \"PR\", \"SUL\"),\n",
    "    (\"86\", \"PR\", \"SUL\"), (\"87\", \"PR\", \"SUL\"),\n",
    "    (\"88\", \"SC\", \"SUL\"),\n",
    "    (\"89\", \"SC\", \"SUL\"),\n",
    "    (\"90\", \"RS\", \"SUL\"),\n",
    "    (\"91\", \"RS\", \"SUL\"),\n",
    "    (\"92\", \"RS\", \"SUL\"),\n",
    "    (\"93\", \"RS\", \"SUL\"),\n",
    "    (\"94\", \"RS\", \"SUL\"),\n",
    "    (\"95\", \"RS\", \"SUL\"),\n",
    "    (\"96\", \"RS\", \"SUL\"),\n",
    "    (\"97\", \"RS\", \"SUL\"),\n",
    "    (\"98\", \"RS\", \"SUL\"),\n",
    "    (\"99\", \"RS\", \"SUL\"),\n",
    "\n",
    "    # CENTRO-OESTE\n",
    "    (\"70\", \"DF\", \"CENTRO-OESTE\"), (\"71\", \"DF\", \"CENTRO-OESTE\"),\n",
    "    (\"72\", \"DF\", \"CENTRO-OESTE\"), (\"73\", \"DF\", \"CENTRO-OESTE\"),\n",
    "    (\"74\", \"DF\", \"CENTRO-OESTE\"), (\"75\", \"GO\", \"CENTRO-OESTE\"),\n",
    "    (\"76\", \"GO\", \"CENTRO-OESTE\"),\n",
    "    (\"78\", \"MT\", \"CENTRO-OESTE\"),\n",
    "    (\"79\", \"MS\", \"CENTRO-OESTE\"),\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "3896a7c0-18b3-4e34-a653-c7f8975345b0"
  },
  {
   "cell_type": "code",
   "source": [
    "# Funcao que converte \"CEP3\" em UF e Regiao\n",
    "def convert_cep3_uf_regiao(df):\n",
    "    df = df.withColumn(\n",
    "        \"CEP_2\",\n",
    "        F.col(\"CEP_3_digitos\").substr(1, 2)\n",
    "    )\n",
    "\n",
    "    df_cep_map = spark.createDataFrame(\n",
    "        cep_uf_regiao,\n",
    "        [\"CEP_2\", \"UF\", \"REGIAO\"]\n",
    "    )\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        .join(df_cep_map, on=\"CEP_2\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"UF\", F.coalesce(F.col(\"UF\"), F.lit(\"OUTROS\")))\n",
    "        .withColumn(\"REGIAO\", F.coalesce(F.col(\"REGIAO\"), F.lit(\"OUTROS\")))\n",
    "    )\n",
    "\n",
    "    df = df.drop(\"CEP_3_digitos\", \"CEP_2\")\n",
    "    return df\n"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "f6d25bde-0b32-4b16-96fd-fbc6bd59444b"
  },
  {
   "cell_type": "code",
   "source": [
    "# Funcao de agregacao de todas as transformacoes/limpezas aplicadas nas bases para modelagem/avaliacao\n",
    "def apply_cleanings_to_df(df):\n",
    "    \n",
    "    df = clean_empty_keys_data(df)\n",
    "    df = convert_cep3_uf_regiao(df)\n",
    "    df, date_cols = adjust_and_drop_date_cols(df)\n",
    "    df, high_missing = remove_high_missing_values(df)\n",
    "    df, low_card = remove_low_cardinality_values(df)\n",
    "    df, high_corr = remove_high_correlation_data(df)\n",
    "    df = remove_other_misused_columns(df)\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "311f6e13-75e8-47f3-9506-76cbbe800cf8"
  },
  {
   "cell_type": "code",
   "source": [
    "# Leitura dos dados\n",
    "df_spark = spark.sql(\"SELECT * FROM Gold.feature_store.clientes_consolidado\")\n",
    "\n",
    "# Chamar aqui funcao apply cleanings no df_spark_total\n",
    "df_spark_clean = apply_cleanings_to_df(df_spark)\n",
    "\n",
    "# Filtragem de clientes que contrataram pacote pos -> Adicionar isso apos a aplicacao de transformacao\n",
    "df_spark_clean_clientes_pos = df_spark_clean.filter(F.col(\"FLAG_INSTALACAO\") == 1)\n",
    "df_spark_clean_clientes_reprovados = df_spark_clean.filter(F.col(\"FLAG_INSTALACAO\") == 0)\n",
    "\n",
    "# Remocao da coluna \"FLAG_INSTALACAO\", pois modelos nao utilizarao (variavel apenas para segmentar situacao de clientes)\n",
    "df_spark_clean_clientes_pos = df_spark_clean_clientes_pos.drop(\"FLAG_INSTALACAO\")\n",
    "df_spark_clean_clientes_reprovados = df_spark_clean_clientes_reprovados.drop(\"FLAG_INSTALACAO\")\n",
    "\n",
    "# Separacao das bases de dados (Sample (Treino + Val posteriormente), Teste (OOS) e Out-of-Time (OOT))\n",
    "list_safras = sort_periods(df_spark_clean_clientes_pos)\n",
    "\n",
    "# Gerando lista das safras para treino/teste e OOT\n",
    "safras_train_oos = list_safras[:4]\n",
    "safras_oot = list_safras[4:]\n",
    "\n",
    "# Dataframe contendo todos os dados limpos das 4 primeiras safras\n",
    "df_4_safras = df_spark_clean_clientes_pos.filter(F.col(\"SAFRA\").isin(safras_train_oos))\n",
    "\n",
    "# Split estratificada das bases de treino (+ val) e teste (Out of Sample)\n",
    "df_sample_spark, df_oos_spark = split_stratified_data(df_4_safras, percent=0.25)\n",
    "\n",
    "# Definicao de dataframe com dados do conjunto OOT (Out of Time)\n",
    "df_oot_spark = df_spark_clean_clientes_pos.filter(F.col(\"SAFRA\").isin(safras_oot))\n",
    "\n",
    "# Conversao dos dataframes spark para pandas\n",
    "df_sample = df_sample_spark.toPandas()\n",
    "df_oos = df_oos_spark.toPandas()\n",
    "df_oot = df_oot_spark.toPandas()\n",
    "\n",
    "print(\"Shapes finais:\")\n",
    "print(\"Sample:\", df_sample.shape)\n",
    "print(\"OOS:\", df_oos.shape)\n",
    "print(\"OOT:\", df_oot.shape)"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "998a0893-0e58-4dde-a6a7-7e712f951bd5"
  },
  {
   "cell_type": "code",
   "source": [
    "# Checagem de informacoes de volumetria e tipo das colunas das bases\n",
    "print(\"Info df Sample: \")\n",
    "display(df_sample.info())\n",
    "print(\"\\n\")\n",
    "print(\"Info df OOS: \")\n",
    "display(df_oos.info())\n",
    "print(\"\\n\")\n",
    "print(\"Info df OOT: \")\n",
    "display(df_oot.info())"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "f17dc54c-05a5-426a-b11d-841fa43bf98e"
  },
  {
   "cell_type": "code",
   "source": [
    "# Remocao de eventuais duplicatas das bases\n",
    "df_sample = df_sample.drop_duplicates()\n",
    "df_oos = df_oos.drop_duplicates()\n",
    "df_oot = df_oot.drop_duplicates()"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "3d833671-08c1-477f-a8d9-eac0e8a0cf75"
  },
  {
   "cell_type": "code",
   "source": [
    "# Verificando contagem de registros por safra e target (FPD) - Sample train + val\n",
    "df_sample[[\"SAFRA\",\"FPD\"]].value_counts().to_frame().sort_values(by='SAFRA')"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "collapsed": false
   },
   "id": "f9de7948-1160-411e-83c2-125302534bc4"
  },
  {
   "cell_type": "code",
   "source": [
    "# Verificando contagem de registros por safra e target (FPD) - Teste (OOS)\n",
    "df_oos[[\"SAFRA\",\"FPD\"]].value_counts().to_frame().sort_values(by='SAFRA')"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "381e3dc3-1898-4e3a-b679-0e1d9d978706"
  },
  {
   "cell_type": "code",
   "source": [
    "# Verificando contagem de registros por safra e target (FPD) - OOT\n",
    "df_oot[[\"SAFRA\",\"FPD\"]].value_counts().to_frame().sort_values(by='SAFRA')"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "af40671c-06dc-4297-b9a1-f71df709f9d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Separação da base amostrada em Treino x Validacao X OOT"
   ],
   "metadata": {},
   "id": "16546b13"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Verificar necessidade de blocos de codigo abaixo"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "5322b241-b431-4105-9881-15e7f203a003"
  },
  {
   "cell_type": "code",
   "source": [
    "safras_train = [202410, 202411, 202412]\n",
    "safras_val = [202501]\n",
    "\n",
    "# Divisao dos dataframes\n",
    "df_train = df_sample[df_sample[\"SAFRA\"].isin(safras_train)]\n",
    "df_val = df_sample[df_sample[\"SAFRA\"].isin(safras_val)]\n",
    "\n",
    "# Removendo coluna FPD (target) das features, e gerando y com FPD para cada uma das bases\n",
    "X_train = df_train.drop(columns=[\"FPD\"])\n",
    "y_train = df_train[\"FPD\"]\n",
    "\n",
    "X_val = df_val.drop(columns=[\"FPD\"])\n",
    "y_val = df_val[\"FPD\"]\n",
    "\n",
    "# Para treinamento do modelo considerando as bases train + val (enriquecimento do modelo)\n",
    "X_train_final = pd.concat([X_train, X_val], axis=0)\n",
    "y_train_final = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "# Preparacao da base OOS\n",
    "X_oos_agg = df_oos.drop(columns=[\"FPD\"])\n",
    "y_oos_agg = df_oos[\"FPD\"]\n",
    "\n",
    "# Preparacao da base OOT\n",
    "X_oot_agg = df_oot.drop(columns=[\"FPD\"])\n",
    "y_oot_agg = df_oot[\"FPD\"]\n"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "cb3a07a9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Separação de Variáveis por tipos"
   ],
   "metadata": {},
   "id": "c8712cf2"
  },
  {
   "cell_type": "code",
   "source": [
    "num_features = [n for n in X_train.select_dtypes(include=[\"int32\", \"int64\", \"float32\", \"float64\"]).columns if n != \"SAFRA\"]\n",
    "cat_features = [c for c in X_train.select_dtypes(include=[\"object\", \"category\"]).columns if c != \"NUM_CPF\"]"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "1248e8c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Montagem dos Pipelines sklearn"
   ],
   "metadata": {},
   "id": "8834b53a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Pipeline Regressao Logistica"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "03e27f0b-9d5e-4374-90ff-b784a2de7be6"
  },
  {
   "cell_type": "code",
   "source": [
    "numeric_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", CountEncoder(normalize=True))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", numeric_pipe, num_features),\n",
    "    (\"cat\", categorical_pipe, cat_features)\n",
    "])\n",
    "\n",
    "pipeline_RL = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", LogisticRegression(\n",
    "        solver=\"liblinear\",\n",
    "        penalty=\"l1\",\n",
    "        max_iter=2000,\n",
    "        tol=1e-3,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "b11d7d04"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Pipeline LightGBM"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "84221429-eb7f-4233-90ba-83c3720c3530"
  },
  {
   "cell_type": "code",
   "source": [
    "numeric_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", CountEncoder(\n",
    "        normalize=True,\n",
    "        handle_unknown=0,\n",
    "        handle_missing=0\n",
    "    ))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipe, num_features),\n",
    "        (\"cat\", categorical_pipe, cat_features),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "lgbm = LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    boosting_type=\"gbdt\",\n",
    "    learning_rate=0.05,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "pipeline_LGBM = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", lgbm)\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "298ad55b-bd8b-414e-adb0-f3be78481ca3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Ajuste de Hiperparâmetros"
   ],
   "metadata": {},
   "id": "b58f6ef7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Grid Search e Treino com HPs escolhidos (Treino + Val) - Regressao Logistica"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "a3e8fa06-874e-4f10-a2c6-2a75d8ac2308"
  },
  {
   "cell_type": "code",
   "source": [
    "param_grid_RL = {\n",
    "    \"model__C\": [0.01, 0.05, 0.1, 0.5],\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(\n",
    "    n_splits=4,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "grid_RL = GridSearchCV(\n",
    "    pipeline_RL,\n",
    "    param_grid=param_grid_RL,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=3\n",
    ")\n",
    "\n",
    "# Aplicando fit no conjunto de validacao (para selecao de hiperparametros)\n",
    "grid_RL.fit(X_val, y_val)\n"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "b140417a"
  },
  {
   "cell_type": "code",
   "source": [
    "# Exibicao dos melhores hiper-parametros e scores - Ajuste na base de validacao (safra 2025-01)\n",
    "print(\"Melhores hiper-parâmetros RL:\", grid_RL.best_params_)\n",
    "print(\"Melhor score (AUC) RL: \", grid_RL.best_score_)"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "44e61f5b-dbb3-4df0-9c65-87c60ee1bb4a"
  },
  {
   "cell_type": "code",
   "source": [
    "# Setando melhores hiperparametros encontrados no grid search ao modelo\n",
    "pipeline_RL.set_params(**grid_RL.best_params_)\n",
    "\n",
    "# Realizando novo treinamento com safras 2024-10 a 2025-01 com concatenacao de dados base train e val\n",
    "pipeline_RL.fit(X_train_final, y_train_final)"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "44cdeb70-cc6f-4b1d-a869-1475b15a4475"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Grid Search e Treino com HPs escolhidos (Treino + Val) - LightGBM "
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "ba6e60b4-8402-463e-8042-d05042dc1840"
  },
  {
   "cell_type": "code",
   "source": [
    "param_grid_LGBM = {\n",
    "    \"model__n_estimators\": [250, 500],\n",
    "    \"model__max_depth\": [4, 7]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(\n",
    "    n_splits=4,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "grid_LGBM = GridSearchCV(\n",
    "    pipeline_LGBM,\n",
    "    param_grid=param_grid_LGBM,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=3,\n",
    "    error_score=\"raise\"\n",
    ")\n",
    "\n",
    "# Aplicando fit no conjunto de validacao (para selecao de hiperparametros)\n",
    "grid_LGBM.fit(X_val, y_val)"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "a4bc74ae-e8d0-4eec-a860-98e64703755d"
  },
  {
   "cell_type": "code",
   "source": [
    "# Exibicao dos melhores hiper-parametros e scores - Ajuste na base de validacao (safra 2025-01)\n",
    "print(\"Melhores hiper-parâmetros LGBM:\", grid_LGBM.best_params_)\n",
    "print(\"Melhor score (AUC) LGBM: \", grid_LGBM.best_score_)"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "59d66f9d-9519-4513-b28e-48feeeadbdbc"
  },
  {
   "cell_type": "code",
   "source": [
    "# Setando melhores hiperparametros encontrados no grid search ao modelo\n",
    "pipeline_LGBM.set_params(**grid_LGBM.best_params_)\n",
    "\n",
    "# Realizando novo treinamento com safras 2024-10 a 2025-01 com concatenacao de dados base train e val\n",
    "pipeline_LGBM.fit(X_train_final, y_train_final)"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "b3c806dd-44d7-4a2e-8829-09e1f3fc47f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Avaliação por Safra (AUC e KS)\n",
    "\n",
    "#### Processo realizado nas bases de treino, val e OOT (individuais e consolidadas)"
   ],
   "metadata": {},
   "id": "6846b256"
  },
  {
   "cell_type": "code",
   "source": [
    "# Selecao de dados das bases a partir da safra (para X e y)\n",
    "# X - df de features original\n",
    "# y - serie com target original\n",
    "# list_safras = lista com periodo(s) desejados para filtragem\n",
    "def filter_xy_by_safra(X, y, list_safras):\n",
    "    X_f = X[X[\"SAFRA\"].isin(list_safras)]\n",
    "    y_f = y.loc[X_f.index]\n",
    "    return X_f, y_f"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "5afc8b4b-cb60-4807-8227-3d9f978f2a0e"
  },
  {
   "cell_type": "code",
   "source": [
    "# Funcao para calculo da metrica KS\n",
    "def ks_stat(y_true, y_score):\n",
    "    df = pd.DataFrame({\"y\": y_true, \"p\": y_score})\n",
    "    df = df.sort_values(\"p\")\n",
    "    df[\"cum_good\"] = (1 - df[\"y\"]).cumsum() / (1 - df[\"y\"]).sum()\n",
    "    df[\"cum_bad\"]  = df[\"y\"].cumsum() / df[\"y\"].sum()\n",
    "    return np.max(np.abs(df[\"cum_bad\"] - df[\"cum_good\"]))\n",
    "\n",
    "# Funcao de avaliacao das metricas de KS e AUC\n",
    "def evaluation_auc_ks(X, y, pipe, name_evaluation=\"\", verbose=True):\n",
    "    p_s = pipe.predict_proba(X)[:, 1]\n",
    "    auc = round(roc_auc_score(y, p_s),5)\n",
    "    ks = round(ks_stat(y, p_s),5)\n",
    "    if verbose:\n",
    "        print(f\"AVALIACAO NA(S) SAFRA(S) {name_evaluation}: \")\n",
    "        print(\"AUC:\", auc)\n",
    "        print(\"KS :\", ks)\n",
    "        print(\"-\" * 30)\n",
    "    return auc, ks\n",
    "\n",
    "# Funcao para logar metricas de avaliacao no MLflow por safra\n",
    "def log_safra_metrics_mlflow(model_name, dict_safras, generate_map_func, \n",
    "                              X_train, y_train, X_oos, y_oos, X_oot, y_oot, pipeline):\n",
    "    \"\"\"Loga metricas AUC e KS por safra no MLflow para um dado modelo/pipeline.\"\"\"\n",
    "    results = {}\n",
    "    for key, value in dict_safras.items():\n",
    "        map_step_data = generate_map_func(X_train, y_train, X_oos, y_oos, X_oot, y_oot)\n",
    "        X = map_step_data[key][\"X\"]\n",
    "        y = map_step_data[key][\"Y\"]\n",
    "        X_f, y_f = filter_xy_by_safra(X, y, dict_safras[key])\n",
    "        auc, ks = evaluation_auc_ks(X_f, y_f, pipeline, key)\n",
    "        \n",
    "        # Sanitizar nome para MLflow (remover caracteres especiais)\n",
    "        safe_key = key.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        mlflow.log_metric(f\"{model_name}_AUC_{safe_key}\", auc)\n",
    "        mlflow.log_metric(f\"{model_name}_KS_{safe_key}\", ks)\n",
    "        results[key] = {\"AUC\": auc, \"KS\": ks}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Dicionarios para facilitar filtragem de dados das bases X e y de acordo com safras desejadas para analise\n",
    "dict_safras = {\n",
    "    \"TREINO - 202410\" : [202410],\n",
    "    \"TREINO - 202411\" : [202411],\n",
    "    \"TREINO - 202412\" : [202412],\n",
    "    \"TREINO \\ VAL - 202501\" : [202501],\n",
    "    \"TREINO (CONS)\" : [202410, 202411, 202412, 202501],\n",
    "    \"OOS - 202410\" : [202410],\n",
    "    \"OOS - 202411\" : [202411],\n",
    "    \"OOS - 202412\" : [202412],\n",
    "    \"OOS - 202501\" : [202501],\n",
    "    \"OOS (CONS)\" : [202410, 202411, 202412, 202501],    \n",
    "    \"OOT - 202502\" : [202502],\n",
    "    \"OOT - 202503\" : [202503],\n",
    "    \"OOT GERAL (CONS)\" : [202502, 202503]\n",
    "}\n",
    "\n",
    "def generate_map_step_data(X_train, y_train, X_oos, y_oos, X_oot, y_oot):\n",
    "    map_step_data = {\n",
    "        \"TREINO - 202410\" : {\"X\": X_train, \"Y\" : y_train},\n",
    "        \"TREINO - 202411\" : {\"X\": X_train, \"Y\" : y_train},\n",
    "        \"TREINO - 202412\" : {\"X\": X_train, \"Y\" : y_train},\n",
    "        \"TREINO \\ VAL - 202501\" : {\"X\": X_train, \"Y\" : y_train},\n",
    "        \"TREINO (CONS)\" : {\"X\": X_train, \"Y\" : y_train},\n",
    "        \"OOS - 202410\" : {\"X\": X_oos, \"Y\" : y_oos},\n",
    "        \"OOS - 202411\" : {\"X\": X_oos, \"Y\" : y_oos},\n",
    "        \"OOS - 202412\" : {\"X\": X_oos, \"Y\" : y_oos},\n",
    "        \"OOS - 202501\" : {\"X\": X_oos, \"Y\" : y_oos},\n",
    "        \"OOS (CONS)\" : {\"X\": X_oos, \"Y\" : y_oos},\n",
    "        \"OOT - 202502\" : {\"X\": X_oot, \"Y\" : y_oot},\n",
    "        \"OOT - 202503\" : {\"X\": X_oot, \"Y\" : y_oot},\n",
    "        \"OOT GERAL (CONS)\" : {\"X\": X_oot, \"Y\" : y_oot},\n",
    "    }\n",
    "    return map_step_data"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "c162d305"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Avaliacao por safra inclui: Treino, OOS, OOT (individuais e consolidadas) + MLflow logging"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "ce6a176f-5ee9-4f2a-9ecf-1352f371f5e3"
  },
  {
   "cell_type": "code",
   "source": [
    "### Avalicao dos resultados nas safras treino, val e OOT - Regressao Logistica\n",
    "with mlflow.start_run(run_name=\"LogisticRegression_Baseline\") as run_rl:\n",
    "    # Logar parametros do modelo\n",
    "    best_params_rl = pipeline_RL.named_steps[\"model\"].get_params()\n",
    "    mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"penalty\", best_params_rl.get(\"penalty\", \"l1\"))\n",
    "    mlflow.log_param(\"solver\", best_params_rl.get(\"solver\", \"saga\"))\n",
    "    mlflow.log_param(\"C\", best_params_rl.get(\"C\"))\n",
    "    mlflow.log_param(\"max_iter\", best_params_rl.get(\"max_iter\"))\n",
    "    mlflow.log_param(\"n_features\", len(X_train_final.columns))\n",
    "    mlflow.log_param(\"n_samples_train\", len(X_train_final))\n",
    "    \n",
    "    # Logar metricas por safra\n",
    "    print(\"Avaliacao das metricas da Regressao Logistica por base: \")\n",
    "    results_rl = log_safra_metrics_mlflow(\n",
    "        \"RL\", dict_safras, generate_map_step_data,\n",
    "        X_train_final, y_train_final, X_oos_agg, y_oos_agg, X_oot_agg, y_oot_agg,\n",
    "        pipeline_RL\n",
    "    )\n",
    "    \n",
    "    # Logar modelo\n",
    "    mlflow.sklearn.log_model(pipeline_RL, \"model_logistic_regression\")\n",
    "    \n",
    "    # Logar coeficientes como artefato\n",
    "    coefs = pipeline_RL.named_steps[\"model\"].coef_[0]\n",
    "    feature_names = pipeline_RL.named_steps[\"prep\"].get_feature_names_out()\n",
    "    df_coefs = pd.DataFrame({\"feature\": feature_names, \"coef\": coefs}).sort_values(\"coef\", key=abs, ascending=False)\n",
    "    df_coefs.to_csv(\"/tmp/lr_coefficients.csv\", index=False)\n",
    "    mlflow.log_artifact(\"/tmp/lr_coefficients.csv\", \"feature_analysis\")\n",
    "    \n",
    "    print(f\"\\nMLflow Run ID (RL): {run_rl.info.run_id}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "123ae738-4d52-4204-942e-be565c085db3"
  },
  {
   "cell_type": "code",
   "source": [
    "### Avalicao dos resultados nas safras treino, val e OOT - LightGBM\n",
    "with mlflow.start_run(run_name=\"LightGBM_Baseline\") as run_lgbm:\n",
    "    # Logar parametros do modelo\n",
    "    best_params_lgbm = pipeline_LGBM.named_steps[\"model\"].get_params()\n",
    "    mlflow.log_param(\"model_type\", \"LightGBM\")\n",
    "    mlflow.log_param(\"n_estimators\", best_params_lgbm.get(\"n_estimators\"))\n",
    "    mlflow.log_param(\"max_depth\", best_params_lgbm.get(\"max_depth\"))\n",
    "    mlflow.log_param(\"learning_rate\", best_params_lgbm.get(\"learning_rate\"))\n",
    "    mlflow.log_param(\"num_leaves\", best_params_lgbm.get(\"num_leaves\"))\n",
    "    mlflow.log_param(\"boosting_type\", best_params_lgbm.get(\"boosting_type\", \"gbdt\"))\n",
    "    mlflow.log_param(\"n_features\", len(X_train_final.columns))\n",
    "    mlflow.log_param(\"n_samples_train\", len(X_train_final))\n",
    "    \n",
    "    # Logar metricas por safra\n",
    "    print(\"Avaliacao das metricas do LightGBM por base: \")\n",
    "    results_lgbm = log_safra_metrics_mlflow(\n",
    "        \"LGBM\", dict_safras, generate_map_step_data,\n",
    "        X_train_final, y_train_final, X_oos_agg, y_oos_agg, X_oot_agg, y_oot_agg,\n",
    "        pipeline_LGBM\n",
    "    )\n",
    "    \n",
    "    # Logar modelo\n",
    "    mlflow.sklearn.log_model(pipeline_LGBM, \"model_lightgbm\")\n",
    "    \n",
    "    # Logar feature importance como artefato\n",
    "    lgbm_model = pipeline_LGBM.named_steps[\"model\"]\n",
    "    feature_names = pipeline_LGBM.named_steps[\"prep\"].get_feature_names_out()\n",
    "    df_importance = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance\": lgbm_model.feature_importances_\n",
    "    }).sort_values(\"importance\", ascending=False)\n",
    "    df_importance.to_csv(\"/tmp/lgbm_feature_importance.csv\", index=False)\n",
    "    mlflow.log_artifact(\"/tmp/lgbm_feature_importance.csv\", \"feature_analysis\")\n",
    "    \n",
    "    # Plotar e logar feature importance (top 30)\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    top_30 = df_importance.head(30)\n",
    "    ax.barh(range(len(top_30)), top_30[\"importance\"].values)\n",
    "    ax.set_yticks(range(len(top_30)))\n",
    "    ax.set_yticklabels(top_30[\"feature\"].values, fontsize=8)\n",
    "    ax.set_xlabel(\"Feature Importance\")\n",
    "    ax.set_title(\"Top 30 Features - LightGBM\")\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(\"/tmp/lgbm_feature_importance.png\", dpi=150)\n",
    "    mlflow.log_artifact(\"/tmp/lgbm_feature_importance.png\", \"plots\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nMLflow Run ID (LGBM): {run_lgbm.info.run_id}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "58611f8f-25df-413d-a0f7-a909b1831b90"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adicionar feature selection via LGBM em cada step (Considerando limiar minimo)"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "2716920a-2230-44d7-bae9-191218149394"
  },
  {
   "cell_type": "code",
   "source": [
    "# Mensuracao do ganho de KS por fonte de dados\n",
    "\n",
    "## Colunas por fonte - Etapas do Incremento das features\n",
    "# Score 1 (Base Bureau) - Target Score 1\n",
    "# Score 2 (Base Bureau) - Target Score 2\n",
    "# Base Cadastros - STATUSRF, var_02 a var_25, UF, REGIAO\n",
    "# Base Telco - var_26 a var_93\n",
    "# Base Recargas - REC_...\n",
    "# Base Pagamentos - PAG_...\n",
    "# Base Faturamento - FAT_..."
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "74075896-f8fc-4ee4-80d8-6f81834dff80"
  },
  {
   "cell_type": "code",
   "source": [
    "# Funcao para aplicacao de filtragem das features de acordo com etapa atual\n",
    "def filter_features(X_train, X_oot_agg, list_features):\n",
    "    X_train_filtered = X_train[list_features]\n",
    "    X_oot_agg_filtered = X_oot_agg[list_features]\n",
    "    return X_train_filtered, X_oot_agg_filtered"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "5bbe9bff-515a-4cc5-9151-f07ec00e1de3"
  },
  {
   "cell_type": "code",
   "source": [
    "# Funcao que define em qual etapa de agregacao de features estamos no momento atual\n",
    "def current_step(step_num):\n",
    "    dict_step = {\n",
    "        0 : \"SC 1\",\n",
    "        1 : \"SC 1 + SC 2\",\n",
    "        2 : \"SC 1 + SC 2 + CAD\",\n",
    "        3 : \"SC 1 + SC 2 + CAD + TELCO\",\n",
    "        4 : \"SC 1 + SC 2 + CAD + TELCO + REC\",\n",
    "        5 : \"SC 1 + SC 2 + CAD + TELCO + REC + PAG\",\n",
    "        6 : \"SC 1 + SC 2 + CAD + TELCO + REC + PAG + FAT\",\n",
    "    }\n",
    "    return dict_step[step_num]"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "9e50a57f-fef3-457b-a308-bea3718c0c81"
  },
  {
   "cell_type": "code",
   "source": [
    "# Funcao que atualiza o pipeline do modelo (incluindo as colunas a serem consideradas para o treino)\n",
    "def update_pipeline(X, name_model):\n",
    "\n",
    "    num_features = [n for n in X.select_dtypes(include=[\"int32\", \"int64\", \"float32\", \"float64\"]).columns if n != \"SAFRA\"] \n",
    "    cat_features = [c for c in X.select_dtypes(include=[\"object\", \"category\"]).columns if c != \"NUM_CPF\"]\n",
    "\n",
    "    if name_model == \"Reg Log\":\n",
    "        \n",
    "        numeric_pipe = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ])\n",
    "\n",
    "        categorical_pipe = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"encoder\", CountEncoder(normalize=True))\n",
    "        ])\n",
    "\n",
    "        preprocess = ColumnTransformer([\n",
    "            (\"num\", numeric_pipe, num_features),\n",
    "            (\"cat\", categorical_pipe, cat_features)\n",
    "        ])\n",
    "\n",
    "        model = LogisticRegression(\n",
    "            solver=\"liblinear\",\n",
    "            penalty=\"l1\",\n",
    "            max_iter=2000,\n",
    "            C=grid_RL.best_params_[\"model__C\"],\n",
    "            tol=1e-3,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    else: # LightGBM\n",
    "\n",
    "        numeric_pipe = Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "        ])\n",
    "\n",
    "        categorical_pipe = Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"encoder\", CountEncoder(\n",
    "                normalize=True,\n",
    "                handle_unknown=0,\n",
    "                handle_missing=0\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        preprocess = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num\", numeric_pipe, num_features),\n",
    "                (\"cat\", categorical_pipe, cat_features),\n",
    "            ],\n",
    "            remainder=\"drop\"\n",
    "        )\n",
    "\n",
    "        model = LGBMClassifier(\n",
    "            objective=\"binary\",\n",
    "            boosting_type=\"gbdt\",\n",
    "            learning_rate=0.05,\n",
    "            max_depth=grid_LGBM.best_params_[\"model__max_depth\"],\n",
    "            n_estimators=grid_LGBM.best_params_[\"model__n_estimators\"],\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbosity=-1\n",
    "        )\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    return pipeline"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "a4c94ae5-289b-491b-8aa2-4ab53fb6867f"
  },
  {
   "cell_type": "code",
   "source": [
    "# Treinamento e avalicao de modelos fonte a fonte\n",
    "# Definicao do conjunto de variaveis para cada uma das fontes\n",
    "feat_score_1 = [\"TARGET_SCORE_01\"]\n",
    "feat_score_2 = [\"TARGET_SCORE_02\"]\n",
    "feats_cadastro = [x for x in X_train_final.columns if \"var_\" in x and x <= \"var_25\"] + [\"STATUSRF\", \"UF\", \"REGIAO\", \"DIAS_VAR_12\"]\n",
    "feats_telco = [x for x in X_train_final.columns if \"var_\" in x and x >= \"var_26\"]\n",
    "feats_recargas = [x for x in X_train_final.columns if \"REC_\" in x]\n",
    "feats_pagamentos = [x for x in X_train_final.columns if \"PAG_\" in x]\n",
    "feats_faturamento = [x for x in X_train_final.columns if \"FAT_\" in x]\n",
    "\n",
    "# Execucao de loop de treinamento com inclusao de features incrementais por fonte\n",
    "list_sources = [feat_score_1, feat_score_2, feats_cadastro, feats_telco, feats_recargas, feats_pagamentos, feats_faturamento]\n",
    "list_features = [\"NUM_CPF\", \"SAFRA\"] # Entrarao como colunas para filtragem dos dados, mas nao como features\n",
    "list_dict_results = []\n",
    "list_models = [\"Reg Log\", \"LGBM\"]\n",
    "\n",
    "# Iteracao por cada uma das fontes\n",
    "for idx, source in enumerate(list_sources):\n",
    "    list_features.extend(source) # Montando a lista de features que sera utilizada como\n",
    "    \n",
    "    # Filtragem dos dados para grupo de features atual\n",
    "    X_train_final_filtered, X_oot_agg_filtered = filter_features(X_train_final, X_oot_agg, list_features)\n",
    "    \n",
    "    # Selecao de qual modelo sera treinado/avaliado\n",
    "    for model in list_models:\n",
    "\n",
    "        # Atualizacao dos tipos de dados por coluna (Para nao quebrar o Pipeline)\n",
    "        pipeline_ks_inc = update_pipeline(X_train_final_filtered, name_model = model)\n",
    "\n",
    "        # Treinamento do modelo considerando lista de features atual\n",
    "        pipeline_ks_inc.fit(X_train_final_filtered, y_train_final)\n",
    "\n",
    "        for key, value in dict_safras.items():\n",
    "            map_step_data = generate_map_step_data(X_train_final_filtered, y_train_final, X_oot_agg_filtered, y_oot_agg)\n",
    "            X = map_step_data[key][\"X\"]\n",
    "            y = map_step_data[key][\"Y\"]\n",
    "            X_f , y_f = filter_xy_by_safra(X, y, dict_safras[key])\n",
    "            auc, ks = evaluation_auc_ks(X_f, y_f, pipeline_ks_inc, key, verbose=False)\n",
    "            conj_features = current_step(idx)\n",
    "            dict_result = {\n",
    "                \"MODELO\" : model,\n",
    "                \"CONJ FEATURES\" : conj_features,\n",
    "                \"BASE\" : key,\n",
    "                \"AUC\" : auc,\n",
    "                \"KS\" : ks\n",
    "            }\n",
    "            list_dict_results.append(dict_result)\n",
    "\n",
    "# Dataframe com resultados - apos avaliacoes em cada uma das bases\n",
    "df_results_ks_inc = pd.DataFrame(list_dict_results)\n",
    "df_results_ks_inc"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "collapsed": false
   },
   "id": "bacb419b-9de0-4111-9bfc-0635caa26e78"
  },
  {
   "cell_type": "code",
   "source": [
    "# Analise da evolucao de AUC e KS nas bases Treino + Val para cada grupo de features\n",
    "df_results_ks_inc[df_results_ks_inc[\"BASE\"] == \"TREINO (CONS)\"]"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "a47542e6-2b25-47d6-97d9-4628e12ebd30"
  },
  {
   "cell_type": "code",
   "source": [
    "# Analise da evolucao de AUC e KS nas bases OOT Geral para cada grupo de features\n",
    "df_results_ks_inc[df_results_ks_inc[\"BASE\"] == \"OOT GERAL (CONS)\"]"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "9c4ba234-a0b3-445f-829b-ffb15cf602db"
  },
  {
   "cell_type": "code",
   "source": [
    "# Remocao de features da source \"Cadastro\" - Gerou pouco ganho / perda de AUC e KS ao modelo\n",
    "list_sources_final = [feat_score_1, feat_score_2, feats_telco,feats_recargas, feats_pagamentos, feats_faturamento]\n",
    "\n",
    "list_source_final_columns = []\n",
    "for x in list_sources_final:\n",
    "    list_source_final_columns.extend(x)\n",
    "\n",
    "X_train_final_filtered_sources = X_train_final[list_source_final_columns]"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "8e16bfa5-2a6c-4f85-96c9-aba2f894be75"
  },
  {
   "cell_type": "code",
   "source": [
    "# Feature Selection - Univariada (IV)\n",
    "def calculate_iv_numericals(df, feature, target, total_good, total_bad, bins=10):\n",
    "    s = df[feature]\n",
    "\n",
    "    if s.nunique() < 2:\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        bins_series = pd.qcut(s, q=bins, duplicates=\"drop\")\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "    grouped = (\n",
    "        df.assign(bin=bins_series)\n",
    "        .groupby(\"bin\")[target]\n",
    "        .value_counts()\n",
    "        .unstack(fill_value=0)\n",
    "    )\n",
    "\n",
    "    good = grouped.get(0, 0) + 0.5\n",
    "    bad  = grouped.get(1, 0) + 0.5\n",
    "\n",
    "    dist_good = good / total_good\n",
    "    dist_bad  = bad / total_bad\n",
    "\n",
    "    iv = ((dist_bad - dist_good) * np.log(dist_bad / dist_good)).sum()\n",
    "    return iv\n",
    "\n",
    "\n",
    "def calculate_iv_categoricals(df, feature, target, total_good, total_bad):\n",
    "    s = df[feature].astype(\"str\").fillna(\"MISSING\")\n",
    "\n",
    "    grouped = (\n",
    "        df.assign(cat=s)\n",
    "        .groupby(\"cat\")[target]\n",
    "        .value_counts()\n",
    "        .unstack(fill_value=0)\n",
    "    )\n",
    "\n",
    "    good = grouped.get(0, 0) + 0.5\n",
    "    bad  = grouped.get(1, 0) + 0.5\n",
    "\n",
    "    dist_good = good / total_good\n",
    "    dist_bad  = bad / total_bad\n",
    "\n",
    "    iv = ((dist_bad - dist_good) * np.log(dist_bad / dist_good)).sum()\n",
    "    return iv\n",
    "\n",
    "\n",
    "dict_ivs = {}\n",
    "concated_data = pd.concat([X_train_final_filtered_sources, y_train_final], axis=1)\n",
    "df_iv = concated_data.copy()\n",
    "total_good = (df_iv[\"FPD\"] == 0).sum()\n",
    "total_bad  = (df_iv[\"FPD\"] == 1).sum()\n",
    "\n",
    "# Separacao das colunas numericas e categoricas apos filtragem das features por source\n",
    "num_columns = [n for n in X_train_final_filtered_sources.select_dtypes(include=[\"int32\", \"int64\", \"float32\", \"float64\"]).columns if n != \"SAFRA\"] \n",
    "cat_columns = [c for c in X_train_final_filtered_sources.select_dtypes(include=[\"object\", \"category\"]).columns if c != \"NUM_CPF\"]\n",
    "\n",
    "# IV das colunas numericas\n",
    "for column in num_columns:\n",
    "    iv = calculate_iv_numericals(df_iv, column, \"FPD\", total_good, total_bad)\n",
    "    dict_ivs[column] = iv\n",
    "\n",
    "# IV das colunas categoricas\n",
    "for column in cat_columns:\n",
    "    iv = calculate_iv_categoricals(df_iv, column, \"FPD\", total_good, total_bad)\n",
    "    dict_ivs[column] = iv\n",
    "\n",
    "# Valor de corte do IV\n",
    "iv_min = 0.02\n",
    "\n",
    "df_ivs = (\n",
    "    pd.Series(dict_ivs, name=\"IV\")\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"feature\"})\n",
    "    .sort_values(\"IV\", ascending=False)\n",
    ")\n",
    "\n",
    "df_ivs_filtered = df_ivs[df_ivs[\"IV\"] < iv_min]\n",
    "features_to_drop_iv = df_ivs_filtered[\"feature\"].unique()\n",
    "df_ivs_filtered"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "b5122d49-4a97-4110-ba49-f3ba96c14214"
  },
  {
   "cell_type": "code",
   "source": [
    "# Feature Selection - Coeficientes L1 (Regressao Logistica)\n",
    "# Treinar RL com features filtradas (sem Cadastro) para extrair coeficientes\n",
    "pipeline_coefs = update_pipeline(X_train_final_filtered_sources, name_model=\"Reg Log\")\n",
    "pipeline_coefs.fit(X_train_final_filtered_sources, y_train_final)\n",
    "\n",
    "# Extrair coeficientes do modelo RL\n",
    "coefs = pipeline_coefs.named_steps[\"model\"].coef_[0]\n",
    "feature_names_coefs = pipeline_coefs.named_steps[\"prep\"].get_feature_names_out()\n",
    "\n",
    "df_coefs = pd.DataFrame({\"feature\": feature_names_coefs, \"coef\": coefs})\n",
    "df_coefs[\"abs_coef\"] = df_coefs[\"coef\"].abs()\n",
    "df_coefs = df_coefs.sort_values(\"abs_coef\", ascending=False)\n",
    "\n",
    "# Features com coeficiente zero (removidas pelo L1)\n",
    "features_to_drop_coefs = df_coefs[df_coefs[\"abs_coef\"] == 0][\"feature\"].tolist()\n",
    "\n",
    "print(f\"Features com coeficiente L1 = 0: {len(features_to_drop_coefs)}\")\n",
    "print(f\"Features restantes: {len(feature_names_coefs) - len(features_to_drop_coefs)}\")\n",
    "df_coefs.head(20)"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "a25ee2bc-ec75-4d84-9f39-457582aaef20",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Feature Selection - Remocao de features com alta correlacao (> 0.95)\n",
    "corr_threshold = 0.95\n",
    "num_cols_for_corr = X_train_final_filtered_sources.select_dtypes(include=[\"int32\", \"int64\", \"float32\", \"float64\"]).columns.tolist()\n",
    "\n",
    "corr_matrix = X_train_final_filtered_sources[num_cols_for_corr].corr().abs()\n",
    "\n",
    "# Triangulo superior (evitar duplicatas)\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Features com correlacao acima do threshold\n",
    "features_to_drop_corrs = [column for column in upper.columns if any(upper[column] > corr_threshold)]\n",
    "\n",
    "print(f\"Features com correlacao > {corr_threshold}: {len(features_to_drop_corrs)}\")\n",
    "if features_to_drop_corrs:\n",
    "    print(f\"  Removidas: {features_to_drop_corrs[:10]}{'...' if len(features_to_drop_corrs) > 10 else ''}\")"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "624a9f62-3640-4650-b7e4-29fa9147d889",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Feature Selection - LGBM Feature Importance\n",
    "pipeline_lgbm_fi = update_pipeline(X_train_final_filtered_sources, name_model=\"LGBM\")\n",
    "pipeline_lgbm_fi.fit(X_train_final_filtered_sources, y_train_final)\n",
    "\n",
    "# Extrair feature importance\n",
    "lgbm_model_fi = pipeline_lgbm_fi.named_steps[\"model\"]\n",
    "feature_names_lgbm = pipeline_lgbm_fi.named_steps[\"prep\"].get_feature_names_out()\n",
    "\n",
    "df_lgbm_importance = pd.DataFrame({\n",
    "    \"feature\": feature_names_lgbm,\n",
    "    \"importance\": lgbm_model_fi.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Top features por importance (manter top 70 para LGBM)\n",
    "TOP_N_LGBM = 70\n",
    "top_lgbm_features = df_lgbm_importance.head(TOP_N_LGBM)[\"feature\"].tolist()\n",
    "\n",
    "print(f\"LGBM Feature Importance - Top {TOP_N_LGBM} features selecionadas\")\n",
    "print(f\"Importance cumulativa do top {TOP_N_LGBM}: {df_lgbm_importance.head(TOP_N_LGBM)['importance'].sum() / df_lgbm_importance['importance'].sum():.1%}\")\n",
    "\n",
    "# Plotar top 30\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top_30_fi = df_lgbm_importance.head(30)\n",
    "ax.barh(range(len(top_30_fi)), top_30_fi[\"importance\"].values)\n",
    "ax.set_yticks(range(len(top_30_fi)))\n",
    "ax.set_yticklabels(top_30_fi[\"feature\"].values, fontsize=8)\n",
    "ax.set_xlabel(\"Feature Importance (LGBM)\")\n",
    "ax.set_title(\"Top 30 Features - LGBM Feature Importance\")\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "bee7d6fb-03f1-4954-8547-fefdbc9f8a4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Selecao de features final - Removendo features descartadas de IV, Coefs L1 e alta correlacao\n",
    "total_features_to_drop = set(features_to_drop_iv).union(set(features_to_drop_coefs)).union(set(features_to_drop_corrs))\n",
    "# total_features_to_drop = set(features_to_drop_iv).union(set(features_to_drop_coefs))\n",
    "final_set_features = list(set(list_source_final_columns) - set(total_features_to_drop))\n",
    "\n",
    "print(f\"Drop de features IV : {len(features_to_drop_iv)}\")\n",
    "print(f\"Drop de features Coefs RegLog: {len(features_to_drop_coefs)}\")\n",
    "print(f\"Drop de features Alta Correlacao : {len(features_to_drop_corrs)}\")\n",
    "print(f\"Total de features a dropar : {len(total_features_to_drop)}\")\n",
    "print(f\"Total de colunas pos remocoes iniciais (Cadastros) : {len(list_source_final_columns)}\")\n",
    "print(f\"Total de features restantes : {len(list_source_final_columns) - len(total_features_to_drop)}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "d11c3b16-9618-4612-aa15-8e97e5649168"
  },
  {
   "cell_type": "code",
   "source": [
    "# Treinamento e avaliacao dos modelos finais com features selecionadas + MLflow logging\n",
    "dimensions = [\"NUM_CPF\", \"SAFRA\"]\n",
    "final_set_features_with_dims = final_set_features + [d for d in dimensions if d not in final_set_features]\n",
    "models = [\"Reg Log\", \"LGBM\"]\n",
    "list_dict_results_feat_selection = []\n",
    "\n",
    "# Filtragem dos dados para grupo de features atual\n",
    "X_train_final_feat_selection, X_oot_agg_feat_selection = filter_features(X_train_final, X_oot_agg, final_set_features_with_dims)\n",
    "_, X_oos_agg_feat_selection = filter_features(X_train_final, X_oos_agg, final_set_features_with_dims)\n",
    "\n",
    "best_model_name = None\n",
    "best_model_pipeline = None\n",
    "best_ks_oot = 0\n",
    "\n",
    "for model in models:\n",
    "    run_name = f\"Final_{model.replace(' ', '')}_FeatureSelection\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        # Atualizacao dos tipos de dados esperados no pipeline\n",
    "        pipeline_feat_selection = update_pipeline(X_train_final_feat_selection, name_model=model)\n",
    "\n",
    "        # Treinamento do modelo considerando lista de features filtrada\n",
    "        pipeline_feat_selection.fit(X_train_final_feat_selection, y_train_final)\n",
    "\n",
    "        # Logar parametros\n",
    "        mlflow.log_param(\"model_type\", model)\n",
    "        mlflow.log_param(\"n_features\", len(final_set_features))\n",
    "        mlflow.log_param(\"feature_selection\", \"IV + L1_coefs + high_corr\")\n",
    "        \n",
    "        model_params = pipeline_feat_selection.named_steps[\"model\"].get_params()\n",
    "        for k, v in model_params.items():\n",
    "            if isinstance(v, (int, float, str, bool)):\n",
    "                mlflow.log_param(f\"model__{k}\", v)\n",
    "\n",
    "        # Avaliacao por safra (Treino + OOS + OOT)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"MODELO FINAL: {model} ({len(final_set_features)} features)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for key, value in dict_safras.items():\n",
    "            map_step_data = generate_map_step_data(\n",
    "                X_train_final_feat_selection, y_train_final, \n",
    "                X_oos_agg_feat_selection, y_oos_agg, \n",
    "                X_oot_agg_feat_selection, y_oot_agg\n",
    "            )\n",
    "            X = map_step_data[key][\"X\"]\n",
    "            y = map_step_data[key][\"Y\"]\n",
    "            X_f, y_f = filter_xy_by_safra(X, y, dict_safras[key])\n",
    "            auc, ks = evaluation_auc_ks(X_f, y_f, pipeline_feat_selection, key, verbose=True)\n",
    "            \n",
    "            # Logar metrica no MLflow\n",
    "            safe_key = key.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "            mlflow.log_metric(f\"AUC_{safe_key}\", auc)\n",
    "            mlflow.log_metric(f\"KS_{safe_key}\", ks)\n",
    "            \n",
    "            dict_result = {\n",
    "                \"MODEL\": model,\n",
    "                \"BASE\": key,\n",
    "                \"AUC\": auc,\n",
    "                \"KS\": ks\n",
    "            }\n",
    "            list_dict_results_feat_selection.append(dict_result)\n",
    "            \n",
    "            # Rastrear melhor modelo por KS na OOT consolidada\n",
    "            if key == \"OOT GERAL (CONS)\" and ks > best_ks_oot:\n",
    "                best_ks_oot = ks\n",
    "                best_model_name = model\n",
    "                best_model_pipeline = pipeline_feat_selection\n",
    "\n",
    "        # Logar modelo\n",
    "        mlflow.sklearn.log_model(pipeline_feat_selection, f\"model_final_{model.replace(' ', '_').lower()}\")\n",
    "        \n",
    "        print(f\"\\nMLflow Run ID ({model}): {run.info.run_id}\")\n",
    "\n",
    "# Dataframe com resultados\n",
    "df_results_feat_selection = pd.DataFrame(list_dict_results_feat_selection)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"MELHOR MODELO (por KS OOT): {best_model_name} — KS OOT = {best_ks_oot:.5f}\")\n",
    "print(f\"Benchmark KS: 33.1% (0.331)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "df_results_feat_selection"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "6b20deb0-00cc-457d-8f9c-63a4c560770b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Swap-in / Swap-out"
   ],
   "metadata": {},
   "id": "047de2e3"
  },
  {
   "cell_type": "code",
   "source": [
    "# Swap-in / Swap-out Analysis\n",
    "# Compara a ordenacao de clientes entre safras OOT usando o melhor modelo\n",
    "\n",
    "def swap_analysis(df_ref, df_new, score_col=\"score\", target_col=\"FPD\", top_pct=0.1):\n",
    "    \"\"\"Calcula percentual de swap-in e swap-out entre dois rankings.\"\"\"\n",
    "    n = int(len(df_ref) * top_pct)\n",
    "    if n == 0:\n",
    "        return {\"swap_in_%\": 0, \"swap_out_%\": 0, \"n_top\": 0}\n",
    "\n",
    "    ref_top = df_ref.nlargest(n, score_col)\n",
    "    new_top = df_new.nlargest(n, score_col)\n",
    "\n",
    "    swap_in = len(set(new_top.index) - set(ref_top.index))\n",
    "    swap_out = len(set(ref_top.index) - set(new_top.index))\n",
    "\n",
    "    # Taxa de default no top (para quantificar ganho)\n",
    "    default_rate_ref = ref_top[target_col].mean() if target_col in ref_top.columns else None\n",
    "    default_rate_new = new_top[target_col].mean() if target_col in new_top.columns else None\n",
    "\n",
    "    return {\n",
    "        \"swap_in_%\": round(swap_in / n * 100, 2),\n",
    "        \"swap_out_%\": round(swap_out / n * 100, 2),\n",
    "        \"n_top\": n,\n",
    "        \"default_rate_ref\": round(default_rate_ref, 4) if default_rate_ref is not None else None,\n",
    "        \"default_rate_new\": round(default_rate_new, 4) if default_rate_new is not None else None\n",
    "    }\n",
    "\n",
    "# Usar melhor modelo para gerar scores nas bases OOT\n",
    "print(f\"Modelo utilizado: {best_model_name}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Gerar scores OOT por safra\n",
    "X_oot_feat = X_oot_agg_feat_selection.copy()\n",
    "y_oot_feat = y_oot_agg.copy()\n",
    "\n",
    "# Adicionar scores\n",
    "scores_oot = best_model_pipeline.predict_proba(X_oot_feat)[:, 1]\n",
    "df_swap = X_oot_feat[[\"SAFRA\"]].copy()\n",
    "df_swap[\"score\"] = scores_oot\n",
    "df_swap[\"FPD\"] = y_oot_feat.values\n",
    "\n",
    "safras_oot = sorted(df_swap[\"SAFRA\"].unique())\n",
    "print(f\"Safras OOT disponíveis: {safras_oot}\")\n",
    "\n",
    "# Analise swap-in/swap-out entre safras OOT\n",
    "if len(safras_oot) >= 2:\n",
    "    for top_pct in [0.05, 0.10, 0.20, 0.30]:\n",
    "        df_ref = df_swap[df_swap[\"SAFRA\"] == safras_oot[0]].reset_index(drop=True)\n",
    "        df_new = df_swap[df_swap[\"SAFRA\"] == safras_oot[1]].reset_index(drop=True)\n",
    "        \n",
    "        swap = swap_analysis(df_ref, df_new, top_pct=top_pct)\n",
    "        print(f\"\\nTop {top_pct:.0%} (n={swap['n_top']}):\")\n",
    "        print(f\"  Swap-in:  {swap['swap_in_%']:.1f}%\")\n",
    "        print(f\"  Swap-out: {swap['swap_out_%']:.1f}%\")\n",
    "        print(f\"  Default Rate OOT1 ({safras_oot[0]}): {swap['default_rate_ref']}\")\n",
    "        print(f\"  Default Rate OOT2 ({safras_oot[1]}): {swap['default_rate_new']}\")\n",
    "else:\n",
    "    print(\"Apenas 1 safra OOT disponivel - swap analysis nao aplicavel\")"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "415a4845"
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualizacoes Finais: KS Curve, Distribuicao de Scores, Confusion Matrix\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# --- 1. KS Curve (CDF) na base OOT consolidada ---\n",
    "ax1 = axes[0, 0]\n",
    "X_oot_vis = X_oot_agg_feat_selection.copy()\n",
    "y_oot_vis = y_oot_agg.values\n",
    "scores_oot_vis = best_model_pipeline.predict_proba(X_oot_vis)[:, 1]\n",
    "\n",
    "df_ks = pd.DataFrame({\"y\": y_oot_vis, \"score\": scores_oot_vis}).sort_values(\"score\")\n",
    "df_ks[\"cum_good\"] = (1 - df_ks[\"y\"]).cumsum() / (1 - df_ks[\"y\"]).sum()\n",
    "df_ks[\"cum_bad\"] = df_ks[\"y\"].cumsum() / df_ks[\"y\"].sum()\n",
    "df_ks[\"ks_diff\"] = np.abs(df_ks[\"cum_bad\"] - df_ks[\"cum_good\"])\n",
    "ks_max_idx = df_ks[\"ks_diff\"].idxmax()\n",
    "ks_max_val = df_ks.loc[ks_max_idx, \"ks_diff\"]\n",
    "ks_max_score = df_ks.loc[ks_max_idx, \"score\"]\n",
    "\n",
    "x_axis = np.linspace(0, 1, len(df_ks))\n",
    "ax1.plot(x_axis, df_ks[\"cum_good\"].values, label=\"Bons (FPD=0)\", color=\"blue\")\n",
    "ax1.plot(x_axis, df_ks[\"cum_bad\"].values, label=\"Maus (FPD=1)\", color=\"red\")\n",
    "ax1.axvline(x=x_axis[df_ks.index.get_loc(ks_max_idx)], color=\"green\", linestyle=\"--\", alpha=0.7)\n",
    "ax1.set_title(f\"KS Curve - OOT ({best_model_name}) | KS = {ks_max_val:.4f}\")\n",
    "ax1.set_xlabel(\"Populacao (%)\")\n",
    "ax1.set_ylabel(\"CDF\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# --- 2. Distribuicao de Scores por FPD ---\n",
    "ax2 = axes[0, 1]\n",
    "mask_good = y_oot_vis == 0\n",
    "mask_bad = y_oot_vis == 1\n",
    "ax2.hist(scores_oot_vis[mask_good], bins=50, alpha=0.6, label=\"Bons (FPD=0)\", color=\"blue\", density=True)\n",
    "ax2.hist(scores_oot_vis[mask_bad], bins=50, alpha=0.6, label=\"Maus (FPD=1)\", color=\"red\", density=True)\n",
    "ax2.set_title(f\"Distribuicao de Scores - OOT ({best_model_name})\")\n",
    "ax2.set_xlabel(\"Score (Probabilidade de FPD)\")\n",
    "ax2.set_ylabel(\"Densidade\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# --- 3. Confusion Matrix (threshold = 0.5) ---\n",
    "ax3 = axes[1, 0]\n",
    "y_pred_oot = (scores_oot_vis >= 0.5).astype(int)\n",
    "cm = confusion_matrix(y_oot_vis, y_pred_oot)\n",
    "im = ax3.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax3.set_title(f\"Confusion Matrix - OOT ({best_model_name})\")\n",
    "ax3.set_ylabel(\"Real\")\n",
    "ax3.set_xlabel(\"Predito\")\n",
    "ax3.set_xticks([0, 1])\n",
    "ax3.set_yticks([0, 1])\n",
    "ax3.set_xticklabels([\"Bom (0)\", \"Mau (1)\"])\n",
    "ax3.set_yticklabels([\"Bom (0)\", \"Mau (1)\"])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax3.text(j, i, f\"{cm[i, j]:,}\", ha=\"center\", va=\"center\", \n",
    "                color=\"white\" if cm[i, j] > cm.max()/2 else \"black\", fontsize=12)\n",
    "\n",
    "# --- 4. KS por Safra (barplot) ---\n",
    "ax4 = axes[1, 1]\n",
    "df_oot_results = df_results_feat_selection[\n",
    "    (df_results_feat_selection[\"MODEL\"] == best_model_name) & \n",
    "    (df_results_feat_selection[\"BASE\"].str.contains(\"OOT|OOS\"))\n",
    "]\n",
    "if not df_oot_results.empty:\n",
    "    bars = ax4.bar(range(len(df_oot_results)), df_oot_results[\"KS\"].values, color=\"steelblue\")\n",
    "    ax4.set_xticks(range(len(df_oot_results)))\n",
    "    ax4.set_xticklabels(df_oot_results[\"BASE\"].values, rotation=45, ha=\"right\", fontsize=8)\n",
    "    ax4.axhline(y=0.331, color=\"red\", linestyle=\"--\", label=\"Benchmark KS = 33.1%\")\n",
    "    ax4.set_title(f\"KS por Base - {best_model_name}\")\n",
    "    ax4.set_ylabel(\"KS\")\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    for bar, val in zip(bars, df_oot_results[\"KS\"].values):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, val + 0.005, f\"{val:.3f}\", ha=\"center\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Salvar e logar no MLflow\n",
    "fig.savefig(\"/tmp/final_model_visualizations.png\", dpi=150, bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(\"/tmp/final_model_visualizations.png\", \"plots\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nModelo final: {best_model_name}\")\n",
    "print(f\"KS OOT: {best_ks_oot:.5f}\")\n",
    "print(f\"Benchmark: 0.331\")"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "0dda1128-8400-4122-8c89-6afd29da61e7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  },
  "microsoft": {
   "language": "python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default"
  },
  "dependencies": {
   "lakehouse": {
    "known_lakehouses": [
     {
      "id": "6a7135c7-0d8d-4625-815d-c4c4a02e4ed4"
     },
     {
      "id": "5f8a4808-6f65-401b-a427-b0dd9d331b35"
     }
    ],
    "default_lakehouse": "6a7135c7-0d8d-4625-815d-c4c4a02e4ed4",
    "default_lakehouse_name": "Gold",
    "default_lakehouse_workspace_id": "febb8631-d5c0-43d8-bf08-5e89c8f2d17e"
   }
  },
  "synapse_widget": {
   "version": "0.1",
   "state": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
