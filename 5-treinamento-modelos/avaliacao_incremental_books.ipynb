{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliacao Incremental de Books — Credit Risk FPD\n",
    "\n",
    "**Objetivo**: Medir o impacto marginal de cada book (Recarga, Pagamento, Faturamento) na performance do modelo de FPD.\n",
    "\n",
    "**Metodologia**:\n",
    "- Step 1: Base (Cadastro + Telco) + Book Recarga (REC_*)\n",
    "- Step 2: Base + Recarga + Book Pagamento (PAG_*)\n",
    "- Step 3: Base + Recarga + Pagamento + Book Faturamento (FAT_*) — Full\n",
    "\n",
    "**Modelos**: Logistic Regression (L1) + LightGBM (GBDT)\n",
    "\n",
    "**Validacao**: Train / OOS (temporal) / OOT1 (202502) / OOT2 (202503)\n",
    "\n",
    "**Metricas**: KS, AUC, Gini (rank-based, threshold-independent)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# IMPORTS E CONFIGURACAO\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from datetime import date\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from category_encoders import CountEncoder\n",
    "from scipy.stats import ks_2samp\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Config centralizado\n",
    "import sys; sys.path.insert(0, \"/lakehouse/default/Files/projeto-final\")\n",
    "from config.pipeline_config import EXPERIMENT_NAME, SAFRAS\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%H:%M:%S')\n",
    "logger = logging.getLogger('incremental_eval')\n",
    "\n",
    "print('Imports OK')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# FUNCOES UTILITARIAS (reutilizadas do baseline)\n",
    "# =============================================================================\n",
    "\n",
    "def ks_stat(y_true, y_score):\n",
    "    \"\"\"Calcula estatistica KS entre positivos e negativos.\"\"\"\n",
    "    from scipy.stats import ks_2samp\n",
    "    pos_scores = y_score[y_true == 1]\n",
    "    neg_scores = y_score[y_true == 0]\n",
    "    if len(pos_scores) == 0 or len(neg_scores) == 0:\n",
    "        logger.warning('KS indefinido: pos=%d, neg=%d', len(pos_scores), len(neg_scores))\n",
    "        return np.nan\n",
    "    ks, _ = ks_2samp(pos_scores, neg_scores)\n",
    "    return ks\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_score):\n",
    "    \"\"\"Calcula metricas rank-based: KS, AUC, Gini.\n",
    "    \n",
    "    Nao inclui Precision/Recall/F1 pois dependem de threshold\n",
    "    e sao inadequados para targets desbalanceados (FPD ~5-15%).\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    ks = ks_stat(y_true, y_score)\n",
    "    gini = 2 * auc - 1\n",
    "    return {\n",
    "        'KS': round(ks, 4) if not np.isnan(ks) else 0.0,\n",
    "        'AUC': round(auc, 4),\n",
    "        'Gini': round(gini, 4),\n",
    "    }\n",
    "\n",
    "\n",
    "def filter_xy_by_safra(X, y, list_safras):\n",
    "    \"\"\"Filtra X e y por lista de SAFRAs.\"\"\"\n",
    "    mask = X['SAFRA'].isin(list_safras)\n",
    "    return X[mask].copy(), y[mask].copy()\n",
    "\n",
    "\n",
    "def split_stratified_data(df, percent=0.25, target_col='FPD'):\n",
    "    \"\"\"Split estratificado PySpark por (SAFRA, FPD).\"\"\"\n",
    "    w = Window.partitionBy('SAFRA', target_col).orderBy(F.rand(seed=42))\n",
    "    df_ranked = df.withColumn('_rank', F.percent_rank().over(w))\n",
    "    df_sample = df_ranked.filter(F.col('_rank') <= (1.0 - percent)).drop('_rank')\n",
    "    df_oos = df_ranked.filter(F.col('_rank') > (1.0 - percent)).drop('_rank')\n",
    "    return df_sample, df_oos\n",
    "\n",
    "\n",
    "print('Funcoes utilitarias carregadas')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# FUNCOES NOVAS — AVALIACAO INCREMENTAL\n",
    "# =============================================================================\n",
    "\n",
    "# Colunas que NAO sao features (metadados, targets, keys)\n",
    "NON_FEATURE_COLS = {\n",
    "    'NUM_CPF', 'SAFRA', 'FPD', 'TARGET_SCORE_01', 'TARGET_SCORE_02',\n",
    "    '_execution_id', '_data_inclusao', '_data_alteracao_silver',\n",
    "    'DT_PROCESSAMENTO', 'DATADENASCIMENTO', 'FLAG_INSTALACAO',\n",
    "}\n",
    "\n",
    "# Prefixos dos books\n",
    "BOOK_PREFIXES = ['REC_', 'PAG_', 'FAT_']\n",
    "\n",
    "\n",
    "def get_feature_groups(columns):\n",
    "    \"\"\"Separa colunas em grupos: base (cadastro+telco), REC_, PAG_, FAT_.\n",
    "    \n",
    "    Args:\n",
    "        columns: Lista de nomes de colunas.\n",
    "    \n",
    "    Returns:\n",
    "        dict com keys 'base', 'REC', 'PAG', 'FAT' e listas de colunas.\n",
    "    \"\"\"\n",
    "    groups = {'base': [], 'REC': [], 'PAG': [], 'FAT': []}\n",
    "    for col in columns:\n",
    "        if col in NON_FEATURE_COLS:\n",
    "            continue\n",
    "        if col.startswith('REC_'):\n",
    "            groups['REC'].append(col)\n",
    "        elif col.startswith('PAG_'):\n",
    "            groups['PAG'].append(col)\n",
    "        elif col.startswith('FAT_'):\n",
    "            groups['FAT'].append(col)\n",
    "        else:\n",
    "            groups['base'].append(col)\n",
    "    return groups\n",
    "\n",
    "\n",
    "def build_increment_features(feature_groups, increment_id):\n",
    "    \"\"\"Retorna lista de features para o incremento especificado.\n",
    "    \n",
    "    Incrementos:\n",
    "        1: base + REC_*\n",
    "        2: base + REC_* + PAG_*\n",
    "        3: base + REC_* + PAG_* + FAT_* (full)\n",
    "    \"\"\"\n",
    "    features = list(feature_groups['base'])\n",
    "    if increment_id >= 1:\n",
    "        features += feature_groups['REC']\n",
    "    if increment_id >= 2:\n",
    "        features += feature_groups['PAG']\n",
    "    if increment_id >= 3:\n",
    "        features += feature_groups['FAT']\n",
    "    return features\n",
    "\n",
    "\n",
    "def build_pipeline(X, model_type='LR'):\n",
    "    \"\"\"Constroi pipeline sklearn com preprocessamento + modelo.\n",
    "    \n",
    "    Args:\n",
    "        X: DataFrame de features (para detectar tipos).\n",
    "        model_type: 'LR' para Logistic Regression, 'LGBM' para LightGBM.\n",
    "    \n",
    "    Returns:\n",
    "        sklearn Pipeline configurado.\n",
    "    \"\"\"\n",
    "    num_features = [c for c in X.select_dtypes(include=['int32', 'int64', 'float32', 'float64']).columns]\n",
    "    cat_features = [c for c in X.select_dtypes(include=['object', 'category']).columns]\n",
    "    \n",
    "    num_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "    ])\n",
    "    cat_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', CountEncoder(normalize=True, handle_unknown=0, handle_missing=0)),\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', num_transformer, num_features),\n",
    "        ('cat', cat_transformer, cat_features),\n",
    "    ], remainder='drop')\n",
    "    \n",
    "    if model_type == 'LR':\n",
    "        model = LogisticRegression(\n",
    "            solver='liblinear', penalty='l1', C=0.1,\n",
    "            max_iter=2000, tol=1e-3, class_weight='balanced', random_state=42\n",
    "        )\n",
    "    else:  # LGBM\n",
    "        model = LGBMClassifier(\n",
    "            objective='binary', boosting_type='gbdt',\n",
    "            learning_rate=0.05, n_estimators=250, max_depth=7,\n",
    "            colsample_bytree=0.8, subsample=0.8,\n",
    "            random_state=42, n_jobs=-1, verbosity=-1,\n",
    "        )\n",
    "    \n",
    "    return Pipeline([('prep', preprocessor), ('model', model)])\n",
    "\n",
    "\n",
    "def train_and_evaluate_increment(\n",
    "    X_train, y_train, X_oos, y_oos, X_oot, y_oot,\n",
    "    features, increment_id, increment_name,\n",
    "    safras_oot_detail=None\n",
    "):\n",
    "    \"\"\"Treina LR + LGBM para um incremento, avalia em todos os splits, loga no MLflow.\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: Dados de treino.\n",
    "        X_oos, y_oos: Dados out-of-sample.\n",
    "        X_oot, y_oot: Dados out-of-time.\n",
    "        features: Lista de features para este incremento.\n",
    "        increment_id: 1, 2 ou 3.\n",
    "        increment_name: Nome descritivo do incremento.\n",
    "        safras_oot_detail: Dict {safra: (X, y)} para avaliacao por safra OOT.\n",
    "    \n",
    "    Returns:\n",
    "        list[dict]: Resultados com metricas por modelo e split.\n",
    "    \"\"\"\n",
    "    # Filtrar features\n",
    "    keep_cols = [c for c in features if c in X_train.columns]\n",
    "    # reset_index para garantir alinhamento posicional entre X e y\n",
    "    X_tr = X_train[keep_cols].reset_index(drop=True)\n",
    "    y_tr = y_train.reset_index(drop=True)\n",
    "    X_os = X_oos[keep_cols].reset_index(drop=True)\n",
    "    y_os = y_oos.reset_index(drop=True)\n",
    "    X_ot = X_oot[keep_cols].reset_index(drop=True)\n",
    "    y_ot = y_oot.reset_index(drop=True)\n",
    "    \n",
    "    results = []\n",
    "    trained_models = {}\n",
    "    \n",
    "    for model_type in ['LR', 'LGBM']:\n",
    "        model_label = f'LR_L1' if model_type == 'LR' else 'LGBM'\n",
    "        run_name = f'{model_label}_increment{increment_id}'\n",
    "        \n",
    "        with mlflow.start_run(run_name=run_name, nested=True):\n",
    "            mlflow.set_tags({\n",
    "                'model_type': model_label,\n",
    "                'increment_id': str(increment_id),\n",
    "                'increment_name': increment_name,\n",
    "                'n_features': str(len(keep_cols)),\n",
    "            })\n",
    "            mlflow.log_param('n_features', len(keep_cols))\n",
    "            mlflow.log_param('increment_name', increment_name)\n",
    "            mlflow.log_param('features', str(keep_cols[:20]) + '...' if len(keep_cols) > 20 else str(keep_cols))\n",
    "            \n",
    "            # Treinar\n",
    "            pipe = build_pipeline(X_tr, model_type)\n",
    "            pipe.fit(X_tr, y_tr)\n",
    "            trained_models[model_type] = pipe\n",
    "            \n",
    "            # Avaliar em cada split\n",
    "            for split_name, X_eval, y_eval in [\n",
    "                ('Train', X_tr, y_tr),\n",
    "                ('OOS', X_os, y_os),\n",
    "                ('OOT', X_ot, y_ot),\n",
    "            ]:\n",
    "                scores = pipe.predict_proba(X_eval)[:, 1]\n",
    "                metrics = compute_metrics(y_eval, scores)\n",
    "                \n",
    "                for metric_name, metric_val in metrics.items():\n",
    "                    mlflow.log_metric(f'{split_name}_{metric_name}', metric_val)\n",
    "                \n",
    "                results.append({\n",
    "                    'Increment': increment_id,\n",
    "                    'Increment_Name': increment_name,\n",
    "                    'N_Features': len(keep_cols),\n",
    "                    'Model': model_label,\n",
    "                    'Split': split_name,\n",
    "                    **metrics,\n",
    "                })\n",
    "            \n",
    "            # Avaliar por safra OOT individual\n",
    "            if safras_oot_detail:\n",
    "                for safra_name, (X_s, y_s) in safras_oot_detail.items():\n",
    "                    X_s_inc = X_s[keep_cols].reset_index(drop=True)\n",
    "                    y_s_inc = y_s.reset_index(drop=True)\n",
    "                    scores_s = pipe.predict_proba(X_s_inc)[:, 1]\n",
    "                    metrics_s = compute_metrics(y_s_inc, scores_s)\n",
    "                    for metric_name, metric_val in metrics_s.items():\n",
    "                        mlflow.log_metric(f'OOT_{safra_name}_{metric_name}', metric_val)\n",
    "                    results.append({\n",
    "                        'Increment': increment_id,\n",
    "                        'Increment_Name': increment_name,\n",
    "                        'N_Features': len(keep_cols),\n",
    "                        'Model': model_label,\n",
    "                        'Split': f'OOT_{safra_name}',\n",
    "                        **metrics_s,\n",
    "                    })\n",
    "            \n",
    "            # Log modelo\n",
    "            mlflow.sklearn.log_model(pipe, f'model_{model_label}_inc{increment_id}')\n",
    "    \n",
    "    return results, trained_models\n",
    "\n",
    "\n",
    "print('Funcoes de avaliacao incremental carregadas')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# MLFLOW SETUP\n",
    "# =============================================================================\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "# Desabilitar autolog — controle manual evita duplicacao de metricas/params\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "print(f'MLflow experiment: {EXPERIMENT_NAME}')\n",
    "print(f'Tracking URI: {mlflow.get_tracking_uri()}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# CARREGAR DADOS DO GOLD FEATURE STORE\n",
    "# =============================================================================\n",
    "logger.info('Carregando feature store...')\n",
    "\n",
    "df_spark = spark.sql('SELECT * FROM Gold.feature_store.clientes_consolidado')\n",
    "\n",
    "# Filtrar apenas clientes com FLAG_INSTALACAO = 1 (clientes ativos)\n",
    "df_spark_pos = df_spark.filter(F.col('FLAG_INSTALACAO') == 1)\n",
    "\n",
    "# Remover colunas com > 75% missing\n",
    "total = df_spark_pos.count()\n",
    "cols_to_keep = []\n",
    "for c in df_spark_pos.columns:\n",
    "    null_pct = df_spark_pos.filter(F.col(c).isNull()).count() / total\n",
    "    if null_pct <= 0.75:\n",
    "        cols_to_keep.append(c)\n",
    "\n",
    "df_spark_clean = df_spark_pos.select(cols_to_keep)\n",
    "\n",
    "logger.info('Feature store: %d registros, %d colunas (apos filtro missing)', total, len(cols_to_keep))\n",
    "print(f'Registros: {total:,}')\n",
    "print(f'Colunas: {len(cols_to_keep)}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SPLIT TEMPORAL: SAMPLE (TRAIN+VAL) / OOS / OOT\n",
    "# =============================================================================\n",
    "safras_train_oos = [202410, 202411, 202412, 202501]\n",
    "safras_oot = [202502, 202503]\n",
    "\n",
    "# Separar OOT (temporal holdout)\n",
    "df_train_pool = df_spark_clean.filter(F.col('SAFRA').isin(safras_train_oos))\n",
    "df_oot_spark = df_spark_clean.filter(F.col('SAFRA').isin(safras_oot))\n",
    "\n",
    "# Split estratificado train/OOS (75/25)\n",
    "df_sample_spark, df_oos_spark = split_stratified_data(df_train_pool, percent=0.25)\n",
    "\n",
    "logger.info('Convertendo para Pandas...')\n",
    "df_sample = df_sample_spark.toPandas()\n",
    "df_oos = df_oos_spark.toPandas()\n",
    "df_oot = df_oot_spark.toPandas()\n",
    "\n",
    "logger.info('Sample: %d rows, OOS: %d rows, OOT: %d rows', len(df_sample), len(df_oos), len(df_oot))\n",
    "print(f'Sample (Train): {df_sample.shape}')\n",
    "print(f'OOS: {df_oos.shape}')\n",
    "print(f'OOT: {df_oot.shape}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# PREPARAR SPLITS X/Y\n",
    "# =============================================================================\n",
    "target = 'FPD'\n",
    "\n",
    "# Drop duplicates e NaN no target\n",
    "df_sample = df_sample.drop_duplicates(subset=['NUM_CPF', 'SAFRA']).dropna(subset=[target])\n",
    "df_oos = df_oos.drop_duplicates(subset=['NUM_CPF', 'SAFRA']).dropna(subset=[target])\n",
    "df_oot = df_oot.drop_duplicates(subset=['NUM_CPF', 'SAFRA']).dropna(subset=[target])\n",
    "\n",
    "# Separar X / y — SAFRA excluida para evitar leakage temporal (C1 fix)\n",
    "drop_cols = ['NUM_CPF', 'SAFRA', 'FPD', 'TARGET_SCORE_01', 'TARGET_SCORE_02',\n",
    "             '_execution_id', '_data_inclusao', '_data_alteracao_silver',\n",
    "             'DT_PROCESSAMENTO', 'DATADENASCIMENTO', 'FLAG_INSTALACAO']\n",
    "\n",
    "feature_cols = [c for c in df_sample.columns if c not in drop_cols]\n",
    "\n",
    "X_train = df_sample[feature_cols].copy()\n",
    "y_train = df_sample[target].astype(int).copy()\n",
    "\n",
    "X_oos = df_oos[feature_cols].copy()\n",
    "y_oos = df_oos[target].astype(int).copy()\n",
    "\n",
    "X_oot = df_oot[feature_cols].copy()\n",
    "y_oot = df_oot[target].astype(int).copy()\n",
    "\n",
    "# OOT por safra individual (usar SAFRA do df original, nao do X filtrado)\n",
    "safras_oot_detail = {}\n",
    "for safra in safras_oot:\n",
    "    mask = df_oot['SAFRA'] == safra\n",
    "    safras_oot_detail[str(safra)] = (X_oot[mask].copy(), y_oot[mask].copy())\n",
    "\n",
    "print(f'X_train: {X_train.shape}, FPD rate: {y_train.mean():.4f}')\n",
    "print(f'X_oos:   {X_oos.shape}, FPD rate: {y_oos.mean():.4f}')\n",
    "print(f'X_oot:   {X_oot.shape}, FPD rate: {y_oot.mean():.4f}')\n",
    "for s, (x, y) in safras_oot_detail.items():\n",
    "    print(f'  OOT {s}: {x.shape[0]:,} rows, FPD rate: {y.mean():.4f}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SEPARAR FEATURES POR PREFIXO (BOOK)\n",
    "# =============================================================================\n",
    "feature_groups = get_feature_groups(feature_cols)\n",
    "\n",
    "print('Feature Groups:')\n",
    "for group, cols in feature_groups.items():\n",
    "    print(f'  {group}: {len(cols)} features')\n",
    "    if len(cols) <= 10:\n",
    "        print(f'    {cols}')\n",
    "    else:\n",
    "        print(f'    {cols[:5]} ... {cols[-3:]}')\n",
    "\n",
    "print(f'\\nTotal features disponiveis: {sum(len(v) for v in feature_groups.values())}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# DEFINIR INCREMENTOS\n",
    "# =============================================================================\n",
    "INCREMENTS = [\n",
    "    {'id': 1, 'name': 'Base + Recarga',           'books': ['REC']},\n",
    "    {'id': 2, 'name': 'Base + Recarga + Pagamento', 'books': ['REC', 'PAG']},\n",
    "    {'id': 3, 'name': 'Full (+ Faturamento)',       'books': ['REC', 'PAG', 'FAT']},\n",
    "]\n",
    "\n",
    "for inc in INCREMENTS:\n",
    "    features = build_increment_features(feature_groups, inc['id'])\n",
    "    print(f\"Step {inc['id']}: {inc['name']} — {len(features)} features\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# LOOP PRINCIPAL: TREINAR E AVALIAR CADA INCREMENTO\n",
    "# =============================================================================\n",
    "all_results = []\n",
    "all_models = {}\n",
    "\n",
    "with mlflow.start_run(run_name='Incremental_Book_Evaluation') as parent_run:\n",
    "    mlflow.set_tag('evaluation_type', 'incremental_books')\n",
    "    mlflow.log_param('n_increments', len(INCREMENTS))\n",
    "    mlflow.log_param('safras_train', str(safras_train_oos))\n",
    "    mlflow.log_param('safras_oot', str(safras_oot))\n",
    "    \n",
    "    for inc in INCREMENTS:\n",
    "        logger.info('='*60)\n",
    "        logger.info('INCREMENTO %d: %s', inc['id'], inc['name'])\n",
    "        logger.info('='*60)\n",
    "        \n",
    "        features = build_increment_features(feature_groups, inc['id'])\n",
    "        logger.info('Features: %d', len(features))\n",
    "        \n",
    "        results, models = train_and_evaluate_increment(\n",
    "            X_train, y_train,\n",
    "            X_oos, y_oos,\n",
    "            X_oot, y_oot,\n",
    "            features=features,\n",
    "            increment_id=inc['id'],\n",
    "            increment_name=inc['name'],\n",
    "            safras_oot_detail=safras_oot_detail,\n",
    "        )\n",
    "        \n",
    "        all_results.extend(results)\n",
    "        all_models[inc['id']] = models\n",
    "        \n",
    "        # Log resumo do incremento no parent run\n",
    "        for r in results:\n",
    "            if r['Split'] == 'OOT':\n",
    "                mlflow.log_metric(f\"inc{inc['id']}_{r['Model']}_KS_OOT\", r['KS'])\n",
    "                mlflow.log_metric(f\"inc{inc['id']}_{r['Model']}_AUC_OOT\", r['AUC'])\n",
    "    \n",
    "    # Log parent run metadata\n",
    "    mlflow.log_param('parent_run_id', parent_run.info.run_id)\n",
    "\n",
    "logger.info('Loop de avaliacao incremental concluido — %d resultados', len(all_results))\n",
    "print(f'Total de resultados: {len(all_results)}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# CONSOLIDAR RESULTADOS EM DATAFRAME\n",
    "# =============================================================================\n",
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('TABELA COMPLETA DE RESULTADOS')\n",
    "print('='*80)\n",
    "display(df_results.to_string(index=False))\n",
    "\n",
    "# Pivot: KS por Incremento x Modelo x Split\n",
    "df_ks_pivot = df_results.pivot_table(\n",
    "    values='KS', index=['Increment', 'Increment_Name', 'N_Features'],\n",
    "    columns=['Model', 'Split'], aggfunc='first'\n",
    ")\n",
    "print('\\n' + '='*80)\n",
    "print('TABELA KS (Incremento x Modelo x Split)')\n",
    "print('='*80)\n",
    "display(df_ks_pivot)\n",
    "\n",
    "# Pivot: AUC por Incremento x Modelo x Split\n",
    "df_auc_pivot = df_results.pivot_table(\n",
    "    values='AUC', index=['Increment', 'Increment_Name', 'N_Features'],\n",
    "    columns=['Model', 'Split'], aggfunc='first'\n",
    ")\n",
    "print('\\n' + '='*80)\n",
    "print('TABELA AUC (Incremento x Modelo x Split)')\n",
    "print('='*80)\n",
    "display(df_auc_pivot)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados — Contribuicao Marginal"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# CONTRIBUICAO MARGINAL (DELTA ENTRE INCREMENTOS)\n",
    "# =============================================================================\n",
    "marginal_rows = []\n",
    "\n",
    "for model in ['LR_L1', 'LGBM']:\n",
    "    for split in ['Train', 'OOS', 'OOT']:\n",
    "        prev_ks = None\n",
    "        prev_auc = None\n",
    "        for inc in INCREMENTS:\n",
    "            row = df_results[\n",
    "                (df_results['Increment'] == inc['id']) &\n",
    "                (df_results['Model'] == model) &\n",
    "                (df_results['Split'] == split)\n",
    "            ]\n",
    "            if row.empty:\n",
    "                continue\n",
    "            ks = row.iloc[0]['KS']\n",
    "            auc = row.iloc[0]['AUC']\n",
    "            gini = row.iloc[0]['Gini']\n",
    "            delta_ks = round(ks - prev_ks, 4) if prev_ks is not None else None\n",
    "            delta_auc = round(auc - prev_auc, 4) if prev_auc is not None else None\n",
    "            marginal_rows.append({\n",
    "                'Increment': inc['id'],\n",
    "                'Name': inc['name'],\n",
    "                'Model': model,\n",
    "                'Split': split,\n",
    "                'KS': ks,\n",
    "                'AUC': auc,\n",
    "                'Gini': gini,\n",
    "                'Delta_KS': delta_ks,\n",
    "                'Delta_AUC': delta_auc,\n",
    "            })\n",
    "            prev_ks = ks\n",
    "            prev_auc = auc\n",
    "\n",
    "df_marginal = pd.DataFrame(marginal_rows)\n",
    "\n",
    "print('='*80)\n",
    "print('CONTRIBUICAO MARGINAL POR BOOK')\n",
    "print('='*80)\n",
    "display(df_marginal.to_string(index=False))\n",
    "\n",
    "# Resumo executivo\n",
    "print('\\n--- RESUMO EXECUTIVO ---')\n",
    "for model in ['LR_L1', 'LGBM']:\n",
    "    print(f'\\nModelo: {model}')\n",
    "    for split in ['OOS', 'OOT']:\n",
    "        subset = df_marginal[(df_marginal['Model'] == model) & (df_marginal['Split'] == split)]\n",
    "        for _, r in subset.iterrows():\n",
    "            delta = f\"(delta KS: {r['Delta_KS']:+.4f})\" if r['Delta_KS'] is not None else '(baseline)'\n",
    "            print(f\"  Step {r['Increment']} [{split}]: KS={r['KS']:.4f} {delta}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# VISUALIZACAO 1: KS EVOLUTION POR INCREMENTO\n",
    "# =============================================================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "fig.suptitle('KS por Split — Evolucao Incremental de Books', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, split in enumerate(['Train', 'OOS', 'OOT']):\n",
    "    ax = axes[idx]\n",
    "    for model in ['LR_L1', 'LGBM']:\n",
    "        subset = df_results[(df_results['Model'] == model) & (df_results['Split'] == split)]\n",
    "        if not subset.empty:\n",
    "            ax.plot(\n",
    "                subset['Increment'].values,\n",
    "                subset['KS'].values,\n",
    "                marker='o', linewidth=2, markersize=8,\n",
    "                label=model,\n",
    "            )\n",
    "            # Anotar valores\n",
    "            for _, r in subset.iterrows():\n",
    "                ax.annotate(f\"{r['KS']:.3f}\", (r['Increment'], r['KS']),\n",
    "                           textcoords='offset points', xytext=(0, 10), ha='center', fontsize=9)\n",
    "    \n",
    "    ax.set_title(f'{split}', fontsize=12)\n",
    "    ax.set_xlabel('Incremento')\n",
    "    ax.set_xticks([1, 2, 3])\n",
    "    ax.set_xticklabels(['Base+REC', '+PAG', '+FAT'], rotation=15)\n",
    "    if idx == 0:\n",
    "        ax.set_ylabel('KS')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/fig_incremental_ks_evolution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Grafico KS salvo')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# VISUALIZACAO 2: AUC PROGRESSION (BAR CHART COMPARATIVO)\n",
    "# =============================================================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "fig.suptitle('AUC por Split — Evolucao Incremental de Books', fontsize=14, fontweight='bold')\n",
    "\n",
    "colors = {'LR_L1': '#2196F3', 'LGBM': '#4CAF50'}\n",
    "bar_width = 0.35\n",
    "\n",
    "for idx, split in enumerate(['Train', 'OOS', 'OOT']):\n",
    "    ax = axes[idx]\n",
    "    for j, model in enumerate(['LR_L1', 'LGBM']):\n",
    "        subset = df_results[(df_results['Model'] == model) & (df_results['Split'] == split)]\n",
    "        if not subset.empty:\n",
    "            x = np.arange(len(subset))\n",
    "            bars = ax.bar(\n",
    "                x + j * bar_width, subset['AUC'].values,\n",
    "                bar_width, label=model, color=colors[model], alpha=0.85\n",
    "            )\n",
    "            for bar, val in zip(bars, subset['AUC'].values):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.003,\n",
    "                       f'{val:.3f}', ha='center', fontsize=8)\n",
    "    \n",
    "    ax.set_title(f'{split}', fontsize=12)\n",
    "    ax.set_xticks(np.arange(3) + bar_width / 2)\n",
    "    ax.set_xticklabels(['Base+REC', '+PAG', '+FAT'], rotation=15)\n",
    "    if idx == 0:\n",
    "        ax.set_ylabel('AUC')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/fig_incremental_auc_progression.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Grafico AUC salvo')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# VISUALIZACAO 3: CONTRIBUICAO MARGINAL (WATERFALL / BAR DELTA)\n",
    "# =============================================================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Contribuicao Marginal por Book (Delta KS)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, model in enumerate(['LR_L1', 'LGBM']):\n",
    "    ax = axes[idx]\n",
    "    subset = df_marginal[\n",
    "        (df_marginal['Model'] == model) &\n",
    "        (df_marginal['Split'] == 'OOT') &\n",
    "        (df_marginal['Delta_KS'].notna())\n",
    "    ]\n",
    "    if not subset.empty:\n",
    "        colors_bar = ['#4CAF50' if d > 0 else '#F44336' for d in subset['Delta_KS']]\n",
    "        bars = ax.bar(\n",
    "            range(len(subset)), subset['Delta_KS'].values,\n",
    "            color=colors_bar, alpha=0.85, edgecolor='black', linewidth=0.5\n",
    "        )\n",
    "        for bar, val in zip(bars, subset['Delta_KS'].values):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2,\n",
    "                   bar.get_height() + 0.001 * (1 if val >= 0 else -3),\n",
    "                   f'{val:+.4f}', ha='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_title(f'{model} — OOT', fontsize=12)\n",
    "    ax.set_xticks(range(len(subset)))\n",
    "    ax.set_xticklabels(['+Pagamento', '+Faturamento'], rotation=0)\n",
    "    ax.set_ylabel('Delta KS')\n",
    "    ax.axhline(y=0, color='black', linewidth=0.8)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/fig_incremental_marginal_delta.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Grafico marginal delta salvo')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# VISUALIZACAO 4: FEATURE IMPORTANCE TOP-15 POR INCREMENTO (LGBM)\n",
    "# =============================================================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "fig.suptitle('Top 15 Features por Incremento (LGBM Importance)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, inc in enumerate(INCREMENTS):\n",
    "    ax = axes[idx]\n",
    "    lgbm_model = all_models[inc['id']].get('LGBM')\n",
    "    if lgbm_model is None:\n",
    "        ax.set_title(f\"Step {inc['id']}: N/A\")\n",
    "        continue\n",
    "    \n",
    "    # Extrair importancias\n",
    "    booster = lgbm_model.named_steps['model']\n",
    "    prep = lgbm_model.named_steps['prep']\n",
    "    try:\n",
    "        feature_names = prep.get_feature_names_out()\n",
    "    except Exception:\n",
    "        features_inc = build_increment_features(feature_groups, inc['id'])\n",
    "        feature_names = [c for c in features_inc if c != 'SAFRA']\n",
    "    \n",
    "    importances = booster.feature_importances_\n",
    "    n = min(len(feature_names), len(importances))\n",
    "    df_imp = pd.DataFrame({\n",
    "        'feature': list(feature_names)[:n],\n",
    "        'importance': list(importances)[:n]\n",
    "    }).sort_values('importance', ascending=True).tail(15)\n",
    "    \n",
    "    # Colorir por source\n",
    "    colors_fi = []\n",
    "    for f in df_imp['feature']:\n",
    "        if 'REC_' in f or f.startswith('num__REC_') or f.startswith('cat__REC_'):\n",
    "            colors_fi.append('#2196F3')  # Azul\n",
    "        elif 'PAG_' in f or f.startswith('num__PAG_') or f.startswith('cat__PAG_'):\n",
    "            colors_fi.append('#FF9800')  # Laranja\n",
    "        elif 'FAT_' in f or f.startswith('num__FAT_') or f.startswith('cat__FAT_'):\n",
    "            colors_fi.append('#9C27B0')  # Roxo\n",
    "        else:\n",
    "            colors_fi.append('#607D8B')  # Cinza (base)\n",
    "    \n",
    "    ax.barh(range(len(df_imp)), df_imp['importance'].values, color=colors_fi, alpha=0.85)\n",
    "    ax.set_yticks(range(len(df_imp)))\n",
    "    ax.set_yticklabels(df_imp['feature'].values, fontsize=7)\n",
    "    ax.set_title(f\"Step {inc['id']}: {inc['name']}\\n({len(build_increment_features(feature_groups, inc['id']))} features)\", fontsize=10)\n",
    "    ax.set_xlabel('Importance')\n",
    "\n",
    "# Legenda\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#607D8B', label='Base (Cadastro+Telco)'),\n",
    "    Patch(facecolor='#2196F3', label='Recarga (REC_)'),\n",
    "    Patch(facecolor='#FF9800', label='Pagamento (PAG_)'),\n",
    "    Patch(facecolor='#9C27B0', label='Faturamento (FAT_)'),\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='lower center', ncol=4, fontsize=10)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "plt.savefig('/tmp/fig_incremental_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Grafico feature importance salvo')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SWAP ANALYSIS POR INCREMENTO\n",
    "# =============================================================================\n",
    "\n",
    "def swap_analysis_incremental(y_true_1, scores_1, y_true_2, scores_2, top_pct=0.1):\n",
    "    \"\"\"Compara ranking entre dois conjuntos (OOT1 vs OOT2) para estabilidade.\n",
    "    \n",
    "    Mede se os clientes classificados como alto risco em OOT1 continuam\n",
    "    sendo alto risco em OOT2 (swap-in/swap-out).\n",
    "    \"\"\"\n",
    "    # Reset index para garantir alinhamento posicional com np.argsort\n",
    "    y1 = y_true_1.reset_index(drop=True)\n",
    "    y2 = y_true_2.reset_index(drop=True)\n",
    "    s1 = np.array(scores_1)\n",
    "    s2 = np.array(scores_2)\n",
    "\n",
    "    n1 = int(len(s1) * top_pct)\n",
    "    n2 = int(len(s2) * top_pct)\n",
    "    \n",
    "    # Top risco em cada periodo\n",
    "    top_idx_1 = np.argsort(s1)[-n1:]\n",
    "    top_idx_2 = np.argsort(s2)[-n2:]\n",
    "    \n",
    "    # FPD rate no top\n",
    "    fpd_rate_top_1 = y1.iloc[top_idx_1].mean() if len(top_idx_1) > 0 else 0\n",
    "    fpd_rate_top_2 = y2.iloc[top_idx_2].mean() if len(top_idx_2) > 0 else 0\n",
    "    \n",
    "    # Metricas gerais\n",
    "    metrics_1 = compute_metrics(y1, s1)\n",
    "    metrics_2 = compute_metrics(y2, s2)\n",
    "    \n",
    "    return {\n",
    "        'KS_OOT1': metrics_1['KS'],\n",
    "        'KS_OOT2': metrics_2['KS'],\n",
    "        'Delta_KS': round(metrics_2['KS'] - metrics_1['KS'], 4),\n",
    "        'AUC_OOT1': metrics_1['AUC'],\n",
    "        'AUC_OOT2': metrics_2['AUC'],\n",
    "        'Delta_AUC': round(metrics_2['AUC'] - metrics_1['AUC'], 4),\n",
    "        'FPD_Rate_Top10_OOT1': round(fpd_rate_top_1, 4),\n",
    "        'FPD_Rate_Top10_OOT2': round(fpd_rate_top_2, 4),\n",
    "        'Delta_FPD_Top10': round(fpd_rate_top_2 - fpd_rate_top_1, 4),\n",
    "    }\n",
    "\n",
    "\n",
    "# Executar swap para cada incremento\n",
    "swap_results = []\n",
    "\n",
    "for inc in INCREMENTS:\n",
    "    features = build_increment_features(feature_groups, inc['id'])\n",
    "    keep_cols = [c for c in features if c in X_oot.columns]\n",
    "    \n",
    "    for model_type in ['LR', 'LGBM']:\n",
    "        model_label = 'LR_L1' if model_type == 'LR' else 'LGBM'\n",
    "        pipe = all_models[inc['id']][model_type]\n",
    "        \n",
    "        # Scores por safra OOT\n",
    "        X_oot1, y_oot1 = safras_oot_detail['202502']\n",
    "        X_oot2, y_oot2 = safras_oot_detail['202503']\n",
    "        \n",
    "        scores_oot1 = pipe.predict_proba(X_oot1[keep_cols].reset_index(drop=True))[:, 1]\n",
    "        scores_oot2 = pipe.predict_proba(X_oot2[keep_cols].reset_index(drop=True))[:, 1]\n",
    "        \n",
    "        swap = swap_analysis_incremental(y_oot1, scores_oot1, y_oot2, scores_oot2)\n",
    "        swap_results.append({\n",
    "            'Increment': inc['id'],\n",
    "            'Name': inc['name'],\n",
    "            'Model': model_label,\n",
    "            **swap\n",
    "        })\n",
    "\n",
    "df_swap = pd.DataFrame(swap_results)\n",
    "\n",
    "print('='*80)\n",
    "print('SWAP ANALYSIS — ESTABILIDADE TEMPORAL (OOT1 vs OOT2)')\n",
    "print('='*80)\n",
    "display(df_swap.to_string(index=False))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# VISUALIZACAO 5: SWAP COMPARISON\n",
    "# =============================================================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Swap Analysis — Estabilidade OOT1 vs OOT2', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, metric in enumerate(['Delta_KS', 'Delta_FPD_Top10']):\n",
    "    ax = axes[idx]\n",
    "    for j, model in enumerate(['LR_L1', 'LGBM']):\n",
    "        subset = df_swap[df_swap['Model'] == model]\n",
    "        x = np.arange(len(subset))\n",
    "        bars = ax.bar(\n",
    "            x + j * 0.35, subset[metric].values,\n",
    "            0.35, label=model, alpha=0.85\n",
    "        )\n",
    "        for bar, val in zip(bars, subset[metric].values):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                   f'{val:+.4f}', ha='center', fontsize=9)\n",
    "    \n",
    "    ax.set_title(metric.replace('_', ' '), fontsize=12)\n",
    "    ax.set_xticks(np.arange(3) + 0.175)\n",
    "    ax.set_xticklabels(['Base+REC', '+PAG', '+FAT'], rotation=0)\n",
    "    ax.axhline(y=0, color='black', linewidth=0.8)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/fig_incremental_swap_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Grafico swap salvo')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# EXPORT — CSVs E SUMMARY\n",
    "# =============================================================================\n",
    "import os\n",
    "artifacts_dir = '/tmp/incremental_eval'\n",
    "os.makedirs(artifacts_dir, exist_ok=True)\n",
    "\n",
    "# CSVs\n",
    "df_results.to_csv(f'{artifacts_dir}/incremental_evaluation_full_results.csv', index=False)\n",
    "df_marginal.to_csv(f'{artifacts_dir}/incremental_marginal_contribution.csv', index=False)\n",
    "df_swap.to_csv(f'{artifacts_dir}/swap_analysis_consolidated.csv', index=False)\n",
    "df_ks_pivot.to_csv(f'{artifacts_dir}/incremental_comparison_ks.csv')\n",
    "df_auc_pivot.to_csv(f'{artifacts_dir}/incremental_comparison_auc.csv')\n",
    "\n",
    "# Log artefatos no MLflow\n",
    "with mlflow.start_run(run_name='Incremental_Summary_Artifacts', nested=False):\n",
    "    mlflow.set_tag('artifact_type', 'incremental_evaluation_summary')\n",
    "    for fname in os.listdir(artifacts_dir):\n",
    "        mlflow.log_artifact(f'{artifacts_dir}/{fname}', 'incremental_evaluation')\n",
    "    \n",
    "    # Log figuras\n",
    "    for fig_path in [\n",
    "        '/tmp/fig_incremental_ks_evolution.png',\n",
    "        '/tmp/fig_incremental_auc_progression.png',\n",
    "        '/tmp/fig_incremental_marginal_delta.png',\n",
    "        '/tmp/fig_incremental_feature_importance.png',\n",
    "        '/tmp/fig_incremental_swap_comparison.png',\n",
    "    ]:\n",
    "        if os.path.exists(fig_path):\n",
    "            mlflow.log_artifact(fig_path, 'figures')\n",
    "\n",
    "print(f'Artefatos exportados para: {artifacts_dir}')\n",
    "print(f'CSVs: {len([f for f in os.listdir(artifacts_dir) if f.endswith(\".csv\")])}')\n",
    "print('MLflow artifacts logged')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# HEATMAP RESUMO FINAL\n",
    "# =============================================================================\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "fig.suptitle('Heatmap — KS por Incremento, Modelo e Split', fontsize=14, fontweight='bold')\n",
    "\n",
    "heatmap_data = df_results.pivot_table(\n",
    "    values='KS',\n",
    "    index=['Increment', 'Model'],\n",
    "    columns='Split',\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "# Ordenar colunas\n",
    "col_order = ['Train', 'OOS', 'OOT']\n",
    "heatmap_data = heatmap_data[[c for c in col_order if c in heatmap_data.columns]]\n",
    "\n",
    "im = ax.imshow(heatmap_data.values, cmap='RdYlGn', aspect='auto', vmin=0.15, vmax=0.45)\n",
    "\n",
    "ax.set_xticks(range(len(heatmap_data.columns)))\n",
    "ax.set_xticklabels(heatmap_data.columns, fontsize=11)\n",
    "ax.set_yticks(range(len(heatmap_data.index)))\n",
    "ax.set_yticklabels([f'Step {i} — {m}' for i, m in heatmap_data.index], fontsize=10)\n",
    "\n",
    "# Anotar valores\n",
    "for i in range(len(heatmap_data.index)):\n",
    "    for j in range(len(heatmap_data.columns)):\n",
    "        val = heatmap_data.values[i, j]\n",
    "        ax.text(j, i, f'{val:.3f}', ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='KS')\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/fig_incremental_summary_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Heatmap resumo salvo')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusoes\n",
    "\n",
    "### Interpretacao dos Resultados\n",
    "\n",
    "| Incremento | O que avalia |\n",
    "|------------|-------------|\n",
    "| Step 1 (Base + Recarga) | Poder preditivo do comportamento de recarga isolado |\n",
    "| Step 2 (+ Pagamento) | Ganho marginal do historico de pagamentos |\n",
    "| Step 3 (+ Faturamento) | Ganho marginal do perfil de faturamento |\n",
    "\n",
    "### Criterios de Decisao\n",
    "\n",
    "- **Delta KS > +0.02**: Book contribui significativamente\n",
    "- **Delta KS entre -0.01 e +0.02**: Contribuicao marginal (considerar complexidade vs ganho)\n",
    "- **Delta KS < -0.01**: Book pode estar adicionando ruido (investigar)\n",
    "- **Delta FPD Top10 < |0.02|**: Modelo estavel temporalmente (swap aceitavel)\n",
    "\n",
    "### Proximos Passos\n",
    "\n",
    "1. Se algum book nao contribui, considerar remove-lo para simplicidade\n",
    "2. Aplicar feature selection (IV + L1) no melhor incremento\n",
    "3. Treinar modelo final com features selecionadas\n",
    "4. Registrar modelo no MLflow Model Registry"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  },
  "microsoft": {
   "language": "python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
