{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Avaliacao Incremental de Books — Credit Risk FPD\n\n**Objetivo**: Medir o impacto marginal de cada book (Recarga, Pagamento, Faturamento) na performance do modelo de FPD.\n\n**Metodologia**:\n- Step 1: Base (Cadastro + Telco) + Book Recarga (REC_*)\n- Step 2: Base + Recarga + Book Pagamento (PAG_*)\n- Step 3: Base + Recarga + Pagamento + Book Faturamento (FAT_*) — Full\n\n**Modelos**: Logistic Regression (L1) + LightGBM (GBDT)\n\n**Validacao**: Train / OOS (temporal) / OOT1 (202502) / OOT2 (202503)\n\n**Metricas**: KS, AUC, Gini, Precision, Recall, F1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# IMPORTS E CONFIGURACAO\n# =============================================================================\nimport pandas as pd\nimport numpy as np\nimport mlflow\nimport mlflow.sklearn\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport warnings\nfrom datetime import date\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom lightgbm import LGBMClassifier\nfrom category_encoders import CountEncoder\nfrom scipy.stats import ks_2samp\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import Window\n\nwarnings.filterwarnings('ignore')\n\n# Config centralizado\nimport sys; sys.path.insert(0, \"/lakehouse/default/Files/projeto-final\")\nfrom config.pipeline_config import EXPERIMENT_NAME, SAFRAS\n\nimport logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%H:%M:%S')\nlogger = logging.getLogger('incremental_eval')\n\nprint('Imports OK')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# FUNCOES UTILITARIAS (reutilizadas do baseline)\n# =============================================================================\n\ndef ks_stat(y_true, y_score):\n    \"\"\"Calcula estatistica KS entre positivos e negativos.\"\"\"\n    from scipy.stats import ks_2samp\n    pos_scores = y_score[y_true == 1]\n    neg_scores = y_score[y_true == 0]\n    if len(pos_scores) == 0 or len(neg_scores) == 0:\n        return 0.0\n    ks, _ = ks_2samp(pos_scores, neg_scores)\n    return ks\n\n\ndef compute_metrics(y_true, y_score, threshold=0.5):\n    \"\"\"Calcula metricas completas: KS, AUC, Gini, Precision, Recall, F1.\"\"\"\n    auc = roc_auc_score(y_true, y_score)\n    ks = ks_stat(y_true, y_score)\n    gini = 2 * auc - 1\n    y_pred = (y_score >= threshold).astype(int)\n    prec = precision_score(y_true, y_pred, zero_division=0)\n    rec = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    return {\n        'KS': round(ks, 4),\n        'AUC': round(auc, 4),\n        'Gini': round(gini, 4),\n        'Precision': round(prec, 4),\n        'Recall': round(rec, 4),\n        'F1': round(f1, 4),\n    }\n\n\ndef filter_xy_by_safra(X, y, list_safras):\n    \"\"\"Filtra X e y por lista de SAFRAs.\"\"\"\n    mask = X['SAFRA'].isin(list_safras)\n    return X[mask].copy(), y[mask].copy()\n\n\ndef split_stratified_data(df, percent=0.25, target_col='FPD'):\n    \"\"\"Split estratificado PySpark por (SAFRA, FPD).\"\"\"\n    w = Window.partitionBy('SAFRA', target_col).orderBy(F.rand(seed=42))\n    df_ranked = df.withColumn('_rank', F.percent_rank().over(w))\n    df_sample = df_ranked.filter(F.col('_rank') <= (1.0 - percent)).drop('_rank')\n    df_oos = df_ranked.filter(F.col('_rank') > (1.0 - percent)).drop('_rank')\n    return df_sample, df_oos\n\n\nprint('Funcoes utilitarias carregadas')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# FUNCOES NOVAS — AVALIACAO INCREMENTAL\n# =============================================================================\n\n# Colunas que NAO sao features (metadados, targets, keys)\nNON_FEATURE_COLS = {\n    'NUM_CPF', 'SAFRA', 'FPD', 'TARGET_SCORE_01', 'TARGET_SCORE_02',\n    '_execution_id', '_data_inclusao', '_data_alteracao_silver',\n    'DT_PROCESSAMENTO', 'DATADENASCIMENTO', 'FLAG_INSTALACAO',\n}\n\n# Prefixos dos books\nBOOK_PREFIXES = ['REC_', 'PAG_', 'FAT_']\n\n\ndef get_feature_groups(columns):\n    \"\"\"Separa colunas em grupos: base (cadastro+telco), REC_, PAG_, FAT_.\n    \n    Args:\n        columns: Lista de nomes de colunas.\n    \n    Returns:\n        dict com keys 'base', 'REC', 'PAG', 'FAT' e listas de colunas.\n    \"\"\"\n    groups = {'base': [], 'REC': [], 'PAG': [], 'FAT': []}\n    for col in columns:\n        if col in NON_FEATURE_COLS:\n            continue\n        if col.startswith('REC_'):\n            groups['REC'].append(col)\n        elif col.startswith('PAG_'):\n            groups['PAG'].append(col)\n        elif col.startswith('FAT_'):\n            groups['FAT'].append(col)\n        else:\n            groups['base'].append(col)\n    return groups\n\n\ndef build_increment_features(feature_groups, increment_id):\n    \"\"\"Retorna lista de features para o incremento especificado.\n    \n    Incrementos:\n        1: base + REC_*\n        2: base + REC_* + PAG_*\n        3: base + REC_* + PAG_* + FAT_* (full)\n    \"\"\"\n    features = list(feature_groups['base'])\n    if increment_id >= 1:\n        features += feature_groups['REC']\n    if increment_id >= 2:\n        features += feature_groups['PAG']\n    if increment_id >= 3:\n        features += feature_groups['FAT']\n    return features\n\n\ndef build_pipeline(X, model_type='LR'):\n    \"\"\"Constroi pipeline sklearn com preprocessamento + modelo.\n    \n    Args:\n        X: DataFrame de features (para detectar tipos).\n        model_type: 'LR' para Logistic Regression, 'LGBM' para LightGBM.\n    \n    Returns:\n        sklearn Pipeline configurado.\n    \"\"\"\n    num_features = [c for c in X.select_dtypes(include=['int32', 'int64', 'float32', 'float64']).columns if c != 'SAFRA']\n    cat_features = [c for c in X.select_dtypes(include=['object', 'category']).columns if c != 'NUM_CPF']\n    \n    num_transformer = Pipeline([\n        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler', StandardScaler()),\n    ])\n    cat_transformer = Pipeline([\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('encoder', CountEncoder(normalize=True, handle_unknown=0, handle_missing=0)),\n    ])\n    \n    preprocessor = ColumnTransformer([\n        ('num', num_transformer, num_features),\n        ('cat', cat_transformer, cat_features),\n    ], remainder='drop')\n    \n    if model_type == 'LR':\n        model = LogisticRegression(\n            solver='liblinear', penalty='l1', C=0.1,\n            max_iter=2000, tol=1e-3, class_weight='balanced', random_state=42\n        )\n    else:  # LGBM\n        model = LGBMClassifier(\n            objective='binary', boosting_type='gbdt',\n            learning_rate=0.05, n_estimators=250, max_depth=7,\n            colsample_bytree=0.8, subsample=0.8,\n            random_state=42, n_jobs=-1, verbosity=-1,\n        )\n    \n    return Pipeline([('prep', preprocessor), ('model', model)])\n\n\ndef train_and_evaluate_increment(\n    X_train, y_train, X_oos, y_oos, X_oot, y_oot,\n    features, increment_id, increment_name,\n    safras_oot_detail=None\n):\n    \"\"\"Treina LR + LGBM para um incremento, avalia em todos os splits, loga no MLflow.\n    \n    Args:\n        X_train, y_train: Dados de treino.\n        X_oos, y_oos: Dados out-of-sample.\n        X_oot, y_oot: Dados out-of-time.\n        features: Lista de features para este incremento.\n        increment_id: 1, 2 ou 3.\n        increment_name: Nome descritivo do incremento.\n        safras_oot_detail: Dict {safra: (X, y)} para avaliacao por safra OOT.\n    \n    Returns:\n        list[dict]: Resultados com metricas por modelo e split.\n    \"\"\"\n    # Filtrar features (manter SAFRA para referencia)\n    keep_cols = [c for c in features if c in X_train.columns]\n    X_tr = X_train[keep_cols].copy()\n    X_os = X_oos[keep_cols].copy()\n    X_ot = X_oot[keep_cols].copy()\n    \n    results = []\n    trained_models = {}\n    \n    for model_type in ['LR', 'LGBM']:\n        model_label = f'LR_L1' if model_type == 'LR' else 'LGBM'\n        run_name = f'{model_label}_increment{increment_id}'\n        \n        with mlflow.start_run(run_name=run_name, nested=True):\n            mlflow.set_tags({\n                'model_type': model_label,\n                'increment_id': str(increment_id),\n                'increment_name': increment_name,\n                'n_features': str(len(keep_cols)),\n            })\n            mlflow.log_param('n_features', len(keep_cols))\n            mlflow.log_param('increment_name', increment_name)\n            mlflow.log_param('features', str(keep_cols[:20]) + '...' if len(keep_cols) > 20 else str(keep_cols))\n            \n            # Treinar\n            pipe = build_pipeline(X_tr, model_type)\n            pipe.fit(X_tr, y_train)\n            trained_models[model_type] = pipe\n            \n            # Avaliar em cada split\n            for split_name, X_eval, y_eval in [\n                ('Train', X_tr, y_train),\n                ('OOS', X_os, y_oos),\n                ('OOT', X_ot, y_oot),\n            ]:\n                scores = pipe.predict_proba(X_eval)[:, 1]\n                metrics = compute_metrics(y_eval, scores)\n                \n                for metric_name, metric_val in metrics.items():\n                    mlflow.log_metric(f'{split_name}_{metric_name}', metric_val)\n                \n                results.append({\n                    'Increment': increment_id,\n                    'Increment_Name': increment_name,\n                    'N_Features': len(keep_cols),\n                    'Model': model_label,\n                    'Split': split_name,\n                    **metrics,\n                })\n            \n            # Avaliar por safra OOT individual\n            if safras_oot_detail:\n                for safra_name, (X_s, y_s) in safras_oot_detail.items():\n                    X_s_inc = X_s[keep_cols].copy()\n                    scores_s = pipe.predict_proba(X_s_inc)[:, 1]\n                    metrics_s = compute_metrics(y_s, scores_s)\n                    for metric_name, metric_val in metrics_s.items():\n                        mlflow.log_metric(f'OOT_{safra_name}_{metric_name}', metric_val)\n                    results.append({\n                        'Increment': increment_id,\n                        'Increment_Name': increment_name,\n                        'N_Features': len(keep_cols),\n                        'Model': model_label,\n                        'Split': f'OOT_{safra_name}',\n                        **metrics_s,\n                    })\n            \n            # Log modelo\n            mlflow.sklearn.log_model(pipe, f'model_{model_label}_inc{increment_id}')\n    \n    return results, trained_models\n\n\nprint('Funcoes de avaliacao incremental carregadas')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# MLFLOW SETUP\n# =============================================================================\nmlflow.set_experiment(EXPERIMENT_NAME)\nmlflow.autolog(\n    log_models=False,  # Controlamos manualmente\n    log_input_examples=False,\n    log_model_signatures=True,\n    silent=True\n)\n\nprint(f'MLflow experiment: {EXPERIMENT_NAME}')\nprint(f'Tracking URI: {mlflow.get_tracking_uri()}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# CARREGAR DADOS DO GOLD FEATURE STORE\n# =============================================================================\nlogger.info('Carregando feature store...')\n\ndf_spark = spark.sql('SELECT * FROM Gold.feature_store.clientes_consolidado')\n\n# Filtrar apenas clientes com FLAG_INSTALACAO = 1 (clientes ativos)\ndf_spark_pos = df_spark.filter(F.col('FLAG_INSTALACAO') == 1)\n\n# Remover colunas com > 75% missing\ntotal = df_spark_pos.count()\ncols_to_keep = []\nfor c in df_spark_pos.columns:\n    null_pct = df_spark_pos.filter(F.col(c).isNull()).count() / total\n    if null_pct <= 0.75:\n        cols_to_keep.append(c)\n\ndf_spark_clean = df_spark_pos.select(cols_to_keep)\n\nlogger.info('Feature store: %d registros, %d colunas (apos filtro missing)', total, len(cols_to_keep))\nprint(f'Registros: {total:,}')\nprint(f'Colunas: {len(cols_to_keep)}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# SPLIT TEMPORAL: SAMPLE (TRAIN+VAL) / OOS / OOT\n# =============================================================================\nsafras_train_oos = [202410, 202411, 202412, 202501]\nsafras_oot = [202502, 202503]\n\n# Separar OOT (temporal holdout)\ndf_train_pool = df_spark_clean.filter(F.col('SAFRA').isin(safras_train_oos))\ndf_oot_spark = df_spark_clean.filter(F.col('SAFRA').isin(safras_oot))\n\n# Split estratificado train/OOS (75/25)\ndf_sample_spark, df_oos_spark = split_stratified_data(df_train_pool, percent=0.25)\n\nlogger.info('Convertendo para Pandas...')\ndf_sample = df_sample_spark.toPandas()\ndf_oos = df_oos_spark.toPandas()\ndf_oot = df_oot_spark.toPandas()\n\nlogger.info('Sample: %d rows, OOS: %d rows, OOT: %d rows', len(df_sample), len(df_oos), len(df_oot))\nprint(f'Sample (Train): {df_sample.shape}')\nprint(f'OOS: {df_oos.shape}')\nprint(f'OOT: {df_oot.shape}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# PREPARAR SPLITS X/Y\n# =============================================================================\ntarget = 'FPD'\n\n# Drop duplicates e NaN no target\ndf_sample = df_sample.drop_duplicates(subset=['NUM_CPF', 'SAFRA']).dropna(subset=[target])\ndf_oos = df_oos.drop_duplicates(subset=['NUM_CPF', 'SAFRA']).dropna(subset=[target])\ndf_oot = df_oot.drop_duplicates(subset=['NUM_CPF', 'SAFRA']).dropna(subset=[target])\n\n# Separar X / y\ndrop_cols = ['NUM_CPF', 'FPD', 'TARGET_SCORE_01', 'TARGET_SCORE_02',\n             '_execution_id', '_data_inclusao', '_data_alteracao_silver',\n             'DT_PROCESSAMENTO', 'DATADENASCIMENTO', 'FLAG_INSTALACAO']\n\nfeature_cols = [c for c in df_sample.columns if c not in drop_cols]\n\nX_train = df_sample[feature_cols].copy()\ny_train = df_sample[target].astype(int).copy()\n\nX_oos = df_oos[feature_cols].copy()\ny_oos = df_oos[target].astype(int).copy()\n\nX_oot = df_oot[feature_cols].copy()\ny_oot = df_oot[target].astype(int).copy()\n\n# OOT por safra individual (para avaliacao detalhada)\nsafras_oot_detail = {}\nfor safra in safras_oot:\n    mask = X_oot['SAFRA'] == safra\n    safras_oot_detail[str(safra)] = (X_oot[mask].copy(), y_oot[mask].copy())\n\nprint(f'X_train: {X_train.shape}, FPD rate: {y_train.mean():.4f}')\nprint(f'X_oos:   {X_oos.shape}, FPD rate: {y_oos.mean():.4f}')\nprint(f'X_oot:   {X_oot.shape}, FPD rate: {y_oot.mean():.4f}')\nfor s, (x, y) in safras_oot_detail.items():\n    print(f'  OOT {s}: {x.shape[0]:,} rows, FPD rate: {y.mean():.4f}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# SEPARAR FEATURES POR PREFIXO (BOOK)\n# =============================================================================\nfeature_groups = get_feature_groups(feature_cols)\n\nprint('Feature Groups:')\nfor group, cols in feature_groups.items():\n    print(f'  {group}: {len(cols)} features')\n    if len(cols) <= 10:\n        print(f'    {cols}')\n    else:\n        print(f'    {cols[:5]} ... {cols[-3:]}')\n\nprint(f'\\nTotal features disponiveis: {sum(len(v) for v in feature_groups.values())}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# DEFINIR INCREMENTOS\n# =============================================================================\nINCREMENTS = [\n    {'id': 1, 'name': 'Base + Recarga',           'books': ['REC']},\n    {'id': 2, 'name': 'Base + Recarga + Pagamento', 'books': ['REC', 'PAG']},\n    {'id': 3, 'name': 'Full (+ Faturamento)',       'books': ['REC', 'PAG', 'FAT']},\n]\n\nfor inc in INCREMENTS:\n    features = build_increment_features(feature_groups, inc['id'])\n    print(f\"Step {inc['id']}: {inc['name']} — {len(features)} features\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# LOOP PRINCIPAL: TREINAR E AVALIAR CADA INCREMENTO\n# =============================================================================\nall_results = []\nall_models = {}\n\nwith mlflow.start_run(run_name='Incremental_Book_Evaluation') as parent_run:\n    mlflow.set_tag('evaluation_type', 'incremental_books')\n    mlflow.log_param('n_increments', len(INCREMENTS))\n    mlflow.log_param('safras_train', str(safras_train_oos))\n    mlflow.log_param('safras_oot', str(safras_oot))\n    \n    for inc in INCREMENTS:\n        logger.info('='*60)\n        logger.info('INCREMENTO %d: %s', inc['id'], inc['name'])\n        logger.info('='*60)\n        \n        features = build_increment_features(feature_groups, inc['id'])\n        logger.info('Features: %d', len(features))\n        \n        results, models = train_and_evaluate_increment(\n            X_train, y_train,\n            X_oos, y_oos,\n            X_oot, y_oot,\n            features=features,\n            increment_id=inc['id'],\n            increment_name=inc['name'],\n            safras_oot_detail=safras_oot_detail,\n        )\n        \n        all_results.extend(results)\n        all_models[inc['id']] = models\n        \n        # Log resumo do incremento no parent run\n        for r in results:\n            if r['Split'] == 'OOT':\n                mlflow.log_metric(f\"inc{inc['id']}_{r['Model']}_KS_OOT\", r['KS'])\n                mlflow.log_metric(f\"inc{inc['id']}_{r['Model']}_AUC_OOT\", r['AUC'])\n    \n    # Log parent run metadata\n    mlflow.log_param('parent_run_id', parent_run.info.run_id)\n\nlogger.info('Loop de avaliacao incremental concluido — %d resultados', len(all_results))\nprint(f'Total de resultados: {len(all_results)}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# CONSOLIDAR RESULTADOS EM DATAFRAME\n# =============================================================================\ndf_results = pd.DataFrame(all_results)\n\nprint('\\n' + '='*80)\nprint('TABELA COMPLETA DE RESULTADOS')\nprint('='*80)\ndisplay(df_results.to_string(index=False))\n\n# Pivot: KS por Incremento x Modelo x Split\ndf_ks_pivot = df_results.pivot_table(\n    values='KS', index=['Increment', 'Increment_Name', 'N_Features'],\n    columns=['Model', 'Split'], aggfunc='first'\n)\nprint('\\n' + '='*80)\nprint('TABELA KS (Incremento x Modelo x Split)')\nprint('='*80)\ndisplay(df_ks_pivot)\n\n# Pivot: AUC por Incremento x Modelo x Split\ndf_auc_pivot = df_results.pivot_table(\n    values='AUC', index=['Increment', 'Increment_Name', 'N_Features'],\n    columns=['Model', 'Split'], aggfunc='first'\n)\nprint('\\n' + '='*80)\nprint('TABELA AUC (Incremento x Modelo x Split)')\nprint('='*80)\ndisplay(df_auc_pivot)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Resultados — Contribuicao Marginal"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# CONTRIBUICAO MARGINAL (DELTA ENTRE INCREMENTOS)\n# =============================================================================\nmarginal_rows = []\n\nfor model in ['LR_L1', 'LGBM']:\n    for split in ['Train', 'OOS', 'OOT']:\n        prev_ks = None\n        prev_auc = None\n        for inc in INCREMENTS:\n            row = df_results[\n                (df_results['Increment'] == inc['id']) &\n                (df_results['Model'] == model) &\n                (df_results['Split'] == split)\n            ]\n            if row.empty:\n                continue\n            ks = row.iloc[0]['KS']\n            auc = row.iloc[0]['AUC']\n            gini = row.iloc[0]['Gini']\n            delta_ks = round(ks - prev_ks, 4) if prev_ks is not None else None\n            delta_auc = round(auc - prev_auc, 4) if prev_auc is not None else None\n            marginal_rows.append({\n                'Increment': inc['id'],\n                'Name': inc['name'],\n                'Model': model,\n                'Split': split,\n                'KS': ks,\n                'AUC': auc,\n                'Gini': gini,\n                'Delta_KS': delta_ks,\n                'Delta_AUC': delta_auc,\n            })\n            prev_ks = ks\n            prev_auc = auc\n\ndf_marginal = pd.DataFrame(marginal_rows)\n\nprint('='*80)\nprint('CONTRIBUICAO MARGINAL POR BOOK')\nprint('='*80)\ndisplay(df_marginal.to_string(index=False))\n\n# Resumo executivo\nprint('\\n--- RESUMO EXECUTIVO ---')\nfor model in ['LR_L1', 'LGBM']:\n    print(f'\\nModelo: {model}')\n    for split in ['OOS', 'OOT']:\n        subset = df_marginal[(df_marginal['Model'] == model) & (df_marginal['Split'] == split)]\n        for _, r in subset.iterrows():\n            delta = f\"(delta KS: {r['Delta_KS']:+.4f})\" if r['Delta_KS'] is not None else '(baseline)'\n            print(f\"  Step {r['Increment']} [{split}]: KS={r['KS']:.4f} {delta}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# VISUALIZACAO 1: KS EVOLUTION POR INCREMENTO\n# =============================================================================\nfig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\nfig.suptitle('KS por Split — Evolucao Incremental de Books', fontsize=14, fontweight='bold')\n\nfor idx, split in enumerate(['Train', 'OOS', 'OOT']):\n    ax = axes[idx]\n    for model in ['LR_L1', 'LGBM']:\n        subset = df_results[(df_results['Model'] == model) & (df_results['Split'] == split)]\n        if not subset.empty:\n            ax.plot(\n                subset['Increment'].values,\n                subset['KS'].values,\n                marker='o', linewidth=2, markersize=8,\n                label=model,\n            )\n            # Anotar valores\n            for _, r in subset.iterrows():\n                ax.annotate(f\"{r['KS']:.3f}\", (r['Increment'], r['KS']),\n                           textcoords='offset points', xytext=(0, 10), ha='center', fontsize=9)\n    \n    ax.set_title(f'{split}', fontsize=12)\n    ax.set_xlabel('Incremento')\n    ax.set_xticks([1, 2, 3])\n    ax.set_xticklabels(['Base+REC', '+PAG', '+FAT'], rotation=15)\n    if idx == 0:\n        ax.set_ylabel('KS')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('/tmp/fig_incremental_ks_evolution.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint('Grafico KS salvo')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# VISUALIZACAO 2: AUC PROGRESSION (BAR CHART COMPARATIVO)\n# =============================================================================\nfig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\nfig.suptitle('AUC por Split — Evolucao Incremental de Books', fontsize=14, fontweight='bold')\n\ncolors = {'LR_L1': '#2196F3', 'LGBM': '#4CAF50'}\nbar_width = 0.35\n\nfor idx, split in enumerate(['Train', 'OOS', 'OOT']):\n    ax = axes[idx]\n    for j, model in enumerate(['LR_L1', 'LGBM']):\n        subset = df_results[(df_results['Model'] == model) & (df_results['Split'] == split)]\n        if not subset.empty:\n            x = np.arange(len(subset))\n            bars = ax.bar(\n                x + j * bar_width, subset['AUC'].values,\n                bar_width, label=model, color=colors[model], alpha=0.85\n            )\n            for bar, val in zip(bars, subset['AUC'].values):\n                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.003,\n                       f'{val:.3f}', ha='center', fontsize=8)\n    \n    ax.set_title(f'{split}', fontsize=12)\n    ax.set_xticks(np.arange(3) + bar_width / 2)\n    ax.set_xticklabels(['Base+REC', '+PAG', '+FAT'], rotation=15)\n    if idx == 0:\n        ax.set_ylabel('AUC')\n    ax.legend()\n    ax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.savefig('/tmp/fig_incremental_auc_progression.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint('Grafico AUC salvo')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# VISUALIZACAO 3: CONTRIBUICAO MARGINAL (WATERFALL / BAR DELTA)\n# =============================================================================\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nfig.suptitle('Contribuicao Marginal por Book (Delta KS)', fontsize=14, fontweight='bold')\n\nfor idx, model in enumerate(['LR_L1', 'LGBM']):\n    ax = axes[idx]\n    subset = df_marginal[\n        (df_marginal['Model'] == model) &\n        (df_marginal['Split'] == 'OOT') &\n        (df_marginal['Delta_KS'].notna())\n    ]\n    if not subset.empty:\n        colors_bar = ['#4CAF50' if d > 0 else '#F44336' for d in subset['Delta_KS']]\n        bars = ax.bar(\n            range(len(subset)), subset['Delta_KS'].values,\n            color=colors_bar, alpha=0.85, edgecolor='black', linewidth=0.5\n        )\n        for bar, val in zip(bars, subset['Delta_KS'].values):\n            ax.text(bar.get_x() + bar.get_width()/2,\n                   bar.get_height() + 0.001 * (1 if val >= 0 else -3),\n                   f'{val:+.4f}', ha='center', fontsize=10, fontweight='bold')\n    \n    ax.set_title(f'{model} — OOT', fontsize=12)\n    ax.set_xticks(range(len(subset)))\n    ax.set_xticklabels(['+Pagamento', '+Faturamento'], rotation=0)\n    ax.set_ylabel('Delta KS')\n    ax.axhline(y=0, color='black', linewidth=0.8)\n    ax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.savefig('/tmp/fig_incremental_marginal_delta.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint('Grafico marginal delta salvo')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# VISUALIZACAO 4: FEATURE IMPORTANCE TOP-15 POR INCREMENTO (LGBM)\n# =============================================================================\nfig, axes = plt.subplots(1, 3, figsize=(20, 8))\nfig.suptitle('Top 15 Features por Incremento (LGBM Importance)', fontsize=14, fontweight='bold')\n\nfor idx, inc in enumerate(INCREMENTS):\n    ax = axes[idx]\n    lgbm_model = all_models[inc['id']].get('LGBM')\n    if lgbm_model is None:\n        ax.set_title(f\"Step {inc['id']}: N/A\")\n        continue\n    \n    # Extrair importancias\n    booster = lgbm_model.named_steps['model']\n    prep = lgbm_model.named_steps['prep']\n    try:\n        feature_names = prep.get_feature_names_out()\n    except Exception:\n        features_inc = build_increment_features(feature_groups, inc['id'])\n        feature_names = [c for c in features_inc if c != 'SAFRA']\n    \n    importances = booster.feature_importances_\n    n = min(len(feature_names), len(importances))\n    df_imp = pd.DataFrame({\n        'feature': list(feature_names)[:n],\n        'importance': list(importances)[:n]\n    }).sort_values('importance', ascending=True).tail(15)\n    \n    # Colorir por source\n    colors_fi = []\n    for f in df_imp['feature']:\n        if 'REC_' in f or f.startswith('num__REC_') or f.startswith('cat__REC_'):\n            colors_fi.append('#2196F3')  # Azul\n        elif 'PAG_' in f or f.startswith('num__PAG_') or f.startswith('cat__PAG_'):\n            colors_fi.append('#FF9800')  # Laranja\n        elif 'FAT_' in f or f.startswith('num__FAT_') or f.startswith('cat__FAT_'):\n            colors_fi.append('#9C27B0')  # Roxo\n        else:\n            colors_fi.append('#607D8B')  # Cinza (base)\n    \n    ax.barh(range(len(df_imp)), df_imp['importance'].values, color=colors_fi, alpha=0.85)\n    ax.set_yticks(range(len(df_imp)))\n    ax.set_yticklabels(df_imp['feature'].values, fontsize=7)\n    ax.set_title(f\"Step {inc['id']}: {inc['name']}\\n({len(build_increment_features(feature_groups, inc['id']))} features)\", fontsize=10)\n    ax.set_xlabel('Importance')\n\n# Legenda\nfrom matplotlib.patches import Patch\nlegend_elements = [\n    Patch(facecolor='#607D8B', label='Base (Cadastro+Telco)'),\n    Patch(facecolor='#2196F3', label='Recarga (REC_)'),\n    Patch(facecolor='#FF9800', label='Pagamento (PAG_)'),\n    Patch(facecolor='#9C27B0', label='Faturamento (FAT_)'),\n]\nfig.legend(handles=legend_elements, loc='lower center', ncol=4, fontsize=10)\n\nplt.tight_layout(rect=[0, 0.05, 1, 0.95])\nplt.savefig('/tmp/fig_incremental_feature_importance.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint('Grafico feature importance salvo')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# SWAP ANALYSIS POR INCREMENTO\n# =============================================================================\n\ndef swap_analysis_incremental(y_true_1, scores_1, y_true_2, scores_2, top_pct=0.1):\n    \"\"\"Compara ranking entre dois conjuntos (OOT1 vs OOT2) para estabilidade.\n    \n    Mede se os clientes classificados como alto risco em OOT1 continuam\n    sendo alto risco em OOT2 (swap-in/swap-out).\n    \"\"\"\n    n1 = int(len(scores_1) * top_pct)\n    n2 = int(len(scores_2) * top_pct)\n    \n    # Top risco em cada periodo\n    top_idx_1 = np.argsort(scores_1)[-n1:]\n    top_idx_2 = np.argsort(scores_2)[-n2:]\n    \n    # FPD rate no top\n    fpd_rate_top_1 = y_true_1.iloc[top_idx_1].mean() if len(top_idx_1) > 0 else 0\n    fpd_rate_top_2 = y_true_2.iloc[top_idx_2].mean() if len(top_idx_2) > 0 else 0\n    \n    # Metricas gerais\n    metrics_1 = compute_metrics(y_true_1, scores_1)\n    metrics_2 = compute_metrics(y_true_2, scores_2)\n    \n    return {\n        'KS_OOT1': metrics_1['KS'],\n        'KS_OOT2': metrics_2['KS'],\n        'Delta_KS': round(metrics_2['KS'] - metrics_1['KS'], 4),\n        'AUC_OOT1': metrics_1['AUC'],\n        'AUC_OOT2': metrics_2['AUC'],\n        'Delta_AUC': round(metrics_2['AUC'] - metrics_1['AUC'], 4),\n        'FPD_Rate_Top10_OOT1': round(fpd_rate_top_1, 4),\n        'FPD_Rate_Top10_OOT2': round(fpd_rate_top_2, 4),\n        'Delta_FPD_Top10': round(fpd_rate_top_2 - fpd_rate_top_1, 4),\n    }\n\n\n# Executar swap para cada incremento\nswap_results = []\n\nfor inc in INCREMENTS:\n    features = build_increment_features(feature_groups, inc['id'])\n    keep_cols = [c for c in features if c in X_oot.columns]\n    \n    for model_type in ['LR', 'LGBM']:\n        model_label = 'LR_L1' if model_type == 'LR' else 'LGBM'\n        pipe = all_models[inc['id']][model_type]\n        \n        # Scores por safra OOT\n        X_oot1, y_oot1 = safras_oot_detail['202502']\n        X_oot2, y_oot2 = safras_oot_detail['202503']\n        \n        scores_oot1 = pipe.predict_proba(X_oot1[keep_cols])[:, 1]\n        scores_oot2 = pipe.predict_proba(X_oot2[keep_cols])[:, 1]\n        \n        swap = swap_analysis_incremental(y_oot1, scores_oot1, y_oot2, scores_oot2)\n        swap_results.append({\n            'Increment': inc['id'],\n            'Name': inc['name'],\n            'Model': model_label,\n            **swap\n        })\n\ndf_swap = pd.DataFrame(swap_results)\n\nprint('='*80)\nprint('SWAP ANALYSIS — ESTABILIDADE TEMPORAL (OOT1 vs OOT2)')\nprint('='*80)\ndisplay(df_swap.to_string(index=False))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# VISUALIZACAO 5: SWAP COMPARISON\n# =============================================================================\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nfig.suptitle('Swap Analysis — Estabilidade OOT1 vs OOT2', fontsize=14, fontweight='bold')\n\nfor idx, metric in enumerate(['Delta_KS', 'Delta_FPD_Top10']):\n    ax = axes[idx]\n    for j, model in enumerate(['LR_L1', 'LGBM']):\n        subset = df_swap[df_swap['Model'] == model]\n        x = np.arange(len(subset))\n        bars = ax.bar(\n            x + j * 0.35, subset[metric].values,\n            0.35, label=model, alpha=0.85\n        )\n        for bar, val in zip(bars, subset[metric].values):\n            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n                   f'{val:+.4f}', ha='center', fontsize=9)\n    \n    ax.set_title(metric.replace('_', ' '), fontsize=12)\n    ax.set_xticks(np.arange(3) + 0.175)\n    ax.set_xticklabels(['Base+REC', '+PAG', '+FAT'], rotation=0)\n    ax.axhline(y=0, color='black', linewidth=0.8)\n    ax.legend()\n    ax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.savefig('/tmp/fig_incremental_swap_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint('Grafico swap salvo')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# EXPORT — CSVs E SUMMARY\n# =============================================================================\nimport os\nartifacts_dir = '/tmp/incremental_eval'\nos.makedirs(artifacts_dir, exist_ok=True)\n\n# CSVs\ndf_results.to_csv(f'{artifacts_dir}/incremental_evaluation_full_results.csv', index=False)\ndf_marginal.to_csv(f'{artifacts_dir}/incremental_marginal_contribution.csv', index=False)\ndf_swap.to_csv(f'{artifacts_dir}/swap_analysis_consolidated.csv', index=False)\ndf_ks_pivot.to_csv(f'{artifacts_dir}/incremental_comparison_ks.csv')\ndf_auc_pivot.to_csv(f'{artifacts_dir}/incremental_comparison_auc.csv')\n\n# Log artefatos no MLflow\nwith mlflow.start_run(run_name='Incremental_Summary_Artifacts', nested=False):\n    mlflow.set_tag('artifact_type', 'incremental_evaluation_summary')\n    for fname in os.listdir(artifacts_dir):\n        mlflow.log_artifact(f'{artifacts_dir}/{fname}', 'incremental_evaluation')\n    \n    # Log figuras\n    for fig_path in [\n        '/tmp/fig_incremental_ks_evolution.png',\n        '/tmp/fig_incremental_auc_progression.png',\n        '/tmp/fig_incremental_marginal_delta.png',\n        '/tmp/fig_incremental_feature_importance.png',\n        '/tmp/fig_incremental_swap_comparison.png',\n    ]:\n        if os.path.exists(fig_path):\n            mlflow.log_artifact(fig_path, 'figures')\n\nprint(f'Artefatos exportados para: {artifacts_dir}')\nprint(f'CSVs: {len([f for f in os.listdir(artifacts_dir) if f.endswith(\".csv\")])}')\nprint('MLflow artifacts logged')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# HEATMAP RESUMO FINAL\n# =============================================================================\nfig, ax = plt.subplots(figsize=(12, 6))\nfig.suptitle('Heatmap — KS por Incremento, Modelo e Split', fontsize=14, fontweight='bold')\n\nheatmap_data = df_results.pivot_table(\n    values='KS',\n    index=['Increment', 'Model'],\n    columns='Split',\n    aggfunc='first'\n)\n\n# Ordenar colunas\ncol_order = ['Train', 'OOS', 'OOT']\nheatmap_data = heatmap_data[[c for c in col_order if c in heatmap_data.columns]]\n\nim = ax.imshow(heatmap_data.values, cmap='RdYlGn', aspect='auto', vmin=0.15, vmax=0.45)\n\nax.set_xticks(range(len(heatmap_data.columns)))\nax.set_xticklabels(heatmap_data.columns, fontsize=11)\nax.set_yticks(range(len(heatmap_data.index)))\nax.set_yticklabels([f'Step {i} — {m}' for i, m in heatmap_data.index], fontsize=10)\n\n# Anotar valores\nfor i in range(len(heatmap_data.index)):\n    for j in range(len(heatmap_data.columns)):\n        val = heatmap_data.values[i, j]\n        ax.text(j, i, f'{val:.3f}', ha='center', va='center', fontsize=11, fontweight='bold')\n\nplt.colorbar(im, ax=ax, label='KS')\nplt.tight_layout()\nplt.savefig('/tmp/fig_incremental_summary_heatmap.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint('Heatmap resumo salvo')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Conclusoes\n\n### Interpretacao dos Resultados\n\n| Incremento | O que avalia |\n|------------|-------------|\n| Step 1 (Base + Recarga) | Poder preditivo do comportamento de recarga isolado |\n| Step 2 (+ Pagamento) | Ganho marginal do historico de pagamentos |\n| Step 3 (+ Faturamento) | Ganho marginal do perfil de faturamento |\n\n### Criterios de Decisao\n\n- **Delta KS > +0.02**: Book contribui significativamente\n- **Delta KS entre -0.01 e +0.02**: Contribuicao marginal (considerar complexidade vs ganho)\n- **Delta KS < -0.01**: Book pode estar adicionando ruido (investigar)\n- **Delta FPD Top10 < |0.02|**: Modelo estavel temporalmente (swap aceitavel)\n\n### Proximos Passos\n\n1. Se algum book nao contribui, considerar remove-lo para simplicidade\n2. Aplicar feature selection (IV + L1) no melhor incremento\n3. Treinar modelo final com features selecionadas\n4. Registrar modelo no MLflow Model Registry"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}